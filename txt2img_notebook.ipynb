{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNHvQBhzyXCI",
    "outputId": "0a79e979-8484-4c62-96d9-7c79b1835162",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# may have to restart runtime/reset kernel after running\n",
    "!git clone https://github.com/CompVis/latent-diffusion.git\n",
    "!git clone https://github.com/CompVis/taming-transformers\n",
    "!pip install -e ./taming-transformers\n",
    "!pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install transformers kornia imageio imageio_ffmpeg pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p models/ldm/stable-diffusion-v1/\n",
    "# when weight become available drop link here\n",
    "!wget -O models/ldm/stable-diffusion-v1/model.ckpt DROP_LINK_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/ldm/text2img-large/\n",
    "!wget -O models/ldm/text2img-large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import imageio\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange, repeat\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import contextmanager, nullcontext\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "use_laion400m = False\n",
    "use_plms = False\n",
    "\n",
    "if use_laion400m:\n",
    "    print(\"Falling back to LAION 400M model...\")\n",
    "    config_location = \"configs/latent-diffusion/txt2img-1p4B-eval.yaml\"\n",
    "    model_location = \"models/ldm/text2img-large/model.ckpt\"\n",
    "    outdir = \"outputs/txt2img-samples-laion400m\"\n",
    "else:\n",
    "    print(\"Using Stable Diffusion model...\")\n",
    "    config_location = 'configs/stable-diffusion/v1-inference.yaml'\n",
    "    model_location = 'models/ldm/stable-diffusion-v1/model.ckpt'\n",
    "    outdir = 'outputs/txt2img-samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "fnGwQRhtyBhb"
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def load_model_from_config(config, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt)#, map_location=\"cpu\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_model():\n",
    "    config = OmegaConf.load(config_location)\n",
    "    model = load_model_from_config(config, model_location)\n",
    "    return model\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "def load_img(path, size):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    w, h = image.size\n",
    "    print(f\"loaded input image of size ({w}, {h}) and resized to ({size[0]}, {size[1]}) from {path}\")\n",
    "    w, h = size #map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
    "    image = image.resize((w, h), resample=Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.*image - 1.\n",
    "\n",
    "def slerp(val, low, high):\n",
    "    low_norm = low/torch.norm(low)\n",
    "    high_norm = high/torch.norm(high)\n",
    "    omega = torch.acos((low_norm*high_norm).sum())\n",
    "    so = torch.sin(omega)\n",
    "    res = (torch.sin((1.0-val)*omega)/so)*low + (torch.sin(val*omega)/so) * high\n",
    "    return res\n",
    "\n",
    "def get_slerp_vectors(start, end, frames_per_degree=1, start_code=False, frames=20):\n",
    "    if not start_code:\n",
    "        start_norm = start/torch.norm(start)\n",
    "        end_norm = end/torch.norm(end)\n",
    "        omega = torch.acos((start_norm*end_norm).sum())\n",
    "        frames = round(omega.item() * frames_per_degree * 57.2957795)\n",
    "    out = torch.Tensor(frames, start.shape[0]).to(device)\n",
    "    factor = 1.0 / (frames - 1)\n",
    "    for i in range(frames):\n",
    "        out[i] = slerp(factor*i, start, end)\n",
    "    return out, frames\n",
    "\n",
    "def get_starting_code_and_conditioning_vector(seed, prompt):\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(np.iinfo(np.int32).max)\n",
    "    seed_everything(seed)\n",
    "    start_code = torch.randn([1, C, H // f, W // f], device=device)\n",
    "    c = model.get_learned_conditioning(prompt)\n",
    "    return (c, start_code)\n",
    "\n",
    "def unflatten(l, n):\n",
    "    res = []\n",
    "    t = l[:]\n",
    "    while len(t) > 0:\n",
    "        res.append(t[:n])\n",
    "        t = t[n:]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPnyd-XUKbfE",
    "outputId": "0fcd10e4-0df2-4ab9-cbf5-f08f4902c954",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "if use_plms:\n",
    "    print(\"Warning: img2img and disco style animations not compatible with PLMSSampler\")\n",
    "    sampler = PLMSSampler(model)\n",
    "else:\n",
    "    sampler = DDIMSampler(model)\n",
    "\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "sample_path = os.path.join(outdir, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "grid_count = len(os.listdir(outdir)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text -> Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Joe Biden exhaling a large smoke cloud from his bong, candid photography\"\n",
    "] # list of string prompts\n",
    "seed = None\n",
    "\n",
    "init_image_location = \"\" # file location of init image (use a blank string if none desired)\n",
    "init_noise_strength = 0.75 # how much to noise init image (0-1.0 where 1.0 is full destruction of init image information)\n",
    "\n",
    "ddim_steps = 50 # ddim sampling steps\n",
    "ddim_eta = 0.0 # ddim eta (eta=0.0 corresponds to deterministic sampling) (must be 0.0 if using PLMS sampling)\n",
    "unconditional_guidance_scale = 7.5 # unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\n",
    "\n",
    "precision = 'autocast' # precision to evaluate at (full or autocast)\n",
    "\n",
    "n_iter = 1 # how many sample iterations\n",
    "batch_size = 1 # how many samples to generate per prompt\n",
    "n_rows = 0 # rows in the grid (will use batch_size if set to 0)\n",
    "\n",
    "fixed_code = True # if enabled, uses the same starting code across samples\n",
    "skip_grid = True # do not save a grid, only individual samples\n",
    "skip_save = False # do not save individual samples\n",
    "show_images = True # whether or not to show images after generation\n",
    "\n",
    "H = 512 # height\n",
    "W = 512 # width\n",
    "C = 4 # channels\n",
    "f = 8 # downsampling factor\n",
    "\n",
    "if n_rows == 0:\n",
    "    n_rows = batch_size\n",
    "\n",
    "if seed is None:\n",
    "    seed = np.random.randint(np.iinfo(np.int32).max)\n",
    "seed_everything(seed)\n",
    "\n",
    "start_code = None\n",
    "if fixed_code:\n",
    "    start_code = torch.randn([batch_size, C, H // f, W // f], device=device)\n",
    "\n",
    "precision_scope = autocast if precision==\"autocast\" else nullcontext\n",
    "data = chunk(prompts, batch_size)\n",
    "\n",
    "if init_image_location != \"\":\n",
    "    assert os.path.isfile(init_image_location)\n",
    "    init_image = load_img(init_image_location, ()).to(device)\n",
    "    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "\n",
    "    sampler.make_schedule(ddim_num_steps=ddim_steps, ddim_eta=ddim_eta, verbose=False)\n",
    "\n",
    "    assert 0. <= init_noise_strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "    t_enc = int(init_noise_strength * ddim_steps)\n",
    "    print(f\"target t_enc is {t_enc} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            tic = time.time()\n",
    "            all_samples = list()\n",
    "            for n in trange(n_iter, desc=\"Sampling\"):\n",
    "                for prompts in tqdm(data, desc=\"data\"):\n",
    "                    uc = None\n",
    "                    if unconditional_guidance_scale != 1.0:\n",
    "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "                    c = model.get_learned_conditioning(prompts)\n",
    "\n",
    "                    if len(init_image_location) == 0 or init_image_location == '':\n",
    "                        shape = [C, H // f, W // f]\n",
    "                        samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                            conditioning=c,\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            shape=shape,\n",
    "                                                            verbose=False,\n",
    "                                                            unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                                            unconditional_conditioning=uc,\n",
    "                                                            eta=ddim_eta,\n",
    "                                                            x_T=start_code)\n",
    "                    else:\n",
    "                        # encode (scaled latent)\n",
    "                        z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "                        # decode it\n",
    "                        samples_ddim = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                                unconditional_conditioning=uc)\n",
    "\n",
    "                    x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                    x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                    if not skip_save:\n",
    "                        for x_sample in x_samples_ddim:\n",
    "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                            img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                            img.save(\n",
    "                                os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "                            base_count += 1\n",
    "                            \n",
    "                            if show_images:\n",
    "                                display(img)\n",
    "                                \n",
    "                    if skip_save and show_images:\n",
    "                        for x_sample in x_samples_ddim:\n",
    "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                            img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                            display(img)\n",
    "\n",
    "                    if not skip_grid:\n",
    "                        all_samples.append(x_samples_ddim)\n",
    "\n",
    "            if not skip_grid:\n",
    "                # additionally, save as grid\n",
    "                grid = torch.stack(all_samples, 0)\n",
    "                grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "                # to image\n",
    "                grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "                Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outdir, f'grid-{grid_count:04}.png'))\n",
    "                grid_count += 1\n",
    "\n",
    "            toc = time.time()\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outdir}\\n\"\n",
    "        f\"\\nTime elapsed: {round(toc - tic, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text, Text -> Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92QkRfm0e6K0"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    (26208852, \"Joe Biden exhaling a large smoke cloud from his bong, candid photography\"),\n",
    "    (685626752, \"Barack Obama exhaling a large smoke cloud from his bong, candid photography\"),\n",
    "] # (seed, prompt)\n",
    "\n",
    "ddim_steps = 50 # ddim sampling steps\n",
    "ddim_eta = 0.0 # ddim eta (eta=0.0 corresponds to deterministic sampling) (must be 0.0 if using PLMS sampling)\n",
    "unconditional_guidance_scale = 7.5 # unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\n",
    "\n",
    "precision = 'autocast' # precision to evaluate at (full or autocast)\n",
    "\n",
    "batch_size = 10 # how many samples to generate per prompt (currently must be set to one for animation)\n",
    "\n",
    "loop = True # if enabled, make animation loop by interpolating between end and start vectors\n",
    "# fixed_code = True # if enabled, uses the same starting code across samples\n",
    "skip_save = False # do not save individual samples\n",
    "show_images = True # whether or not to show images after generation\n",
    "\n",
    "H = 512 # height\n",
    "W = 512 # width\n",
    "C = 4 # channels\n",
    "f = 8 # downsampling factor\n",
    "\n",
    "precision_scope = autocast if precision==\"autocast\" else nullcontext\n",
    "\n",
    "degrees_per_second = 20 # degrees to travel per second\n",
    "fps = 40 # frames per second of output mp4\n",
    "\n",
    "frames_per_degree = fps / degrees_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_c = None\n",
    "previous_start_code = None\n",
    "slerp_c_vectors = []\n",
    "slerp_start_codes = []\n",
    "for i, data in enumerate(map(lambda x: get_starting_code_and_conditioning_vector(*x), prompts)):\n",
    "    c, start_code = data\n",
    "    if i == 0:\n",
    "        slerp_c_vectors.append(c)\n",
    "        slerp_start_codes.append(start_code)\n",
    "    else:\n",
    "        original_c_shape = c.shape\n",
    "        original_start_code_shape = start_code.shape\n",
    "        c_vectors, frames = get_slerp_vectors(previous_c.flatten(), c.flatten(), frames_per_degree=frames_per_degree)\n",
    "        c_vectors = c_vectors.reshape(-1, *original_c_shape)\n",
    "        slerp_c_vectors.extend(list(c_vectors[1:])) # drop first frame to prevent repeating frames\n",
    "        start_codes, frames = get_slerp_vectors(previous_start_code.flatten(), start_code.flatten(), frames_per_degree=frames_per_degree, start_code = True, frames=frames)\n",
    "        start_codes = start_codes.reshape(-1, *original_start_code_shape)\n",
    "        slerp_start_codes.extend(list(start_codes[1:])) # drop first frame to prevent repeating frames\n",
    "        if loop and i == len(prompts) - 1:\n",
    "            c_vectors, frames = get_slerp_vectors(c.flatten(), slerp_c_vectors[0].flatten(), frames_per_degree=frames_per_degree)\n",
    "            c_vectors = c_vectors.reshape(-1, *original_c_shape)\n",
    "            slerp_c_vectors.extend(list(c_vectors[1:-1])) # drop first and last frame to prevent repeating frames\n",
    "            start_codes, frames = get_slerp_vectors(start_code.flatten(), slerp_start_codes[0].flatten(), frames_per_degree=frames_per_degree, start_code = True, frames=frames)\n",
    "            start_codes = start_codes.reshape(-1, *original_start_code_shape)\n",
    "            slerp_start_codes.extend(list(start_codes[1:-1])) # drop first and last frame to prevent repeating frames\n",
    "    previous_c = c\n",
    "    previous_start_code = start_code\n",
    "    \n",
    "slerp_c_vectors = unflatten(slerp_c_vectors, batch_size)\n",
    "slerp_start_codes = unflatten(slerp_start_codes, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_out = imageio.get_writer('test.mp4', mode='I', fps=fps, codec='libx264')\n",
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            tic = time.time()\n",
    "            #all_samples = list()\n",
    "            for c, start_code in tqdm(zip(slerp_c_vectors, slerp_start_codes), desc=\"data\", total=len(slerp_c_vectors)):\n",
    "                uc = None\n",
    "                if unconditional_guidance_scale != 1.0:\n",
    "                    uc = model.get_learned_conditioning(len(c) * [\"\"])\n",
    "                if isinstance(c, tuple) or isinstance(c, list):\n",
    "                    c = torch.stack(list(c), dim=0)\n",
    "                \n",
    "                c = torch.cat(tuple(c))\n",
    "                start_code = torch.cat(tuple(start_code))\n",
    "                \n",
    "                shape = [C, H // f, W // f]\n",
    "\n",
    "                samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                    conditioning=c,\n",
    "                                                    batch_size=len(c),\n",
    "                                                    shape=shape,\n",
    "                                                    verbose=False,\n",
    "                                                    unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                                    unconditional_conditioning=uc,\n",
    "                                                    eta=ddim_eta,\n",
    "                                                    x_T=start_code)\n",
    "\n",
    "                x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                if not skip_save:\n",
    "                    for x_sample in x_samples_ddim:\n",
    "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                        video_out.append_data(x_sample)\n",
    "                \n",
    "                        if show_images:\n",
    "                            img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                            display(img)\n",
    "                            \n",
    "                if skip_save and show_images:\n",
    "                    for x_sample in x_samples_ddim:\n",
    "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                        img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                        display(img)\n",
    "\n",
    "            toc = time.time()\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outdir}\\n\"\n",
    "        f\"\\nTime elapsed: {round(toc - tic, 2)} seconds\")\n",
    "video_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disco Diffusion Style Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt update\n",
    "!apt install python3-opencv -y\n",
    "!pip install timm opencv-python matplotlib pandas\n",
    "!git clone https://github.com/alembics/disco-diffusion.git\n",
    "!mv disco-diffusion/disco_xform_utils.py disco_xform_utils.py\n",
    "!git clone https://github.com/isl-org/MiDaS.git\n",
    "!git clone https://github.com/MSFTserver/pytorch3d-lite.git\n",
    "!mv MiDaS/utils.py MiDaS/midas_utils.py\n",
    "!git clone https://github.com/shariqfarooq123/AdaBins.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models/depth/\n",
    "!mkdir models/depth/midas/\n",
    "!wget -O models/depth/midas/model.ckpt https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir pretrained/\n",
    "!wget -O pretrained/AdaBins_nyu.pt https://cloudflare-ipfs.com/ipfs/Qmd2mMnDLWePKmgfS8m6ntAg4nhV5VkUyAydYBp8cWWeB7/AdaBins_nyu.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(f'{os.path.abspath(os.getcwd())}')\n",
    "sys.path.append(f'{os.path.abspath(os.getcwd())}/MiDaS')\n",
    "sys.path.append(f'{os.path.abspath(os.getcwd())}/pytorch3d-lite')\n",
    "sys.path.append(f'{os.path.abspath(os.getcwd())}/AdaBins/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gc\n",
    "import math\n",
    "import py3d_tools as p3dT\n",
    "import disco_xform_utils as dxf\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import pandas as pd\n",
    "from ipywidgets import Output\n",
    "from midas.dpt_depth import DPTDepthModel\n",
    "from midas.midas_net import MidasNet\n",
    "from midas.midas_net_custom import MidasNet_small\n",
    "from midas.transforms import Resize, NormalizeImage, PrepareForNet\n",
    "from types import SimpleNamespace\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_midas_depth_model():\n",
    "    midas_model = None\n",
    "    net_w = None\n",
    "    net_h = None\n",
    "    resize_mode = None\n",
    "    normalization = None\n",
    "\n",
    "    print(f\"Initializing MiDaS depth model...\")\n",
    "    # load network\n",
    "    midas_model_path = 'models/depth/midas/model.ckpt'\n",
    "    midas_model = DPTDepthModel(\n",
    "        path=midas_model_path,\n",
    "        backbone=\"vitl16_384\",\n",
    "        non_negative=True,\n",
    "    )\n",
    "    net_w, net_h = 384, 384\n",
    "    resize_mode = \"minimal\"\n",
    "    normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    "    midas_transform = T.Compose(\n",
    "        [\n",
    "            Resize(\n",
    "                net_w,\n",
    "                net_h,\n",
    "                resize_target=None,\n",
    "                keep_aspect_ratio=True,\n",
    "                ensure_multiple_of=32,\n",
    "                resize_method=resize_mode,\n",
    "                image_interpolation_method=cv2.INTER_CUBIC,\n",
    "            ),\n",
    "            normalization,\n",
    "            PrepareForNet(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    midas_model.eval()\n",
    "\n",
    "    midas_model = midas_model.to(memory_format=torch.channels_last)  \n",
    "    midas_model = midas_model.half()\n",
    "\n",
    "    midas_model.to(device)\n",
    "\n",
    "    print(f\"MiDaS depth model initialized.\")\n",
    "    return midas_model, midas_transform, net_w, net_h, resize_mode, normalization\n",
    "\n",
    "def do_3d_step(img_filepath, frame_num, midas_model, midas_transform):\n",
    "  if args.key_frames:\n",
    "    translation_x = args.translation_x_series[frame_num]\n",
    "    translation_y = args.translation_y_series[frame_num]\n",
    "    translation_z = args.translation_z_series[frame_num]\n",
    "    rotation_3d_x = args.rotation_3d_x_series[frame_num]\n",
    "    rotation_3d_y = args.rotation_3d_y_series[frame_num]\n",
    "    rotation_3d_z = args.rotation_3d_z_series[frame_num]\n",
    "    print(\n",
    "        f'translation_x: {translation_x}',\n",
    "        f'translation_y: {translation_y}',\n",
    "        f'translation_z: {translation_z}',\n",
    "        f'rotation_3d_x: {rotation_3d_x}',\n",
    "        f'rotation_3d_y: {rotation_3d_y}',\n",
    "        f'rotation_3d_z: {rotation_3d_z}',\n",
    "    )\n",
    "\n",
    "  translate_xyz = [-translation_x*TRANSLATION_SCALE, translation_y*TRANSLATION_SCALE, -translation_z*TRANSLATION_SCALE]\n",
    "  rotate_xyz_degrees = [rotation_3d_x, rotation_3d_y, rotation_3d_z]\n",
    "  print('translation:',translate_xyz)\n",
    "  print('rotation:',rotate_xyz_degrees)\n",
    "  rotate_xyz = [math.radians(rotate_xyz_degrees[0]), math.radians(rotate_xyz_degrees[1]), math.radians(rotate_xyz_degrees[2])]\n",
    "  rot_mat = p3dT.euler_angles_to_matrix(torch.tensor(rotate_xyz, device=device), \"XYZ\").unsqueeze(0)\n",
    "  print(\"rot_mat: \" + str(rot_mat))\n",
    "  next_step_pil = dxf.transform_image_3d(img_filepath, midas_model, midas_transform, device,\n",
    "                                          rot_mat, translate_xyz, args.near_plane, args.far_plane,\n",
    "                                          args.fov, padding_mode=args.padding_mode,\n",
    "                                          sampling_mode=args.sampling_mode, midas_weight=args.midas_weight)\n",
    "  return next_step_pil\n",
    "\n",
    "def interp(t):\n",
    "    return 3 * t**2 - 2 * t ** 3\n",
    "\n",
    "def perlin(width, height, scale=10):\n",
    "    gx, gy = torch.randn(2, width + 1, height + 1, 1, 1, device=device)\n",
    "    xs = torch.linspace(0, 1, scale + 1)[:-1, None].to(device)\n",
    "    ys = torch.linspace(0, 1, scale + 1)[None, :-1].to(device)\n",
    "    wx = 1 - interp(xs)\n",
    "    wy = 1 - interp(ys)\n",
    "    dots = 0\n",
    "    dots += wx * wy * (gx[:-1, :-1] * xs + gy[:-1, :-1] * ys)\n",
    "    dots += (1 - wx) * wy * (-gx[1:, :-1] * (1 - xs) + gy[1:, :-1] * ys)\n",
    "    dots += wx * (1 - wy) * (gx[:-1, 1:] * xs - gy[:-1, 1:] * (1 - ys))\n",
    "    dots += (1 - wx) * (1 - wy) * (-gx[1:, 1:] * (1 - xs) - gy[1:, 1:] * (1 - ys))\n",
    "    return dots.permute(0, 2, 1, 3).contiguous().view(width * scale, height * scale)\n",
    "\n",
    "def perlin_ms(octaves, width, height, grayscale):\n",
    "    out_array = [0.5] if grayscale else [0.5, 0.5, 0.5, 0.5]\n",
    "    # out_array = [0.0] if grayscale else [0.0, 0.0, 0.0]\n",
    "    for i in range(1 if grayscale else 4):\n",
    "        scale = 2 ** len(octaves)\n",
    "        oct_width = width\n",
    "        oct_height = height\n",
    "        for oct in octaves:\n",
    "            p = perlin(oct_width, oct_height, scale)\n",
    "            out_array[i] += p * oct\n",
    "            scale //= 2\n",
    "            oct_width *= 2\n",
    "            oct_height *= 2\n",
    "    return torch.cat(out_array)\n",
    "\n",
    "def create_perlin_noise(octaves=[1, 1, 1, 1], width=2, height=2, grayscale=True):\n",
    "    out = perlin_ms(octaves, width, height, grayscale)\n",
    "    if grayscale:\n",
    "        out = TF.resize(size=(args.W//args.f, args.H//args.f), img=out.unsqueeze(0))\n",
    "        out = TF.to_pil_image(out.clamp(0, 1)).convert('RGBA')\n",
    "    else:\n",
    "        out = out.reshape(-1, 4, out.shape[0]//4, out.shape[1])\n",
    "        out = TF.resize(size=(args.W//args.f, args.H//args.f), img=out)\n",
    "        out = TF.to_pil_image(out.clamp(0, 1).squeeze())\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def regen_perlin():\n",
    "    if args.perlin_mode == 'color':\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
    "    elif args.perlin_mode == 'gray':\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "    else:\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "    display(init)\n",
    "    init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "    del init2\n",
    "    return init.expand(args.batch_size, -1, -1, -1)\n",
    "\n",
    "# def save_settings():\n",
    "#     setting_list = {\n",
    "#       'text_prompts': text_prompts,\n",
    "#       'image_prompts': image_prompts,\n",
    "#       'clip_guidance_scale': clip_guidance_scale,\n",
    "#       'tv_scale': tv_scale,\n",
    "#       'range_scale': range_scale,\n",
    "#       'sat_scale': sat_scale,\n",
    "#       # 'cutn': cutn,\n",
    "#       'cutn_batches': cutn_batches,\n",
    "#       'max_frames': max_frames,\n",
    "#       'interp_spline': interp_spline,\n",
    "#       # 'rotation_per_frame': rotation_per_frame,\n",
    "#       'init_image': init_image,\n",
    "#       'init_scale': init_scale,\n",
    "#       'skip_steps': skip_steps,\n",
    "#       # 'zoom_per_frame': zoom_per_frame,\n",
    "#       'frames_scale': frames_scale,\n",
    "#       'frames_skip_steps': frames_skip_steps,\n",
    "#       'perlin_init': perlin_init,\n",
    "#       'perlin_mode': perlin_mode,\n",
    "#       'skip_augs': skip_augs,\n",
    "#       'randomize_class': randomize_class,\n",
    "#       'clip_denoised': clip_denoised,\n",
    "#       'clamp_grad': clamp_grad,\n",
    "#       'clamp_max': clamp_max,\n",
    "#       'seed': seed,\n",
    "#       'fuzzy_prompt': fuzzy_prompt,\n",
    "#       'rand_mag': rand_mag,\n",
    "#       'eta': eta,\n",
    "#       'width': width_height[0],\n",
    "#       'height': width_height[1],\n",
    "#       'diffusion_model': diffusion_model,\n",
    "#       'use_secondary_model': use_secondary_model,\n",
    "#       'steps': steps,\n",
    "#       'diffusion_steps': diffusion_steps,\n",
    "#       'diffusion_sampling_mode': diffusion_sampling_mode,\n",
    "#       'ViTB32': ViTB32,\n",
    "#       'ViTB16': ViTB16,\n",
    "#       'ViTL14': ViTL14,\n",
    "#       'ViTL14_336px': ViTL14_336px,\n",
    "#       'RN101': RN101,\n",
    "#       'RN50': RN50,\n",
    "#       'RN50x4': RN50x4,\n",
    "#       'RN50x16': RN50x16,\n",
    "#       'RN50x64': RN50x64,\n",
    "#       'ViTB32_laion2b_e16': ViTB32_laion2b_e16,\n",
    "#       'ViTB32_laion400m_e31': ViTB32_laion400m_e31,\n",
    "#       'ViTB32_laion400m_32': ViTB32_laion400m_32,\n",
    "#       'ViTB32quickgelu_laion400m_e31': ViTB32quickgelu_laion400m_e31,\n",
    "#       'ViTB32quickgelu_laion400m_e32': ViTB32quickgelu_laion400m_e32,\n",
    "#       'ViTB16_laion400m_e31': ViTB16_laion400m_e31,\n",
    "#       'ViTB16_laion400m_e32': ViTB16_laion400m_e32,\n",
    "#       'RN50_yffcc15m': RN50_yffcc15m,\n",
    "#       'RN50_cc12m': RN50_cc12m,\n",
    "#       'RN50_quickgelu_yfcc15m': RN50_quickgelu_yfcc15m,\n",
    "#       'RN50_quickgelu_cc12m': RN50_quickgelu_cc12m,\n",
    "#       'RN101_yfcc15m': RN101_yfcc15m,\n",
    "#       'RN101_quickgelu_yfcc15m': RN101_quickgelu_yfcc15m,\n",
    "#       'cut_overview': str(cut_overview),\n",
    "#       'cut_innercut': str(cut_innercut),\n",
    "#       'cut_ic_pow': str(cut_ic_pow),\n",
    "#       'cut_icgray_p': str(cut_icgray_p),\n",
    "#       'key_frames': key_frames,\n",
    "#       'max_frames': max_frames,\n",
    "#       'angle': angle,\n",
    "#       'zoom': zoom,\n",
    "#       'translation_x': translation_x,\n",
    "#       'translation_y': translation_y,\n",
    "#       'translation_z': translation_z,\n",
    "#       'rotation_3d_x': rotation_3d_x,\n",
    "#       'rotation_3d_y': rotation_3d_y,\n",
    "#       'rotation_3d_z': rotation_3d_z,\n",
    "#       'midas_depth_model': midas_depth_model,\n",
    "#       'midas_weight': midas_weight,\n",
    "#       'near_plane': near_plane,\n",
    "#       'far_plane': far_plane,\n",
    "#       'fov': fov,\n",
    "#       'padding_mode': padding_mode,\n",
    "#       'sampling_mode': sampling_mode,\n",
    "#       'video_init_path':video_init_path,\n",
    "#       'extract_nth_frame':extract_nth_frame,\n",
    "#       'video_init_seed_continuity': video_init_seed_continuity,\n",
    "#       'turbo_mode':turbo_mode,\n",
    "#       'turbo_steps':turbo_steps,\n",
    "#       'turbo_preroll':turbo_preroll,\n",
    "#       'use_horizontal_symmetry':use_horizontal_symmetry,\n",
    "#       'use_vertical_symmetry':use_vertical_symmetry,\n",
    "#       'transformation_percent':transformation_percent,\n",
    "#       #video init settings\n",
    "#       'video_init_steps': video_init_steps,\n",
    "#       'video_init_clip_guidance_scale': video_init_clip_guidance_scale,\n",
    "#       'video_init_tv_scale': video_init_tv_scale,\n",
    "#       'video_init_range_scale': video_init_range_scale,\n",
    "#       'video_init_sat_scale': video_init_sat_scale,\n",
    "#       'video_init_cutn_batches': video_init_cutn_batches,\n",
    "#       'video_init_skip_steps': video_init_skip_steps,\n",
    "#       'video_init_frames_scale': video_init_frames_scale,\n",
    "#       'video_init_frames_skip_steps': video_init_frames_skip_steps,\n",
    "#       #warp settings\n",
    "#       'video_init_flow_warp':video_init_flow_warp,\n",
    "#       'video_init_flow_blend':video_init_flow_blend,\n",
    "#       'video_init_check_consistency':video_init_check_consistency,\n",
    "#       'video_init_blend_mode':video_init_blend_mode\n",
    "#     }\n",
    "#     # print('Settings:', setting_list)\n",
    "#     with open(f\"{batchFolder}/{batch_name}({batchNum})_settings.txt\", \"w+\") as f:   #save settings\n",
    "#         json.dump(setting_list, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"A dragons lair, epic matte painting, concept art, trending on artstation\"\n",
    "]\n",
    "\n",
    "seed = None\n",
    "\n",
    "device='cuda'\n",
    "\n",
    "ddim_steps = 150 # ddim sampling steps\n",
    "ddim_eta = 0.2 # ddim eta (eta=0.0 corresponds to deterministic sampling) (must be 0.0 if using PLMS sampling)\n",
    "unconditional_guidance_scale = 7.5 # unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\n",
    "\n",
    "init_image = None\n",
    "init_noise_strength = 0.4\n",
    "previous_frame_noise_strength = 0.4\n",
    "animation_mode = '3D'\n",
    "perlin_init = False # whether ot not to use perlin noi\n",
    "perlin_mode = 'color' # color or gray\n",
    "\n",
    "fixed_code = True\n",
    "\n",
    "n_iter = 1\n",
    "batch_size = 1\n",
    "\n",
    "start_frame = 0\n",
    "\n",
    "resume_run = False\n",
    "\n",
    "batch_name = 'test'\n",
    "batchNum = 1\n",
    "batchFolder = 'outputs'\n",
    "\n",
    "H = 512 # height\n",
    "W = 512 # width\n",
    "C = 4 # channels\n",
    "f = 8 # downsampling factor\n",
    "\n",
    "precision = 'autocast' # precision to evaluate at (full or autocast)\n",
    "\n",
    "video_init_path = \"init.mp4\"  # @param {type: 'string'}\n",
    "extract_nth_frame = 2  # @param {type: 'number'}\n",
    "persistent_frame_output_in_batch_folder = True  # @param {type: 'boolean'}\n",
    "video_init_seed_continuity = False  # @param {type: 'boolean'}\n",
    "# @markdown #####**Video Optical Flow Settings:**\n",
    "video_init_flow_warp = True  # @param {type: 'boolean'}\n",
    "# Call optical flow from video frames and warp prev frame with flow\n",
    "# @param {type: 'number'} #0 - take next frame, 1 - take prev warped frame\n",
    "video_init_flow_blend = 0.999\n",
    "video_init_check_consistency = False  # Insert param here when ready\n",
    "# @param ['None', 'linear', 'optical flow']\n",
    "video_init_blend_mode = \"optical flow\"\n",
    "# Call optical flow from video frames and warp prev frame with flow\n",
    "if animation_mode == \"Video Input\":\n",
    "    # suggested by Chris the Wizard#8082 at discord\n",
    "    if persistent_frame_output_in_batch_folder or (not is_colab):\n",
    "        videoFramesFolder = f'{batchFolder}/videoFrames'\n",
    "    else:\n",
    "        videoFramesFolder = f'/content/videoFrames'\n",
    "    createPath(videoFramesFolder)\n",
    "    print(f\"Exporting Video Frames (1 every {extract_nth_frame})...\")\n",
    "    try:\n",
    "        for f in pathlib.Path(f'{videoFramesFolder}').glob('*.jpg'):\n",
    "            f.unlink()\n",
    "    except:\n",
    "        print('')\n",
    "    vf = f'select=not(mod(n\\,{extract_nth_frame}))'\n",
    "    if os.path.exists(video_init_path):\n",
    "        subprocess.run(['ffmpeg', '-i', f'{video_init_path}', '-vf', f'{vf}', '-vsync', 'vfr', '-q:v', '2', '-loglevel',\n",
    "                       'error', '-stats', f'{videoFramesFolder}/%04d.jpg'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    else:\n",
    "        print(\n",
    "            f'\\nWARNING!\\n\\nVideo not found: {video_init_path}.\\nPlease check your video path.\\n')\n",
    "    #!ffmpeg -i {video_init_path} -vf {vf} -vsync vfr -q:v 2 -loglevel error -stats {videoFramesFolder}/%04d.jpg\n",
    "\n",
    "\n",
    "# @markdown ---\n",
    "\n",
    "# @markdown ####**2D Animation Settings:**\n",
    "# @markdown `zoom` is a multiplier of dimensions, 1 is no zoom.\n",
    "# @markdown All rotations are provided in degrees.\n",
    "\n",
    "key_frames = True  # @param {type:\"boolean\"}\n",
    "max_frames = 10000  # @param {type:\"number\"}\n",
    "\n",
    "if animation_mode == \"Video Input\":\n",
    "    max_frames = len(glob(f'{videoFramesFolder}/*.jpg'))\n",
    "\n",
    "# Do not change, currently will not look good. param ['Linear','Quadratic','Cubic']{type:\"string\"}\n",
    "interp_spline = 'Linear'\n",
    "angle = \"0:(0)\"  # @param {type:\"string\"}\n",
    "zoom = \"0: (1), 10: (1.0)\"  # @param {type:\"string\"}\n",
    "translation_x = \"0: (0)\"  # @param {type:\"string\"}\n",
    "translation_y = \"0: (0)\"  # @param {type:\"string\"}\n",
    "translation_z = \"0: (7.0)\"  # @param {type:\"string\"}\n",
    "rotation_3d_x = \"0: (0.2)\"  # @param {type:\"string\"}\n",
    "rotation_3d_y = \"0: (0)\"  # @param {type:\"string\"}\n",
    "rotation_3d_z = \"0: (0.0)\"  # @param {type:\"string\"}\n",
    "midas_depth_model = \"dpt_large\"  # @param {type:\"string\"}\n",
    "midas_weight = 0.3  # @param {type:\"number\"}\n",
    "near_plane = 200  # @param {type:\"number\"}\n",
    "far_plane = 10000  # @param {type:\"number\"}\n",
    "fov = 40  # @param {type:\"number\"}\n",
    "padding_mode = 'border'  # @param {type:\"string\"}\n",
    "sampling_mode = 'bicubic'  # @param {type:\"string\"}\n",
    "\n",
    "# ======= TURBO MODE\n",
    "# @markdown ---\n",
    "# @markdown ####**Turbo Mode (3D anim only):**\n",
    "# @markdown (Starts after frame 10,) skips diffusion steps and just uses depth map to warp images for skipped frames.\n",
    "# @markdown Speeds up rendering by 2x-4x, and may improve image coherence between frames.\n",
    "# @markdown For different settings tuned for Turbo Mode, refer to the original Disco-Turbo Github: https://github.com/zippy731/disco-diffusion-turbo\n",
    "\n",
    "turbo_mode = False  # @param {type:\"boolean\"}\n",
    "turbo_steps = \"3\"  # @param [\"2\",\"3\",\"4\",\"5\",\"6\"] {type:\"string\"}\n",
    "turbo_preroll = 10  # frames\n",
    "\n",
    "# insist turbo be used only w 3d anim.\n",
    "if turbo_mode and animation_mode != '3D':\n",
    "    print('=====')\n",
    "    print('Turbo mode only available with 3D animations. Disabling Turbo.')\n",
    "    print('=====')\n",
    "    turbo_mode = False\n",
    "\n",
    "# @markdown ---\n",
    "\n",
    "# @markdown ####**Coherency Settings:**\n",
    "# @markdown `frame_scale` tries to guide the new frame to looking like the old one. A good default is 1500.\n",
    "frames_scale = 1500  # @param{type: 'integer'}\n",
    "# @markdown `frame_skip_steps` will blur the previous frame - higher values will flicker less but struggle to add enough new detail to zoom into.\n",
    "# @param ['40%', '50%', '60%', '70%', '80%'] {type: 'string'}\n",
    "frames_skip_steps = '60%'\n",
    "\n",
    "# @markdown ####**Video Init Coherency Settings:**\n",
    "# @markdown `frame_scale` tries to guide the new frame to looking like the old one. A good default is 1500.\n",
    "video_init_frames_scale = 15000  # @param{type: 'integer'}\n",
    "# @markdown `frame_skip_steps` will blur the previous frame - higher values will flicker less but struggle to add enough new detail to zoom into.\n",
    "# @param ['40%', '50%', '60%', '70%', '80%'] {type: 'string'}\n",
    "video_init_frames_skip_steps = '70%'\n",
    "\n",
    "# ======= VR MODE\n",
    "# @markdown ---\n",
    "# @markdown ####**VR Mode (3D anim only):**\n",
    "# @markdown Enables stereo rendering of left/right eye views (supporting Turbo) which use a different (fish-eye) camera projection matrix.\n",
    "# @markdown Note the images you're prompting will work better if they have some inherent wide-angle aspect\n",
    "# @markdown The generated images will need to be combined into left/right videos. These can then be stitched into the VR180 format.\n",
    "# @markdown Google made the VR180 Creator tool but subsequently stopped supporting it. It's available for download in a few places including https://www.patrickgrunwald.de/vr180-creator-download\n",
    "# @markdown The tool is not only good for stitching (videos and photos) but also for adding the correct metadata into existing videos, which is needed for services like YouTube to identify the format correctly.\n",
    "# @markdown Watching YouTube VR videos isn't necessarily the easiest depending on your headset. For instance Oculus have a dedicated media studio and store which makes the files easier to access on a Quest https://creator.oculus.com/manage/mediastudio/\n",
    "# @markdown\n",
    "# @markdown The command to get ffmpeg to concat your frames for each eye is in the form: `ffmpeg -framerate 15 -i frame_%4d_l.png l.mp4` (repeat for r)\n",
    "\n",
    "vr_mode = False  # @param {type:\"boolean\"}\n",
    "# @markdown `vr_eye_angle` is the y-axis rotation of the eyes towards the center\n",
    "vr_eye_angle = 0.5  # @param{type:\"number\"}\n",
    "# @markdown interpupillary distance (between the eyes)\n",
    "vr_ipd = 5.0  # @param{type:\"number\"}\n",
    "\n",
    "# insist VR be used only w 3d anim.\n",
    "if vr_mode and animation_mode != '3D':\n",
    "    print('=====')\n",
    "    print('VR mode only available with 3D animations. Disabling VR.')\n",
    "    print('=====')\n",
    "    vr_mode = False\n",
    "\n",
    "\n",
    "def parse_key_frames(string, prompt_parser=None):\n",
    "    \"\"\"Given a string representing frame numbers paired with parameter values at that frame,\n",
    "    return a dictionary with the frame numbers as keys and the parameter values as the values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    string: string\n",
    "        Frame numbers paired with parameter values at that frame number, in the format\n",
    "        'framenumber1: (parametervalues1), framenumber2: (parametervalues2), ...'\n",
    "    prompt_parser: function or None, optional\n",
    "        If provided, prompt_parser will be applied to each string of parameter values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Frame numbers as keys, parameter values at that frame number as values\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    RuntimeError\n",
    "        If the input string does not match the expected format.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\")\n",
    "    {10: 'Apple: 1| Orange: 0', 20: 'Apple: 0| Orange: 1| Peach: 1'}\n",
    "\n",
    "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\", prompt_parser=lambda x: x.lower()))\n",
    "    {10: 'apple: 1| orange: 0', 20: 'apple: 0| orange: 1| peach: 1'}\n",
    "    \"\"\"\n",
    "    import re\n",
    "    pattern = r'((?P<frame>[0-9]+):[\\s]*[\\(](?P<param>[\\S\\s]*?)[\\)])'\n",
    "    frames = dict()\n",
    "    for match_object in re.finditer(pattern, string):\n",
    "        frame = int(match_object.groupdict()['frame'])\n",
    "        param = match_object.groupdict()['param']\n",
    "        if prompt_parser:\n",
    "            frames[frame] = prompt_parser(param)\n",
    "        else:\n",
    "            frames[frame] = param\n",
    "\n",
    "    if frames == {} and len(string) != 0:\n",
    "        raise RuntimeError('Key Frame string not correctly formatted')\n",
    "    return frames\n",
    "\n",
    "\n",
    "def get_inbetweens(key_frames, integer=False):\n",
    "    \"\"\"Given a dict with frame numbers as keys and a parameter value as values,\n",
    "    return a pandas Series containing the value of the parameter at every frame from 0 to max_frames.\n",
    "    Any values not provided in the input dict are calculated by linear interpolation between\n",
    "    the values of the previous and next provided frames. If there is no previous provided frame, then\n",
    "    the value is equal to the value of the next provided frame, or if there is no next provided frame,\n",
    "    then the value is equal to the value of the previous provided frame. If no frames are provided,\n",
    "    all frame values are NaN.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key_frames: dict\n",
    "        A dict with integer frame numbers as keys and numerical values of a particular parameter as values.\n",
    "    integer: Bool, optional\n",
    "        If True, the values of the output series are converted to integers.\n",
    "        Otherwise, the values are floats.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        A Series with length max_frames representing the parameter values for each frame.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> max_frames = 5\n",
    "    >>> get_inbetweens({1: 5, 3: 6})\n",
    "    0    5.0\n",
    "    1    5.0\n",
    "    2    5.5\n",
    "    3    6.0\n",
    "    4    6.0\n",
    "    dtype: float64\n",
    "\n",
    "    >>> get_inbetweens({1: 5, 3: 6}, integer=True)\n",
    "    0    5\n",
    "    1    5\n",
    "    2    5\n",
    "    3    6\n",
    "    4    6\n",
    "    dtype: int64\n",
    "    \"\"\"\n",
    "    key_frame_series = pd.Series([np.nan for a in range(max_frames)])\n",
    "\n",
    "    for i, value in key_frames.items():\n",
    "        key_frame_series[i] = value\n",
    "    key_frame_series = key_frame_series.astype(float)\n",
    "\n",
    "    interp_method = interp_spline\n",
    "\n",
    "    if interp_method == 'Cubic' and len(key_frames.items()) <= 3:\n",
    "        interp_method = 'Quadratic'\n",
    "\n",
    "    if interp_method == 'Quadratic' and len(key_frames.items()) <= 2:\n",
    "        interp_method = 'Linear'\n",
    "\n",
    "    key_frame_series[0] = key_frame_series[key_frame_series.first_valid_index()]\n",
    "    key_frame_series[max_frames -\n",
    "                     1] = key_frame_series[key_frame_series.last_valid_index()]\n",
    "    # key_frame_series = key_frame_series.interpolate(method=intrp_method,order=1, limit_direction='both')\n",
    "    key_frame_series = key_frame_series.interpolate(\n",
    "        method=interp_method.lower(), limit_direction='both')\n",
    "    if integer:\n",
    "        return key_frame_series.astype(int)\n",
    "    return key_frame_series\n",
    "\n",
    "if key_frames:\n",
    "    try:\n",
    "        angle_series = get_inbetweens(parse_key_frames(angle))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `angle` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `angle` as \"\n",
    "            f'\"0: ({angle})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        angle = f\"0: ({angle})\"\n",
    "        angle_series = get_inbetweens(parse_key_frames(angle))\n",
    "\n",
    "    try:\n",
    "        zoom_series = get_inbetweens(parse_key_frames(zoom))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `zoom` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `zoom` as \"\n",
    "            f'\"0: ({zoom})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        zoom = f\"0: ({zoom})\"\n",
    "        zoom_series = get_inbetweens(parse_key_frames(zoom))\n",
    "\n",
    "    try:\n",
    "        translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `translation_x` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `translation_x` as \"\n",
    "            f'\"0: ({translation_x})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        translation_x = f\"0: ({translation_x})\"\n",
    "        translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n",
    "\n",
    "    try:\n",
    "        translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `translation_y` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `translation_y` as \"\n",
    "            f'\"0: ({translation_y})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        translation_y = f\"0: ({translation_y})\"\n",
    "        translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n",
    "\n",
    "    try:\n",
    "        translation_z_series = get_inbetweens(parse_key_frames(translation_z))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `translation_z` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `translation_z` as \"\n",
    "            f'\"0: ({translation_z})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        translation_z = f\"0: ({translation_z})\"\n",
    "        translation_z_series = get_inbetweens(parse_key_frames(translation_z))\n",
    "\n",
    "    try:\n",
    "        rotation_3d_x_series = get_inbetweens(parse_key_frames(rotation_3d_x))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `rotation_3d_x` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `rotation_3d_x` as \"\n",
    "            f'\"0: ({rotation_3d_x})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        rotation_3d_x = f\"0: ({rotation_3d_x})\"\n",
    "        rotation_3d_x_series = get_inbetweens(parse_key_frames(rotation_3d_x))\n",
    "\n",
    "    try:\n",
    "        rotation_3d_y_series = get_inbetweens(parse_key_frames(rotation_3d_y))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `rotation_3d_y` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `rotation_3d_y` as \"\n",
    "            f'\"0: ({rotation_3d_y})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        rotation_3d_y = f\"0: ({rotation_3d_y})\"\n",
    "        rotation_3d_y_series = get_inbetweens(parse_key_frames(rotation_3d_y))\n",
    "\n",
    "    try:\n",
    "        rotation_3d_z_series = get_inbetweens(parse_key_frames(rotation_3d_z))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `rotation_3d_z` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `rotation_3d_z` as \"\n",
    "            f'\"0: ({rotation_3d_z})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        rotation_3d_z = f\"0: ({rotation_3d_z})\"\n",
    "        rotation_3d_z_series = get_inbetweens(parse_key_frames(rotation_3d_z))\n",
    "\n",
    "else:\n",
    "    angle = float(angle)\n",
    "    zoom = float(zoom)\n",
    "    translation_x = float(translation_x)\n",
    "    translation_y = float(translation_y)\n",
    "    translation_z = float(translation_z)\n",
    "    rotation_3d_x = float(rotation_3d_x)\n",
    "    rotation_3d_y = float(rotation_3d_y)\n",
    "    rotation_3d_z = float(rotation_3d_z)\n",
    "\n",
    "\n",
    "if seed is None:\n",
    "    seed = np.random.randint(np.iinfo(np.int32).max)\n",
    "\n",
    "args = {\n",
    "    'seed': seed,\n",
    "    'animation_mode': animation_mode,\n",
    "    'init_image': init_image,\n",
    "    'perlin_init': perlin_init,\n",
    "    'perlin_mode': perlin_mode,\n",
    "    'H': H,\n",
    "    'W': W,\n",
    "    'C': C,\n",
    "    'f': f,\n",
    "    'start_frame': start_frame,\n",
    "    'max_frames': max_frames,\n",
    "    'n_iter': n_iter,\n",
    "    'batch_size': batch_size,\n",
    "    'init_noise_strength': init_noise_strength,\n",
    "    'previous_frame_noise_strength': previous_frame_noise_strength,\n",
    "    'ddim_steps': ddim_steps,\n",
    "    'ddim_eta': ddim_eta,\n",
    "    'unconditional_guidance_scale': unconditional_guidance_scale,\n",
    "    'fixed_code': fixed_code,\n",
    "    'key_frames': key_frames,\n",
    "    'angle_series': angle_series,\n",
    "    'zoom_series': zoom_series,\n",
    "    'translation_x_series': translation_x_series,\n",
    "    'translation_y_series': translation_y_series,\n",
    "    'translation_z_series': translation_z_series,\n",
    "    'rotation_3d_x_series': rotation_3d_x_series,\n",
    "    'rotation_3d_y_series': rotation_3d_y_series,\n",
    "    'rotation_3d_z_series': rotation_3d_z_series,\n",
    "    'near_plane': near_plane,\n",
    "    'far_plane': far_plane,\n",
    "    'fov': fov,\n",
    "    'padding_mode': padding_mode,\n",
    "    'sampling_mode': sampling_mode,\n",
    "    'midas_weight': midas_weight\n",
    "}\n",
    "\n",
    "args = SimpleNamespace(**args)\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "start_code = None\n",
    "if fixed_code:\n",
    "    start_code = torch.randn([batch_size, C, H // f, W // f], device=device)\n",
    "\n",
    "precision_scope = autocast if precision==\"autocast\" else nullcontext\n",
    "data = list(chunk(prompts, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSLATION_SCALE = 1.0/200.0\n",
    "stop_on_next_loop = False\n",
    "midas_model, midas_transform, midas_net_w, midas_net_h, midas_resize_mode, midas_normalization = init_midas_depth_model()\n",
    "for frame_num in tqdm(range(args.start_frame, args.max_frames), desc='Frames'):\n",
    "    if stop_on_next_loop:\n",
    "        break\n",
    "\n",
    "    if frame_num == 0:\n",
    "        init_image = args.init_image\n",
    "\n",
    "    if args.animation_mode == \"2D\":\n",
    "        if args.key_frames:\n",
    "            angle = args.angle_series[frame_num]\n",
    "            zoom = args.zoom_series[frame_num]\n",
    "            translation_x = args.translation_x_series[frame_num]\n",
    "            translation_y = args.translation_y_series[frame_num]\n",
    "            print(\n",
    "                f'angle: {angle}',\n",
    "                f'zoom: {zoom}',\n",
    "                f'translation_x: {translation_x}',\n",
    "                f'translation_y: {translation_y}',\n",
    "            )\n",
    "\n",
    "        if frame_num > 0:\n",
    "            # seed += 1\n",
    "            if resume_run and frame_num == start_frame:\n",
    "                img_0 = cv2.imread(\n",
    "                    batchFolder+f\"/{batch_name}({batchNum})_{start_frame-1:04}.png\")\n",
    "            else:\n",
    "                img_0 = cv2.imread('prevFrame.png')\n",
    "                center = (1*img_0.shape[1]//2, 1*img_0.shape[0]//2)\n",
    "                trans_mat = np.float32(\n",
    "                    [[1, 0, translation_x],\n",
    "                     [0, 1, translation_y]]\n",
    "                )\n",
    "                rot_mat = cv2.getRotationMatrix2D(center, angle, zoom)\n",
    "                trans_mat = np.vstack([trans_mat, [0, 0, 1]])\n",
    "                rot_mat = np.vstack([rot_mat, [0, 0, 1]])\n",
    "                transformation_matrix = np.matmul(rot_mat, trans_mat)\n",
    "                img_0 = cv2.warpPerspective(\n",
    "                    img_0,\n",
    "                    transformation_matrix,\n",
    "                    (img_0.shape[1], img_0.shape[0]),\n",
    "                    borderMode=cv2.BORDER_WRAP\n",
    "                )\n",
    "\n",
    "            cv2.imwrite('prevFrameScaled.png', img_0)\n",
    "            init_image = 'prevFrameScaled.png'\n",
    "        \n",
    "    if args.animation_mode == \"3D\":\n",
    "        if frame_num > 0:\n",
    "            # seed += 1\n",
    "            if resume_run and frame_num == start_frame:\n",
    "                img_filepath = batchFolder + \\\n",
    "                    f\"/{batch_name}({batchNum})_{start_frame-1:04}.png\"\n",
    "                if turbo_mode and frame_num > turbo_preroll:\n",
    "                    shutil.copyfile(img_filepath, 'oldFrameScaled.png')\n",
    "            else:\n",
    "                img_filepath = 'prevFrame.png'\n",
    "\n",
    "            next_step_pil = do_3d_step(\n",
    "                img_filepath, frame_num, midas_model, midas_transform)\n",
    "            next_step_pil.save('prevFrameScaled.png')\n",
    "\n",
    "            # Turbo mode - skip some diffusions, use 3d morph for clarity and to save time\n",
    "            if turbo_mode:\n",
    "                if frame_num == turbo_preroll:  # start tracking oldframe\n",
    "                    # stash for later blending\n",
    "                    next_step_pil.save('oldFrameScaled.png')\n",
    "                elif frame_num > turbo_preroll:\n",
    "                    # set up 2 warped image sequences, old & new, to blend toward new diff image\n",
    "                    old_frame = do_3d_step(\n",
    "                        'oldFrameScaled.png', frame_num, midas_model, midas_transform)\n",
    "                    old_frame.save('oldFrameScaled.png')\n",
    "                    if frame_num % int(turbo_steps) != 0:\n",
    "                        print(\n",
    "                            'turbo skip this frame: skipping clip diffusion steps')\n",
    "                        filename = f'{batch_name}({batchNum})_{frame_num:04}.png'\n",
    "                        blend_factor = (\n",
    "                            (frame_num % int(turbo_steps))+1)/int(turbo_steps)\n",
    "                        print(\n",
    "                            'turbo skip this frame: skipping clip diffusion steps and saving blended frame')\n",
    "                        # this is already updated..\n",
    "                        newWarpedImg = cv2.imread('prevFrameScaled.png')\n",
    "                        oldWarpedImg = cv2.imread('oldFrameScaled.png')\n",
    "                        blendedImage = cv2.addWeighted(\n",
    "                            newWarpedImg, blend_factor, oldWarpedImg, 1-blend_factor, 0.0)\n",
    "                        cv2.imwrite(\n",
    "                            f'{batchFolder}/{filename}', blendedImage)\n",
    "                        # save it also as prev_frame to feed next iteration\n",
    "                        next_step_pil.save(f'{img_filepath}')\n",
    "                        if vr_mode:\n",
    "                            generate_eye_views(\n",
    "                                TRANSLATION_SCALE, batchFolder, filename, frame_num, midas_model, midas_transform)\n",
    "                        continue\n",
    "                    else:\n",
    "                        # if not a skip frame, will run diffusion and need to blend.\n",
    "                        oldWarpedImg = cv2.imread('prevFrameScaled.png')\n",
    "                        # swap in for blending later\n",
    "                        cv2.imwrite(f'oldFrameScaled.png', oldWarpedImg)\n",
    "                        print('clip/diff this frame - generate clip diff image')\n",
    "\n",
    "            init_image = 'prevFrameScaled.png'\n",
    "\n",
    "        if args.animation_mode == \"Video Input\":\n",
    "            init_scale = args.video_init_frames_scale\n",
    "            skip_steps = args.calc_frames_skip_steps\n",
    "            if not video_init_seed_continuity:\n",
    "                seed += 1\n",
    "            if video_init_flow_warp:\n",
    "                if frame_num == 0:\n",
    "                    skip_steps = args.video_init_skip_steps\n",
    "                    init_image = f'{videoFramesFolder}/{frame_num+1:04}.jpg'\n",
    "                if frame_num > 0:\n",
    "                    prev = PIL.Image.open(\n",
    "                        batchFolder+f\"/{batch_name}({batchNum})_{frame_num-1:04}.png\")\n",
    "\n",
    "                    frame1_path = f'{videoFramesFolder}/{frame_num:04}.jpg'\n",
    "                    frame2 = PIL.Image.open(\n",
    "                        f'{videoFramesFolder}/{frame_num+1:04}.jpg')\n",
    "                    flo_path = f\"/{flo_folder}/{frame1_path.split('/')[-1]}.npy\"\n",
    "\n",
    "                    init_image = 'warped.png'\n",
    "                    print(video_init_flow_blend)\n",
    "                    weights_path = None\n",
    "                    if video_init_check_consistency:\n",
    "                        # TBD\n",
    "                        pass\n",
    "\n",
    "                    warp(prev, frame2, flo_path, blend=video_init_flow_blend,\n",
    "                         weights_path=weights_path).save(init_image)\n",
    "\n",
    "            else:\n",
    "                init_image = f'{videoFramesFolder}/{frame_num+1:04}.jpg'\n",
    "\n",
    "    seed_everything(args.seed+frame_num)\n",
    "\n",
    "    init = None\n",
    "    if init_image is not None:\n",
    "        init = load_img(init_image, (args.W, args.H)).to(device)\n",
    "        init = repeat(init, '1 ... -> b ...', b=args.batch_size)\n",
    "        init_latent = model.get_first_stage_encoding(\n",
    "            model.encode_first_stage(init))  # move to latent space\n",
    "\n",
    "        sampler.make_schedule(ddim_num_steps=args.ddim_steps,\n",
    "                              ddim_eta=args.ddim_eta, verbose=False)\n",
    "\n",
    "        noise_strength = args.init_noise_strength if frame_num == 0 else args.previous_frame_noise_strength\n",
    "        assert 0. <= noise_strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "        t_enc = int(noise_strength * args.ddim_steps)\n",
    "        print(f\"target t_enc is {t_enc} steps\")\n",
    "\n",
    "    if args.perlin_init:\n",
    "        if args.perlin_mode == 'color':\n",
    "            init = create_perlin_noise(\n",
    "                [1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "            init2 = create_perlin_noise(\n",
    "                [1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
    "        elif args.perlin_mode == 'gray':\n",
    "            init = create_perlin_noise(\n",
    "                [1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
    "            init2 = create_perlin_noise(\n",
    "                [1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "        else:\n",
    "            init = create_perlin_noise(\n",
    "                [1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "            init2 = create_perlin_noise(\n",
    "                [1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "        init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(\n",
    "            2).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "        del init2\n",
    "\n",
    "    print(f'Frame {frame_num} Prompt: {prompts}')\n",
    "\n",
    "    image_display = Output()\n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                tic = time.time()\n",
    "                all_samples = list()\n",
    "                print('')\n",
    "                display(image_display)\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                if perlin_init:\n",
    "                    init = regen_perlin()\n",
    "                    start_code = init\n",
    "\n",
    "                for prompts in tqdm(data, desc=\"data\"):\n",
    "                    uc = None\n",
    "                    if unconditional_guidance_scale != 1.0:\n",
    "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "                    c = model.get_learned_conditioning(prompts)\n",
    "\n",
    "                    if init_image == None or init_image == '' or init_image == [] or init_image == ['']:\n",
    "                        shape = [args.C, args.H // args.f, args.W // args.f]\n",
    "                        samples_ddim, _ = sampler.sample(S=args.ddim_steps,\n",
    "                                                         conditioning=c,\n",
    "                                                         batch_size=args.batch_size,\n",
    "                                                         shape=shape,\n",
    "                                                         verbose=False,\n",
    "                                                         unconditional_guidance_scale=args.unconditional_guidance_scale,\n",
    "                                                         unconditional_conditioning=uc,\n",
    "                                                         eta=args.ddim_eta,\n",
    "                                                         x_T=start_code)\n",
    "\n",
    "                    else:\n",
    "                        # encode (scaled latent)\n",
    "                        z_enc = sampler.stochastic_encode(\n",
    "                            init_latent, torch.tensor([t_enc]*args.batch_size).to(device))\n",
    "                        # decode it\n",
    "                        samples_ddim = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=args.unconditional_guidance_scale,\n",
    "                                                      unconditional_conditioning=uc)\n",
    "\n",
    "                    x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                    x_samples_ddim = torch.clamp(\n",
    "                        (x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                    for x_sample in x_samples_ddim:\n",
    "                        x_sample = 255. * \\\n",
    "                            rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                        img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                        display(img)\n",
    "\n",
    "                        if args.animation_mode != \"None\":\n",
    "                            filename = f'{batch_name}({batchNum})_{frame_num}.png'\n",
    "                            img.save('prevFrame.png')\n",
    "                            img.save(f'{batchFolder}/{filename}')\n",
    "                            # if frame_num == 0:\n",
    "                            #     save_settings()\n",
    "                            if args.animation_mode == \"3D\":\n",
    "                                # If turbo, save a blended image\n",
    "                                if turbo_mode and frame_num > 0:\n",
    "                                    # Mix new image with prevFrameScaled\n",
    "                                    blend_factor = (1)/int(turbo_steps)\n",
    "                                    # This is already updated..\n",
    "                                    newFrame = cv2.imread('prevFrame.png')\n",
    "                                    prev_frame_warped = cv2.imread(\n",
    "                                        'prevFrameScaled.png')\n",
    "                                    blendedImage = cv2.addWeighted(\n",
    "                                        newFrame, blend_factor, prev_frame_warped, (1-blend_factor), 0.0)\n",
    "                                    cv2.imwrite(\n",
    "                                        f'{batchFolder}/{filename}', blendedImage)\n",
    "                                else:\n",
    "                                    img.save(f'{batchFolder}/{filename}')\n",
    "\n",
    "                                if vr_mode:\n",
    "                                    generate_eye_views(\n",
    "                                        TRANSLATION_SCALE, batchFolder, filename, frame_num, midas_model, midas_transform)\n",
    "                toc = time.time()\n",
    "\n",
    "    # print(f\"Your samples are ready and waiting for you here: \\n{outdir}\\n\"\n",
    "    #       f\"\\nTime elapsed: {round(toc - tic, 2)} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image, Mask -> Image (inpainting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "latent-imagenet-diffusion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "96115ba3d133a87032cd70b9a322d48617443b59c68527f3d6b41f2029e5d2d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
