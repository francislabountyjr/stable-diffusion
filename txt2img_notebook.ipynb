{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "cNHvQBhzyXCI",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0a79e979-8484-4c62-96d9-7c79b1835162",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# necessary packages and repos - may have to restart runtime/reset kernel after running\n",
    "!git clone https://github.com/CompVis/latent-diffusion.git\n",
    "!git clone https://github.com/CompVis/taming-transformers\n",
    "!git clone https://github.com/crowsonkb/k-diffusion\n",
    "!pip install -e ./taming-transformers\n",
    "!pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install transformers kornia imageio imageio_ffmpeg pillow scikit-image jsonmerge clean-fid resize-right torchdiffeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SD model\n",
    "!mkdir -p models/ldm/stable-diffusion-v1/\n",
    "# when weights become available drop link here\n",
    "!wget -O models/ldm/stable-diffusion-v1/model.ckpt LINK_TO_WEIGHTS_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fallback LAION 400M model\n",
    "!mkdir -p models/ldm/text2img-large/\n",
    "!wget -O models/ldm/text2img-large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import imageio\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange, repeat\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import contextmanager, nullcontext\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "\n",
    "import sys\n",
    "sys.path.append(f'{os.path.abspath(os.getcwd())}/k-diffusion')\n",
    "import k_diffusion as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "use_laion400m = False\n",
    "use_plms = False\n",
    "use_k_lms = False\n",
    "\n",
    "if use_laion400m:\n",
    "    print(\"Falling back to LAION 400M model...\")\n",
    "    config_location = \"configs/latent-diffusion/txt2img-1p4B-eval.yaml\"\n",
    "    model_location = \"models/ldm/text2img-large/model.ckpt\"\n",
    "    outdir = \"outputs/txt2img-samples-laion400m\"\n",
    "else:\n",
    "    print(\"Using Stable Diffusion model...\")\n",
    "    config_location = 'configs/stable-diffusion/v1-inference.yaml'\n",
    "    model_location = 'models/ldm/stable-diffusion-v1/model.ckpt'\n",
    "    outdir = 'outputs/txt2img-samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "fnGwQRhtyBhb"
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def load_model_from_config(config, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt)#, map_location=\"cpu\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_model():\n",
    "    config = OmegaConf.load(config_location)\n",
    "    model = load_model_from_config(config, model_location)\n",
    "    return model\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "def load_img(path, size):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    w, h = image.size\n",
    "    print(f\"loaded input image of size ({w}, {h}) and resized to ({size[0]}, {size[1]}) from {path}\")\n",
    "    w, h = size #map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
    "    image = image.resize((w, h), resample=Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.*image - 1.\n",
    "\n",
    "def slerp(val, low, high):\n",
    "    low_norm = low/torch.norm(low)\n",
    "    high_norm = high/torch.norm(high)\n",
    "    omega = torch.acos((low_norm*high_norm).sum())\n",
    "    so = torch.sin(omega)\n",
    "    res = (torch.sin((1.0-val)*omega)/so)*low + (torch.sin(val*omega)/so) * high\n",
    "    return res\n",
    "\n",
    "def get_slerp_vectors(start, end, frames=20):\n",
    "    out = torch.Tensor(frames, start.shape[0]).to(device)\n",
    "    factor = 1.0 / (frames - 1)\n",
    "    for i in range(frames):\n",
    "        out[i] = slerp(factor*i, start, end)\n",
    "    return out\n",
    "\n",
    "def get_conditioning_vector(prompt):\n",
    "    split_prompt = re.split(r'::([-\\d.]+)', prompt)\n",
    "    if len(split_prompt) == 0:\n",
    "        raise(AttributeError('not a valid prompt'))\n",
    "    elif len(split_prompt) == 1:\n",
    "        return model.get_learned_conditioning(prompt)\n",
    "\n",
    "    split_prompt = iter(split_prompt)\n",
    "    c_tensors = []\n",
    "    weights = []\n",
    "    for text in split_prompt:\n",
    "        if text == '':\n",
    "            continue\n",
    "        text = text.strip()\n",
    "        try:\n",
    "            weight = float(next(split_prompt))\n",
    "            weights.append(weight)\n",
    "            c_tensors.append(model.get_learned_conditioning(text))\n",
    "        except:\n",
    "            print(f'Prompt: \"{text}\" dropped due to invalid weight') \n",
    "            continue\n",
    "    abs_weight = [abs(weight) for weight in weights] if average_weights else weights\n",
    "\n",
    "    c = c_tensors[0] * weights[0]\n",
    "    for c_tensor, weight in zip(c_tensors[1:], weights[1:]):\n",
    "        c += c_tensor * weight\n",
    "    c = c/sum(abs_weight)\n",
    "    return c\n",
    "\n",
    "def get_starting_code_and_conditioning_vector(seed, prompt):\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(-np.iinfo(np.int32).max, np.iinfo(np.int32).max)\n",
    "    seed_everything(seed)\n",
    "    start_code = torch.randn([1, C, H // f, W // f], device=device)\n",
    "    c = get_conditioning_vector(prompt)\n",
    "    return (c, start_code)\n",
    "\n",
    "def unflatten(l, n):\n",
    "    res = []\n",
    "    t = l[:]\n",
    "    while len(t) > 0:\n",
    "        res.append(t[:n])\n",
    "        t = t[n:]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "BPnyd-XUKbfE",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0fcd10e4-0df2-4ab9-cbf5-f08f4902c954",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up model and sampler\n",
    "model = get_model()\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "if use_plms:\n",
    "    print(\"Warning: img2img and disco style animations not compatible with PLMSSampler\")\n",
    "    sampler = PLMSSampler(model)\n",
    "else:\n",
    "    sampler = DDIMSampler(model)\n",
    "\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "sample_path = os.path.join(outdir, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "grid_count = len(os.listdir(outdir)) - 1\n",
    "\n",
    "if use_k_lms:\n",
    "    class CFGDenoiser(nn.Module):\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.inner_model = model\n",
    "\n",
    "        def forward(self, x, sigma, uncond, cond, cond_scale):\n",
    "            x_in = torch.cat([x] * 2)\n",
    "            sigma_in = torch.cat([sigma] * 2)\n",
    "            cond_in = torch.cat([uncond, cond])\n",
    "            uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
    "            return uncond + (cond - uncond) * cond_scale\n",
    "        \n",
    "    model_wrap = K.external.CompVisDenoiser(model)\n",
    "    sigma_min, sigma_max = model_wrap.sigmas[0].item(), model_wrap.sigmas[-1].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text -> Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "prompts = [\n",
    "    \"joe biden::1 happy::0.5\"\n",
    "] # list of string prompts - negative weights don't work well, needs investigating\n",
    "seed = 741 # seed for reproducible generations - use None for random seed\n",
    "average_weights = True # If using prompt weights, whether or not to average them\n",
    "\n",
    "init_image_location = \"\" # file location of init image (use a blank string if none desired)\n",
    "init_noise_strength = 0.75 # how much to noise init image (0-1.0 where 1.0 is full destruction of init image information)\n",
    "\n",
    "ddim_steps = 200 # ddim sampling steps\n",
    "ddim_eta = 0.0 # ddim eta (eta=0.0 corresponds to deterministic sampling) (must be 0.0 if using PLMS/k_lms sampling)\n",
    "unconditional_guidance_scale = 7.5 # unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\n",
    "\n",
    "precision = 'autocast' # precision to evaluate at (full or autocast)\n",
    "\n",
    "n_iter = 1 # how many sample iterations\n",
    "batch_size = 1 # how many samples to generate per prompt\n",
    "n_rows = 0 # rows in the grid (will use batch_size if set to 0)\n",
    "\n",
    "fixed_code = True # if enabled, uses the same starting code across samples\n",
    "skip_grid = True # do not save a grid, only individual samples\n",
    "skip_save = False # do not save individual samples\n",
    "show_images = True # whether or not to show images after generation\n",
    "\n",
    "H = 512 # height\n",
    "W = 512 # width\n",
    "C = 4 # channels\n",
    "f = 8 # downsampling factor\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "if n_rows == 0:\n",
    "    n_rows = batch_size\n",
    "\n",
    "if seed is None:\n",
    "    seed = np.random.randint(-np.iinfo(np.int32).max, np.iinfo(np.int32).max)\n",
    "seed_everything(seed)\n",
    "\n",
    "start_code = None\n",
    "if fixed_code:\n",
    "    start_code = torch.randn([batch_size, C, H // f, W // f], device=device)\n",
    "\n",
    "precision_scope = autocast if precision==\"autocast\" else nullcontext\n",
    "data = chunk(prompts, batch_size)\n",
    "\n",
    "if init_image_location != \"\":\n",
    "    assert os.path.isfile(init_image_location)\n",
    "    init_image = load_img(init_image_location, ()).to(device)\n",
    "    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "\n",
    "    sampler.make_schedule(ddim_num_steps=ddim_steps, ddim_eta=ddim_eta, verbose=False)\n",
    "\n",
    "    assert 0. <= init_noise_strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "    t_enc = int(init_noise_strength * ddim_steps)\n",
    "    print(f\"target t_enc is {t_enc} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run generation(s)\n",
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            tic = time.time()\n",
    "            all_samples = list()\n",
    "            for n in trange(n_iter, desc=\"Sampling\"):\n",
    "                for prompts in tqdm(data, desc=\"data\"):\n",
    "                    uc = None\n",
    "                    if unconditional_guidance_scale != 1.0:\n",
    "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "                    c = torch.cat([get_conditioning_vector(prompt) for prompt in prompts])\n",
    "\n",
    "                    if len(init_image_location) == 0 or init_image_location == '':\n",
    "                        shape = [C, H // f, W // f]\n",
    "                        if use_k_lms:\n",
    "                            sigmas = model_wrap.get_sigmas(ddim_steps)\n",
    "                            torch.manual_seed(seed) # changes manual seeding procedure\n",
    "                            x = torch.randn([batch_size, *shape], device=device) * sigmas[0] # for GPU draw\n",
    "                            model_wrap_cfg = CFGDenoiser(model_wrap)\n",
    "                            extra_args = {'cond': c, 'uncond': uc, 'cond_scale': unconditional_guidance_scale}\n",
    "                            samples_ddim = K.sampling.sample_lms(model_wrap_cfg, x, sigmas, extra_args=extra_args)\n",
    "                        else:\n",
    "                            samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                                conditioning=c,\n",
    "                                                                batch_size=batch_size,\n",
    "                                                                shape=shape,\n",
    "                                                                verbose=False,\n",
    "                                                                unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                                                unconditional_conditioning=uc,\n",
    "                                                                eta=ddim_eta,\n",
    "                                                                x_T=start_code)\n",
    "                    else:\n",
    "                        # encode (scaled latent)\n",
    "                        z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "                        # decode it\n",
    "                        samples_ddim = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                                unconditional_conditioning=uc)\n",
    "\n",
    "                    x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                    x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                    if not skip_save:\n",
    "                        for x_sample in x_samples_ddim:\n",
    "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                            img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                            img.save(\n",
    "                                os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "                            base_count += 1\n",
    "                            \n",
    "                            if show_images:\n",
    "                                display(img)\n",
    "                                \n",
    "                    if skip_save and show_images:\n",
    "                        for x_sample in x_samples_ddim:\n",
    "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                            img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                            display(img)\n",
    "\n",
    "                    if not skip_grid:\n",
    "                        all_samples.append(x_samples_ddim)\n",
    "\n",
    "            if not skip_grid:\n",
    "                # additionally, save as grid\n",
    "                grid = torch.stack(all_samples, 0)\n",
    "                grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "                # to image\n",
    "                grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "                Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outdir, f'grid-{grid_count:04}.png'))\n",
    "                grid_count += 1\n",
    "\n",
    "            toc = time.time()\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outdir}\\n\"\n",
    "        f\"\\nTime elapsed: {round(toc - tic, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text, Text -> Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92QkRfm0e6K0"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "prompts = [\n",
    "    (26208852, \"Joe Biden exhaling a large smoke cloud from his bong, candid photography\"),\n",
    "    (685626752, \"Barack Obama exhaling a large smoke cloud from his bong, candid photography\"),\n",
    "] # (seed, prompt)\n",
    "\n",
    "save_mp4 = 'test.mp4'\n",
    "\n",
    "average_weights = True # If using prompt weights, whether or not to average them\n",
    "\n",
    "ddim_steps = 50 # ddim sampling steps\n",
    "ddim_eta = 0.0 # ddim eta (eta=0.0 corresponds to deterministic sampling) (must be 0.0 if using PLMS sampling)\n",
    "unconditional_guidance_scale = 7.5 # unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\n",
    "\n",
    "precision = 'autocast' # precision to evaluate at (full or autocast)\n",
    "\n",
    "batch_size = 10 # how many samples to generate per prompt (currently must be set to one for animation)\n",
    "\n",
    "loop = True # if enabled, make animation loop by interpolating between end and start vectors\n",
    "fixed_code = False # if enabled, uses the same starting code across samples\n",
    "fixed_seed = None # fixed seed to use if using fixed_code\n",
    "skip_save = True # do not save individual frames\n",
    "skip_save_video = False # do not save mp4\n",
    "show_images = True # whether or not to show images after generation\n",
    "\n",
    "H = 512 # height\n",
    "W = 512 # width\n",
    "C = 4 # channels\n",
    "f = 8 # downsampling factor\n",
    "\n",
    "degrees_per_second = 20 # degrees to travel per second\n",
    "fps = 40 # frames per second of output mp4\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "precision_scope = autocast if precision==\"autocast\" else nullcontext\n",
    "\n",
    "frames_per_degree = fps / degrees_per_second\n",
    "\n",
    "if fixed_code:\n",
    "    for i in range(len(prompts)):\n",
    "        if fixed_seed is None:\n",
    "            fixed_seed = np.random.randint(-np.iinfo(np.int32).max, np.iinfo(np.int32).max)\n",
    "        if isinstance(prompts[i], str):\n",
    "            prompts[i] = (fixed_seed, prompts[i])\n",
    "        else:\n",
    "            prompts[i][0] = fixed_seed\n",
    "\n",
    "# interpolation setup\n",
    "previous_c = None\n",
    "previous_start_code = None\n",
    "slerp_c_vectors = []\n",
    "slerp_start_codes = []\n",
    "for i, data in enumerate(map(lambda x: get_starting_code_and_conditioning_vector(*x), prompts)):\n",
    "    c, start_code = data\n",
    "    if i == 0:\n",
    "        slerp_c_vectors.append(c)\n",
    "        slerp_start_codes.append(start_code)\n",
    "    else:\n",
    "        start_norm = previous_c.flatten()/torch.norm(previous_c.flatten())\n",
    "        end_norm = c.flatten()/torch.norm(c.flatten())\n",
    "        omega = torch.acos((start_norm*end_norm).sum())\n",
    "        frames_c = round(omega.item() * frames_per_degree * 57.2957795)\n",
    "        start_norm = previous_start_code.flatten()/torch.norm(previous_start_code.flatten())\n",
    "        end_norm = start_code.flatten()/torch.norm(start_code.flatten())\n",
    "        omega = torch.acos((start_norm*end_norm).sum())\n",
    "        frames_start_code = round(omega.item() * frames_per_degree * 57.2957795)\n",
    "        \n",
    "        frames = frames_c if frames_c >= frames_start_code else frames_start_code\n",
    "        \n",
    "        original_c_shape = c.shape\n",
    "        original_start_code_shape = start_code.shape\n",
    "        c_vectors = get_slerp_vectors(previous_c.flatten(), c.flatten(), frames=frames)\n",
    "        c_vectors = c_vectors.reshape(-1, *original_c_shape)\n",
    "        slerp_c_vectors.extend(list(c_vectors[1:])) # drop first frame to prevent repeating frames\n",
    "        start_codes = get_slerp_vectors(previous_start_code.flatten(), start_code.flatten(), frames=frames)\n",
    "        start_codes = start_codes.reshape(-1, *original_start_code_shape)\n",
    "        slerp_start_codes.extend(list(start_codes[1:])) # drop first frame to prevent repeating frames\n",
    "        if loop and i == len(prompts) - 1:\n",
    "            c_vectors = get_slerp_vectors(c.flatten(), slerp_c_vectors[0].flatten(), frames=frames)\n",
    "            c_vectors = c_vectors.reshape(-1, *original_c_shape)\n",
    "            slerp_c_vectors.extend(list(c_vectors[1:-1])) # drop first and last frame to prevent repeating frames\n",
    "            start_codes = get_slerp_vectors(start_code.flatten(), slerp_start_codes[0].flatten(), frames=frames)\n",
    "            start_codes = start_codes.reshape(-1, *original_start_code_shape)\n",
    "            slerp_start_codes.extend(list(start_codes[1:-1])) # drop first and last frame to prevent repeating frames\n",
    "    previous_c = c\n",
    "    previous_start_code = start_code\n",
    "    \n",
    "slerp_c_vectors = unflatten(slerp_c_vectors, batch_size)\n",
    "slerp_start_codes = unflatten(slerp_start_codes, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run generation(s)\n",
    "if not skip_save_video:\n",
    "    video_out = imageio.get_writer(save_mp4, mode='I', fps=fps, codec='libx264')\n",
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            tic = time.time()\n",
    "            #all_samples = list()\n",
    "            for c, start_code in tqdm(zip(slerp_c_vectors, slerp_start_codes), desc=\"data\", total=len(slerp_c_vectors)):\n",
    "                uc = None\n",
    "                if unconditional_guidance_scale != 1.0:\n",
    "                    uc = model.get_learned_conditioning(len(c) * [\"\"])\n",
    "                if isinstance(c, tuple) or isinstance(c, list):\n",
    "                    c = torch.stack(list(c), dim=0)\n",
    "                \n",
    "                c = torch.cat(tuple(c))\n",
    "                start_code = torch.cat(tuple(start_code))\n",
    "                \n",
    "                shape = [C, H // f, W // f]\n",
    "                \n",
    "                if use_k_lms:\n",
    "                            sigmas = model_wrap.get_sigmas(ddim_steps)\n",
    "                            model_wrap_cfg = CFGDenoiser(model_wrap)\n",
    "                            extra_args = {'cond': c, 'uncond': uc, 'cond_scale': unconditional_guidance_scale}\n",
    "                            samples_ddim = K.sampling.sample_lms(model_wrap_cfg, start_code*sigmas[0], sigmas, extra_args=extra_args)\n",
    "                else:\n",
    "                    samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                        conditioning=c,\n",
    "                                                        batch_size=len(c),\n",
    "                                                        shape=shape,\n",
    "                                                        verbose=False,\n",
    "                                                        unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                                        unconditional_conditioning=uc,\n",
    "                                                        eta=ddim_eta,\n",
    "                                                        x_T=start_code)\n",
    "\n",
    "                x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                if not skip_save or not skip_save_video or show_images:\n",
    "                    for x_sample in x_samples_ddim:\n",
    "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                        if not skip_save_video:\n",
    "                            video_out.append_data(x_sample)\n",
    "                        if not skip_save or show_images:\n",
    "                            img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                        if not skip_save:\n",
    "                            img.save(os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "                        if show_images:\n",
    "                            display(img)\n",
    "\n",
    "            toc = time.time()\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outdir}\\n\"\n",
    "        f\"\\nTime elapsed: {round(toc - tic, 2)} seconds\")\n",
    "video_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disco Diffusion Style Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# necessary packages and repos\n",
    "!apt update\n",
    "!apt install python3-opencv -y\n",
    "!pip install timm opencv-python matplotlib pandas\n",
    "!git clone https://github.com/alembics/disco-diffusion.git\n",
    "!mv disco-diffusion/disco_xform_utils.py disco_xform_utils.py\n",
    "!git clone https://github.com/isl-org/MiDaS.git\n",
    "!git clone https://github.com/MSFTserver/pytorch3d-lite.git\n",
    "!mv MiDaS/utils.py MiDaS/midas_utils.py\n",
    "!git clone https://github.com/shariqfarooq123/AdaBins.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MiDaS depth model\n",
    "!mkdir models/depth/\n",
    "!mkdir models/depth/midas/\n",
    "!wget -O models/depth/midas/model.ckpt https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBins depth model\n",
    "!mkdir pretrained/\n",
    "!wget -O pretrained/AdaBins_nyu.pt https://cloudflare-ipfs.com/ipfs/Qmd2mMnDLWePKmgfS8m6ntAg4nhV5VkUyAydYBp8cWWeB7/AdaBins_nyu.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system path links\n",
    "import sys\n",
    "sys.path.append(f'{os.path.abspath(os.getcwd())}')\n",
    "sys.path.append(f'{os.path.abspath(os.getcwd())}/MiDaS')\n",
    "sys.path.append(f'{os.path.abspath(os.getcwd())}/pytorch3d-lite')\n",
    "sys.path.append(f'{os.path.abspath(os.getcwd())}/AdaBins/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import cv2\n",
    "import gc\n",
    "import math\n",
    "import py3d_tools as p3dT\n",
    "import disco_xform_utils as dxf\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import pandas as pd\n",
    "from ipywidgets import Output\n",
    "from midas.dpt_depth import DPTDepthModel\n",
    "from midas.midas_net import MidasNet\n",
    "from midas.midas_net_custom import MidasNet_small\n",
    "from midas.transforms import Resize, NormalizeImage, PrepareForNet\n",
    "from types import SimpleNamespace\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def init_midas_depth_model():\n",
    "    midas_model = None\n",
    "    net_w = None\n",
    "    net_h = None\n",
    "    resize_mode = None\n",
    "    normalization = None\n",
    "\n",
    "    print(f\"Initializing MiDaS depth model...\")\n",
    "    # load network\n",
    "    midas_model_path = 'models/depth/midas/model.ckpt'\n",
    "    midas_model = DPTDepthModel(\n",
    "        path=midas_model_path,\n",
    "        backbone=\"vitl16_384\",\n",
    "        non_negative=True,\n",
    "    )\n",
    "    net_w, net_h = 384, 384\n",
    "    resize_mode = \"minimal\"\n",
    "    normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    "    midas_transform = T.Compose(\n",
    "        [\n",
    "            Resize(\n",
    "                net_w,\n",
    "                net_h,\n",
    "                resize_target=None,\n",
    "                keep_aspect_ratio=True,\n",
    "                ensure_multiple_of=32,\n",
    "                resize_method=resize_mode,\n",
    "                image_interpolation_method=cv2.INTER_CUBIC,\n",
    "            ),\n",
    "            normalization,\n",
    "            PrepareForNet(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    midas_model.eval()\n",
    "\n",
    "    midas_model = midas_model.to(memory_format=torch.channels_last)  \n",
    "    midas_model = midas_model.half()\n",
    "\n",
    "    midas_model.to(device)\n",
    "\n",
    "    print(f\"MiDaS depth model initialized.\")\n",
    "    return midas_model, midas_transform, net_w, net_h, resize_mode, normalization\n",
    "\n",
    "def do_3d_step(img_filepath, frame_num, midas_model, midas_transform):\n",
    "  if args.key_frames:\n",
    "    translation_x = args.translation_x_series[frame_num]\n",
    "    translation_y = args.translation_y_series[frame_num]\n",
    "    translation_z = args.translation_z_series[frame_num]\n",
    "    rotation_3d_x = args.rotation_3d_x_series[frame_num]\n",
    "    rotation_3d_y = args.rotation_3d_y_series[frame_num]\n",
    "    rotation_3d_z = args.rotation_3d_z_series[frame_num]\n",
    "    print(\n",
    "        f'translation_x: {translation_x}',\n",
    "        f'translation_y: {translation_y}',\n",
    "        f'translation_z: {translation_z}',\n",
    "        f'rotation_3d_x: {rotation_3d_x}',\n",
    "        f'rotation_3d_y: {rotation_3d_y}',\n",
    "        f'rotation_3d_z: {rotation_3d_z}',\n",
    "    )\n",
    "\n",
    "  translate_xyz = [-translation_x*TRANSLATION_SCALE, translation_y*TRANSLATION_SCALE, -translation_z*TRANSLATION_SCALE]\n",
    "  rotate_xyz_degrees = [rotation_3d_x, rotation_3d_y, rotation_3d_z]\n",
    "  print('translation:',translate_xyz)\n",
    "  print('rotation:',rotate_xyz_degrees)\n",
    "  rotate_xyz = [math.radians(rotate_xyz_degrees[0]), math.radians(rotate_xyz_degrees[1]), math.radians(rotate_xyz_degrees[2])]\n",
    "  rot_mat = p3dT.euler_angles_to_matrix(torch.tensor(rotate_xyz, device=device), \"XYZ\").unsqueeze(0)\n",
    "  print(\"rot_mat: \" + str(rot_mat))\n",
    "  next_step_pil = dxf.transform_image_3d(img_filepath, midas_model, midas_transform, torch.device(\"cuda\"),\n",
    "                                          rot_mat, translate_xyz, args.near_plane, args.far_plane,\n",
    "                                          args.fov, padding_mode=args.padding_mode,\n",
    "                                          sampling_mode=args.sampling_mode, midas_weight=args.midas_weight)\n",
    "  return next_step_pil\n",
    "\n",
    "def interp(t):\n",
    "    return 3 * t**2 - 2 * t ** 3\n",
    "\n",
    "def perlin(width, height, scale=10):\n",
    "    gx, gy = torch.randn(2, width + 1, height + 1, 1, 1, device=device)\n",
    "    xs = torch.linspace(0, 1, scale + 1)[:-1, None].to(device)\n",
    "    ys = torch.linspace(0, 1, scale + 1)[None, :-1].to(device)\n",
    "    wx = 1 - interp(xs)\n",
    "    wy = 1 - interp(ys)\n",
    "    dots = 0\n",
    "    dots += wx * wy * (gx[:-1, :-1] * xs + gy[:-1, :-1] * ys)\n",
    "    dots += (1 - wx) * wy * (-gx[1:, :-1] * (1 - xs) + gy[1:, :-1] * ys)\n",
    "    dots += wx * (1 - wy) * (gx[:-1, 1:] * xs - gy[:-1, 1:] * (1 - ys))\n",
    "    dots += (1 - wx) * (1 - wy) * (-gx[1:, 1:] * (1 - xs) - gy[1:, 1:] * (1 - ys))\n",
    "    return dots.permute(0, 2, 1, 3).contiguous().view(width * scale, height * scale)\n",
    "\n",
    "def perlin_ms(octaves, width, height, grayscale):\n",
    "    out_array = [0.5] if grayscale else [0.5, 0.5, 0.5, 0.5]\n",
    "    # out_array = [0.0] if grayscale else [0.0, 0.0, 0.0]\n",
    "    for i in range(1 if grayscale else 4):\n",
    "        scale = 2 ** len(octaves)\n",
    "        oct_width = width\n",
    "        oct_height = height\n",
    "        for oct in octaves:\n",
    "            p = perlin(oct_width, oct_height, scale)\n",
    "            out_array[i] += p * oct\n",
    "            scale //= 2\n",
    "            oct_width *= 2\n",
    "            oct_height *= 2\n",
    "    return torch.cat(out_array)\n",
    "\n",
    "def create_perlin_noise(octaves=[1, 1, 1, 1], width=2, height=2, grayscale=True):\n",
    "    out = perlin_ms(octaves, width, height, grayscale)\n",
    "    if grayscale:\n",
    "        out = TF.resize(size=(args.W//args.f, args.H//args.f), img=out.unsqueeze(0))\n",
    "        out = TF.to_pil_image(out.clamp(0, 1)).convert('RGBA')\n",
    "    else:\n",
    "        out = out.reshape(-1, 4, out.shape[0]//4, out.shape[1])\n",
    "        out = TF.resize(size=(args.W//args.f, args.H//args.f), img=out)\n",
    "        out = TF.to_pil_image(out.clamp(0, 1).squeeze())\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def regen_perlin():\n",
    "    if args.perlin_mode == 'color':\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
    "    elif args.perlin_mode == 'gray':\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "    else:\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "    display(init)\n",
    "    init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "    del init2\n",
    "    return init.expand(args.batch_size, -1, -1, -1)\n",
    "\n",
    "# def save_settings():\n",
    "#     setting_list = {\n",
    "#       'text_prompts': text_prompts,\n",
    "#       'image_prompts': image_prompts,\n",
    "#       'clip_guidance_scale': clip_guidance_scale,\n",
    "#       'tv_scale': tv_scale,\n",
    "#       'range_scale': range_scale,\n",
    "#       'sat_scale': sat_scale,\n",
    "#       # 'cutn': cutn,\n",
    "#       'cutn_batches': cutn_batches,\n",
    "#       'max_frames': max_frames,\n",
    "#       'interp_spline': interp_spline,\n",
    "#       # 'rotation_per_frame': rotation_per_frame,\n",
    "#       'init_image': init_image,\n",
    "#       'init_scale': init_scale,\n",
    "#       'skip_steps': skip_steps,\n",
    "#       # 'zoom_per_frame': zoom_per_frame,\n",
    "#       'frames_scale': frames_scale,\n",
    "#       'frames_skip_steps': frames_skip_steps,\n",
    "#       'perlin_init': perlin_init,\n",
    "#       'perlin_mode': perlin_mode,\n",
    "#       'skip_augs': skip_augs,\n",
    "#       'randomize_class': randomize_class,\n",
    "#       'clip_denoised': clip_denoised,\n",
    "#       'clamp_grad': clamp_grad,\n",
    "#       'clamp_max': clamp_max,\n",
    "#       'seed': seed,\n",
    "#       'fuzzy_prompt': fuzzy_prompt,\n",
    "#       'rand_mag': rand_mag,\n",
    "#       'eta': eta,\n",
    "#       'width': width_height[0],\n",
    "#       'height': width_height[1],\n",
    "#       'diffusion_model': diffusion_model,\n",
    "#       'use_secondary_model': use_secondary_model,\n",
    "#       'steps': steps,\n",
    "#       'diffusion_steps': diffusion_steps,\n",
    "#       'diffusion_sampling_mode': diffusion_sampling_mode,\n",
    "#       'ViTB32': ViTB32,\n",
    "#       'ViTB16': ViTB16,\n",
    "#       'ViTL14': ViTL14,\n",
    "#       'ViTL14_336px': ViTL14_336px,\n",
    "#       'RN101': RN101,\n",
    "#       'RN50': RN50,\n",
    "#       'RN50x4': RN50x4,\n",
    "#       'RN50x16': RN50x16,\n",
    "#       'RN50x64': RN50x64,\n",
    "#       'ViTB32_laion2b_e16': ViTB32_laion2b_e16,\n",
    "#       'ViTB32_laion400m_e31': ViTB32_laion400m_e31,\n",
    "#       'ViTB32_laion400m_32': ViTB32_laion400m_32,\n",
    "#       'ViTB32quickgelu_laion400m_e31': ViTB32quickgelu_laion400m_e31,\n",
    "#       'ViTB32quickgelu_laion400m_e32': ViTB32quickgelu_laion400m_e32,\n",
    "#       'ViTB16_laion400m_e31': ViTB16_laion400m_e31,\n",
    "#       'ViTB16_laion400m_e32': ViTB16_laion400m_e32,\n",
    "#       'RN50_yffcc15m': RN50_yffcc15m,\n",
    "#       'RN50_cc12m': RN50_cc12m,\n",
    "#       'RN50_quickgelu_yfcc15m': RN50_quickgelu_yfcc15m,\n",
    "#       'RN50_quickgelu_cc12m': RN50_quickgelu_cc12m,\n",
    "#       'RN101_yfcc15m': RN101_yfcc15m,\n",
    "#       'RN101_quickgelu_yfcc15m': RN101_quickgelu_yfcc15m,\n",
    "#       'cut_overview': str(cut_overview),\n",
    "#       'cut_innercut': str(cut_innercut),\n",
    "#       'cut_ic_pow': str(cut_ic_pow),\n",
    "#       'cut_icgray_p': str(cut_icgray_p),\n",
    "#       'key_frames': key_frames,\n",
    "#       'max_frames': max_frames,\n",
    "#       'angle': angle,\n",
    "#       'zoom': zoom,\n",
    "#       'translation_x': translation_x,\n",
    "#       'translation_y': translation_y,\n",
    "#       'translation_z': translation_z,\n",
    "#       'rotation_3d_x': rotation_3d_x,\n",
    "#       'rotation_3d_y': rotation_3d_y,\n",
    "#       'rotation_3d_z': rotation_3d_z,\n",
    "#       'midas_depth_model': midas_depth_model,\n",
    "#       'midas_weight': midas_weight,\n",
    "#       'near_plane': near_plane,\n",
    "#       'far_plane': far_plane,\n",
    "#       'fov': fov,\n",
    "#       'padding_mode': padding_mode,\n",
    "#       'sampling_mode': sampling_mode,\n",
    "#       'video_init_path':video_init_path,\n",
    "#       'extract_nth_frame':extract_nth_frame,\n",
    "#       'video_init_seed_continuity': video_init_seed_continuity,\n",
    "#       'turbo_mode':turbo_mode,\n",
    "#       'turbo_steps':turbo_steps,\n",
    "#       'turbo_preroll':turbo_preroll,\n",
    "#       'use_horizontal_symmetry':use_horizontal_symmetry,\n",
    "#       'use_vertical_symmetry':use_vertical_symmetry,\n",
    "#       'transformation_percent':transformation_percent,\n",
    "#       #video init settings\n",
    "#       'video_init_steps': video_init_steps,\n",
    "#       'video_init_clip_guidance_scale': video_init_clip_guidance_scale,\n",
    "#       'video_init_tv_scale': video_init_tv_scale,\n",
    "#       'video_init_range_scale': video_init_range_scale,\n",
    "#       'video_init_sat_scale': video_init_sat_scale,\n",
    "#       'video_init_cutn_batches': video_init_cutn_batches,\n",
    "#       'video_init_skip_steps': video_init_skip_steps,\n",
    "#       'video_init_frames_scale': video_init_frames_scale,\n",
    "#       'video_init_frames_skip_steps': video_init_frames_skip_steps,\n",
    "#       #warp settings\n",
    "#       'video_init_flow_warp':video_init_flow_warp,\n",
    "#       'video_init_flow_blend':video_init_flow_blend,\n",
    "#       'video_init_check_consistency':video_init_check_consistency,\n",
    "#       'video_init_blend_mode':video_init_blend_mode\n",
    "#     }\n",
    "#     # print('Settings:', setting_list)\n",
    "#     with open(f\"{batchFolder}/{batch_name}({batchNum})_settings.txt\", \"w+\") as f:   #save settings\n",
    "#         json.dump(setting_list, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"A dragons lair, epic matte painting, concept art, trending on artstation\",\n",
    "    \"A wizards magical potion room, epic matte painting, concept art, trending on artstation\"\n",
    "]\n",
    "\n",
    "average_weights = True\n",
    "seed = None\n",
    "loop = True\n",
    "interpolate = True\n",
    "\n",
    "device='cuda'\n",
    "\n",
    "ddim_steps = 250 # ddim sampling steps\n",
    "ddim_eta = 0.05 # ddim eta (eta=0.0 corresponds to deterministic sampling) (must be 0.0 if using PLMS sampling)\n",
    "unconditional_guidance_scale = 12 # unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\n",
    "\n",
    "init_image = None\n",
    "init_noise_strength = 0.4\n",
    "previous_frame_noise_strength = 0.35\n",
    "animation_mode = '3D'\n",
    "perlin_init = False # whether ot not to use perlin noi\n",
    "perlin_mode = 'color' # color or gray\n",
    "\n",
    "fixed_code = True\n",
    "\n",
    "n_iter = 1\n",
    "batch_size = 1\n",
    "\n",
    "start_frame = 0\n",
    "\n",
    "resume_run = False\n",
    "\n",
    "batch_name = 'test'\n",
    "batchNum = 1\n",
    "batchFolder = 'outputs'\n",
    "\n",
    "H = 512 # height\n",
    "W = 512 # width\n",
    "C = 4 # channels\n",
    "f = 8 # downsampling factor\n",
    "\n",
    "degrees_per_second = 20 # degrees to travel per second\n",
    "fps = 40 # frames per second of output mp4\n",
    "\n",
    "frames_per_degree = fps / degrees_per_second\n",
    "\n",
    "precision = 'autocast' # precision to evaluate at (full or autocast)\n",
    "\n",
    "video_init_path = \"init.mp4\"  # @param {type: 'string'}\n",
    "extract_nth_frame = 2  # @param {type: 'number'}\n",
    "persistent_frame_output_in_batch_folder = True  # @param {type: 'boolean'}\n",
    "video_init_seed_continuity = False  # @param {type: 'boolean'}\n",
    "# @markdown #####**Video Optical Flow Settings:**\n",
    "video_init_flow_warp = True  # @param {type: 'boolean'}\n",
    "# Call optical flow from video frames and warp prev frame with flow\n",
    "# @param {type: 'number'} #0 - take next frame, 1 - take prev warped frame\n",
    "video_init_flow_blend = 0.999\n",
    "video_init_check_consistency = False  # Insert param here when ready\n",
    "# @param ['None', 'linear', 'optical flow']\n",
    "video_init_blend_mode = \"optical flow\"\n",
    "# Call optical flow from video frames and warp prev frame with flow\n",
    "if animation_mode == \"Video Input\":\n",
    "    # suggested by Chris the Wizard#8082 at discord\n",
    "    if persistent_frame_output_in_batch_folder or (not is_colab):\n",
    "        videoFramesFolder = f'{batchFolder}/videoFrames'\n",
    "    else:\n",
    "        videoFramesFolder = f'/content/videoFrames'\n",
    "    createPath(videoFramesFolder)\n",
    "    print(f\"Exporting Video Frames (1 every {extract_nth_frame})...\")\n",
    "    try:\n",
    "        for f in pathlib.Path(f'{videoFramesFolder}').glob('*.jpg'):\n",
    "            f.unlink()\n",
    "    except:\n",
    "        print('')\n",
    "    vf = f'select=not(mod(n\\,{extract_nth_frame}))'\n",
    "    if os.path.exists(video_init_path):\n",
    "        subprocess.run(['ffmpeg', '-i', f'{video_init_path}', '-vf', f'{vf}', '-vsync', 'vfr', '-q:v', '2', '-loglevel',\n",
    "                       'error', '-stats', f'{videoFramesFolder}/%04d.jpg'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    else:\n",
    "        print(\n",
    "            f'\\nWARNING!\\n\\nVideo not found: {video_init_path}.\\nPlease check your video path.\\n')\n",
    "    #!ffmpeg -i {video_init_path} -vf {vf} -vsync vfr -q:v 2 -loglevel error -stats {videoFramesFolder}/%04d.jpg\n",
    "\n",
    "\n",
    "# @markdown ---\n",
    "\n",
    "# @markdown ####**2D Animation Settings:**\n",
    "# @markdown `zoom` is a multiplier of dimensions, 1 is no zoom.\n",
    "# @markdown All rotations are provided in degrees.\n",
    "\n",
    "key_frames = True  # @param {type:\"boolean\"}\n",
    "max_frames = 10000  # @param {type:\"number\"}\n",
    "\n",
    "if animation_mode == \"Video Input\":\n",
    "    max_frames = len(glob(f'{videoFramesFolder}/*.jpg'))\n",
    "\n",
    "# Do not change, currently will not look good. param ['Linear','Quadratic','Cubic']{type:\"string\"}\n",
    "interp_spline = 'Linear'\n",
    "angle = \"0:(0)\"  # @param {type:\"string\"}\n",
    "zoom = \"0: (1), 10: (1.0)\"  # @param {type:\"string\"}\n",
    "translation_x = \"0: (0)\"  # @param {type:\"string\"}\n",
    "translation_y = \"0: (0)\"  # @param {type:\"string\"}\n",
    "translation_z = \"0: (5.0)\"  # @param {type:\"string\"}\n",
    "rotation_3d_x = \"0: (0.2)\"  # @param {type:\"string\"}\n",
    "rotation_3d_y = \"0: (0)\"  # @param {type:\"string\"}\n",
    "rotation_3d_z = \"0: (0.0)\"  # @param {type:\"string\"}\n",
    "midas_depth_model = \"dpt_large\"  # @param {type:\"string\"}\n",
    "midas_weight = 0.3  # @param {type:\"number\"}\n",
    "near_plane = 200  # @param {type:\"number\"}\n",
    "far_plane = 10000  # @param {type:\"number\"}\n",
    "fov = 40  # @param {type:\"number\"}\n",
    "padding_mode = 'border'  # @param {type:\"string\"}\n",
    "sampling_mode = 'bicubic'  # @param {type:\"string\"}\n",
    "\n",
    "# ======= TURBO MODE\n",
    "# @markdown ---\n",
    "# @markdown ####**Turbo Mode (3D anim only):**\n",
    "# @markdown (Starts after frame 10,) skips diffusion steps and just uses depth map to warp images for skipped frames.\n",
    "# @markdown Speeds up rendering by 2x-4x, and may improve image coherence between frames.\n",
    "# @markdown For different settings tuned for Turbo Mode, refer to the original Disco-Turbo Github: https://github.com/zippy731/disco-diffusion-turbo\n",
    "\n",
    "turbo_mode = False  # @param {type:\"boolean\"}\n",
    "turbo_steps = \"3\"  # @param [\"2\",\"3\",\"4\",\"5\",\"6\"] {type:\"string\"}\n",
    "turbo_preroll = 10  # frames\n",
    "\n",
    "# insist turbo be used only w 3d anim.\n",
    "if turbo_mode and animation_mode != '3D':\n",
    "    print('=====')\n",
    "    print('Turbo mode only available with 3D animations. Disabling Turbo.')\n",
    "    print('=====')\n",
    "    turbo_mode = False\n",
    "\n",
    "# @markdown ---\n",
    "\n",
    "# @markdown ####**Coherency Settings:**\n",
    "# @markdown `frame_scale` tries to guide the new frame to looking like the old one. A good default is 1500.\n",
    "frames_scale = 1500  # @param{type: 'integer'}\n",
    "# @markdown `frame_skip_steps` will blur the previous frame - higher values will flicker less but struggle to add enough new detail to zoom into.\n",
    "# @param ['40%', '50%', '60%', '70%', '80%'] {type: 'string'}\n",
    "frames_skip_steps = '60%'\n",
    "\n",
    "# @markdown ####**Video Init Coherency Settings:**\n",
    "# @markdown `frame_scale` tries to guide the new frame to looking like the old one. A good default is 1500.\n",
    "video_init_frames_scale = 15000  # @param{type: 'integer'}\n",
    "# @markdown `frame_skip_steps` will blur the previous frame - higher values will flicker less but struggle to add enough new detail to zoom into.\n",
    "# @param ['40%', '50%', '60%', '70%', '80%'] {type: 'string'}\n",
    "video_init_frames_skip_steps = '70%'\n",
    "\n",
    "# ======= VR MODE\n",
    "# @markdown ---\n",
    "# @markdown ####**VR Mode (3D anim only):**\n",
    "# @markdown Enables stereo rendering of left/right eye views (supporting Turbo) which use a different (fish-eye) camera projection matrix.\n",
    "# @markdown Note the images you're prompting will work better if they have some inherent wide-angle aspect\n",
    "# @markdown The generated images will need to be combined into left/right videos. These can then be stitched into the VR180 format.\n",
    "# @markdown Google made the VR180 Creator tool but subsequently stopped supporting it. It's available for download in a few places including https://www.patrickgrunwald.de/vr180-creator-download\n",
    "# @markdown The tool is not only good for stitching (videos and photos) but also for adding the correct metadata into existing videos, which is needed for services like YouTube to identify the format correctly.\n",
    "# @markdown Watching YouTube VR videos isn't necessarily the easiest depending on your headset. For instance Oculus have a dedicated media studio and store which makes the files easier to access on a Quest https://creator.oculus.com/manage/mediastudio/\n",
    "# @markdown\n",
    "# @markdown The command to get ffmpeg to concat your frames for each eye is in the form: `ffmpeg -framerate 15 -i frame_%4d_l.png l.mp4` (repeat for r)\n",
    "\n",
    "vr_mode = False  # @param {type:\"boolean\"}\n",
    "# @markdown `vr_eye_angle` is the y-axis rotation of the eyes towards the center\n",
    "vr_eye_angle = 0.5  # @param{type:\"number\"}\n",
    "# @markdown interpupillary distance (between the eyes)\n",
    "vr_ipd = 5.0  # @param{type:\"number\"}\n",
    "\n",
    "# insist VR be used only w 3d anim.\n",
    "if vr_mode and animation_mode != '3D':\n",
    "    print('=====')\n",
    "    print('VR mode only available with 3D animations. Disabling VR.')\n",
    "    print('=====')\n",
    "    vr_mode = False\n",
    "\n",
    "\n",
    "def parse_key_frames(string, prompt_parser=None):\n",
    "    \"\"\"Given a string representing frame numbers paired with parameter values at that frame,\n",
    "    return a dictionary with the frame numbers as keys and the parameter values as the values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    string: string\n",
    "        Frame numbers paired with parameter values at that frame number, in the format\n",
    "        'framenumber1: (parametervalues1), framenumber2: (parametervalues2), ...'\n",
    "    prompt_parser: function or None, optional\n",
    "        If provided, prompt_parser will be applied to each string of parameter values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Frame numbers as keys, parameter values at that frame number as values\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    RuntimeError\n",
    "        If the input string does not match the expected format.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\")\n",
    "    {10: 'Apple: 1| Orange: 0', 20: 'Apple: 0| Orange: 1| Peach: 1'}\n",
    "\n",
    "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\", prompt_parser=lambda x: x.lower()))\n",
    "    {10: 'apple: 1| orange: 0', 20: 'apple: 0| orange: 1| peach: 1'}\n",
    "    \"\"\"\n",
    "    import re\n",
    "    pattern = r'((?P<frame>[0-9]+):[\\s]*[\\(](?P<param>[\\S\\s]*?)[\\)])'\n",
    "    frames = dict()\n",
    "    for match_object in re.finditer(pattern, string):\n",
    "        frame = int(match_object.groupdict()['frame'])\n",
    "        param = match_object.groupdict()['param']\n",
    "        if prompt_parser:\n",
    "            frames[frame] = prompt_parser(param)\n",
    "        else:\n",
    "            frames[frame] = param\n",
    "\n",
    "    if frames == {} and len(string) != 0:\n",
    "        raise RuntimeError('Key Frame string not correctly formatted')\n",
    "    return frames\n",
    "\n",
    "\n",
    "def get_inbetweens(key_frames, integer=False):\n",
    "    \"\"\"Given a dict with frame numbers as keys and a parameter value as values,\n",
    "    return a pandas Series containing the value of the parameter at every frame from 0 to max_frames.\n",
    "    Any values not provided in the input dict are calculated by linear interpolation between\n",
    "    the values of the previous and next provided frames. If there is no previous provided frame, then\n",
    "    the value is equal to the value of the next provided frame, or if there is no next provided frame,\n",
    "    then the value is equal to the value of the previous provided frame. If no frames are provided,\n",
    "    all frame values are NaN.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key_frames: dict\n",
    "        A dict with integer frame numbers as keys and numerical values of a particular parameter as values.\n",
    "    integer: Bool, optional\n",
    "        If True, the values of the output series are converted to integers.\n",
    "        Otherwise, the values are floats.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        A Series with length max_frames representing the parameter values for each frame.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> max_frames = 5\n",
    "    >>> get_inbetweens({1: 5, 3: 6})\n",
    "    0    5.0\n",
    "    1    5.0\n",
    "    2    5.5\n",
    "    3    6.0\n",
    "    4    6.0\n",
    "    dtype: float64\n",
    "\n",
    "    >>> get_inbetweens({1: 5, 3: 6}, integer=True)\n",
    "    0    5\n",
    "    1    5\n",
    "    2    5\n",
    "    3    6\n",
    "    4    6\n",
    "    dtype: int64\n",
    "    \"\"\"\n",
    "    key_frame_series = pd.Series([np.nan for a in range(max_frames)])\n",
    "\n",
    "    for i, value in key_frames.items():\n",
    "        key_frame_series[i] = value\n",
    "    key_frame_series = key_frame_series.astype(float)\n",
    "\n",
    "    interp_method = interp_spline\n",
    "\n",
    "    if interp_method == 'Cubic' and len(key_frames.items()) <= 3:\n",
    "        interp_method = 'Quadratic'\n",
    "\n",
    "    if interp_method == 'Quadratic' and len(key_frames.items()) <= 2:\n",
    "        interp_method = 'Linear'\n",
    "\n",
    "    key_frame_series[0] = key_frame_series[key_frame_series.first_valid_index()]\n",
    "    key_frame_series[max_frames -\n",
    "                     1] = key_frame_series[key_frame_series.last_valid_index()]\n",
    "    # key_frame_series = key_frame_series.interpolate(method=intrp_method,order=1, limit_direction='both')\n",
    "    key_frame_series = key_frame_series.interpolate(\n",
    "        method=interp_method.lower(), limit_direction='both')\n",
    "    if integer:\n",
    "        return key_frame_series.astype(int)\n",
    "    return key_frame_series\n",
    "\n",
    "if key_frames:\n",
    "    try:\n",
    "        angle_series = get_inbetweens(parse_key_frames(angle))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `angle` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `angle` as \"\n",
    "            f'\"0: ({angle})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        angle = f\"0: ({angle})\"\n",
    "        angle_series = get_inbetweens(parse_key_frames(angle))\n",
    "\n",
    "    try:\n",
    "        zoom_series = get_inbetweens(parse_key_frames(zoom))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `zoom` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `zoom` as \"\n",
    "            f'\"0: ({zoom})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        zoom = f\"0: ({zoom})\"\n",
    "        zoom_series = get_inbetweens(parse_key_frames(zoom))\n",
    "\n",
    "    try:\n",
    "        translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `translation_x` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `translation_x` as \"\n",
    "            f'\"0: ({translation_x})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        translation_x = f\"0: ({translation_x})\"\n",
    "        translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n",
    "\n",
    "    try:\n",
    "        translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `translation_y` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `translation_y` as \"\n",
    "            f'\"0: ({translation_y})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        translation_y = f\"0: ({translation_y})\"\n",
    "        translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n",
    "\n",
    "    try:\n",
    "        translation_z_series = get_inbetweens(parse_key_frames(translation_z))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `translation_z` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `translation_z` as \"\n",
    "            f'\"0: ({translation_z})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        translation_z = f\"0: ({translation_z})\"\n",
    "        translation_z_series = get_inbetweens(parse_key_frames(translation_z))\n",
    "\n",
    "    try:\n",
    "        rotation_3d_x_series = get_inbetweens(parse_key_frames(rotation_3d_x))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `rotation_3d_x` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `rotation_3d_x` as \"\n",
    "            f'\"0: ({rotation_3d_x})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        rotation_3d_x = f\"0: ({rotation_3d_x})\"\n",
    "        rotation_3d_x_series = get_inbetweens(parse_key_frames(rotation_3d_x))\n",
    "\n",
    "    try:\n",
    "        rotation_3d_y_series = get_inbetweens(parse_key_frames(rotation_3d_y))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `rotation_3d_y` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `rotation_3d_y` as \"\n",
    "            f'\"0: ({rotation_3d_y})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        rotation_3d_y = f\"0: ({rotation_3d_y})\"\n",
    "        rotation_3d_y_series = get_inbetweens(parse_key_frames(rotation_3d_y))\n",
    "\n",
    "    try:\n",
    "        rotation_3d_z_series = get_inbetweens(parse_key_frames(rotation_3d_z))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `rotation_3d_z` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `rotation_3d_z` as \"\n",
    "            f'\"0: ({rotation_3d_z})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        rotation_3d_z = f\"0: ({rotation_3d_z})\"\n",
    "        rotation_3d_z_series = get_inbetweens(parse_key_frames(rotation_3d_z))\n",
    "\n",
    "else:\n",
    "    angle = float(angle)\n",
    "    zoom = float(zoom)\n",
    "    translation_x = float(translation_x)\n",
    "    translation_y = float(translation_y)\n",
    "    translation_z = float(translation_z)\n",
    "    rotation_3d_x = float(rotation_3d_x)\n",
    "    rotation_3d_y = float(rotation_3d_y)\n",
    "    rotation_3d_z = float(rotation_3d_z)\n",
    "\n",
    "\n",
    "if seed is None:\n",
    "    seed = np.random.randint(-np.iinfo(np.int32).max, np.iinfo(np.int32).max)\n",
    "\n",
    "args = {\n",
    "    'seed': seed,\n",
    "    'animation_mode': animation_mode,\n",
    "    'init_image': init_image,\n",
    "    'perlin_init': perlin_init,\n",
    "    'perlin_mode': perlin_mode,\n",
    "    'H': H,\n",
    "    'W': W,\n",
    "    'C': C,\n",
    "    'f': f,\n",
    "    'start_frame': start_frame,\n",
    "    'max_frames': max_frames,\n",
    "    'n_iter': n_iter,\n",
    "    'batch_size': batch_size,\n",
    "    'init_noise_strength': init_noise_strength,\n",
    "    'previous_frame_noise_strength': previous_frame_noise_strength,\n",
    "    'ddim_steps': ddim_steps,\n",
    "    'ddim_eta': ddim_eta,\n",
    "    'unconditional_guidance_scale': unconditional_guidance_scale,\n",
    "    'fixed_code': fixed_code,\n",
    "    'key_frames': key_frames,\n",
    "    'angle_series': angle_series,\n",
    "    'zoom_series': zoom_series,\n",
    "    'translation_x_series': translation_x_series,\n",
    "    'translation_y_series': translation_y_series,\n",
    "    'translation_z_series': translation_z_series,\n",
    "    'rotation_3d_x_series': rotation_3d_x_series,\n",
    "    'rotation_3d_y_series': rotation_3d_y_series,\n",
    "    'rotation_3d_z_series': rotation_3d_z_series,\n",
    "    'near_plane': near_plane,\n",
    "    'far_plane': far_plane,\n",
    "    'fov': fov,\n",
    "    'padding_mode': padding_mode,\n",
    "    'sampling_mode': sampling_mode,\n",
    "    'midas_weight': midas_weight\n",
    "}\n",
    "\n",
    "args = SimpleNamespace(**args)\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "start_code = None\n",
    "if fixed_code:\n",
    "    start_code = torch.randn([batch_size, C, H // f, W // f], device=device)\n",
    "\n",
    "precision_scope = autocast if precision==\"autocast\" else nullcontext\n",
    "\n",
    "if interpolate and len(prompts) > 1:\n",
    "    previous_c = None\n",
    "    slerp_c_vectors = []\n",
    "    for i, c in enumerate(map(lambda x: get_conditioning_vector(x), prompts)):\n",
    "        if i == 0:\n",
    "            slerp_c_vectors.append(c)\n",
    "        else:\n",
    "            start_norm = previous_c.flatten()/torch.norm(previous_c.flatten())\n",
    "            end_norm = c.flatten()/torch.norm(c.flatten())\n",
    "            omega = torch.acos((start_norm*end_norm).sum())\n",
    "            frames = round(omega.item() * frames_per_degree * 57.2957795)\n",
    "\n",
    "            original_c_shape = c.shape\n",
    "            c_vectors = get_slerp_vectors(previous_c.flatten(), c.flatten(), frames=frames)\n",
    "            c_vectors = c_vectors.reshape(-1, *original_c_shape)\n",
    "            slerp_c_vectors.extend(list(c_vectors[1:])) # drop first frame to prevent repeating frames\n",
    "            if loop and i == len(prompts) - 1:\n",
    "                c_vectors = get_slerp_vectors(c.flatten(), slerp_c_vectors[0].flatten(), frames=frames)\n",
    "                c_vectors = c_vectors.reshape(-1, *original_c_shape)\n",
    "                slerp_c_vectors.extend(list(c_vectors[1:-1])) # drop first and last frame to prevent repeating frames\n",
    "        previous_c = c\n",
    "    data = ['']\n",
    "else:\n",
    "    data = list(chunk(prompts, batch_size))\n",
    "    interpolate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize midas depth model\n",
    "midas_model, midas_transform, midas_net_w, midas_net_h, midas_resize_mode, midas_normalization = init_midas_depth_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure sampler is DDIM\n",
    "sampler = DDIMSampler(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run animation loop - WIP\n",
    "TRANSLATION_SCALE = 1.0/200.0\n",
    "stop_on_next_loop = False\n",
    "for frame_num in tqdm(range(args.start_frame, args.max_frames), desc='Frames'):\n",
    "    if stop_on_next_loop:\n",
    "        break\n",
    "\n",
    "    if frame_num == 0:\n",
    "        init_image = args.init_image\n",
    "\n",
    "    if args.animation_mode == \"2D\":\n",
    "        if args.key_frames:\n",
    "            angle = args.angle_series[frame_num]\n",
    "            zoom = args.zoom_series[frame_num]\n",
    "            translation_x = args.translation_x_series[frame_num]\n",
    "            translation_y = args.translation_y_series[frame_num]\n",
    "            print(\n",
    "                f'angle: {angle}',\n",
    "                f'zoom: {zoom}',\n",
    "                f'translation_x: {translation_x}',\n",
    "                f'translation_y: {translation_y}',\n",
    "            )\n",
    "\n",
    "        if frame_num > 0:\n",
    "            # seed += 1\n",
    "            if resume_run and frame_num == start_frame:\n",
    "                img_0 = cv2.imread(\n",
    "                    batchFolder+f\"/{batch_name}({batchNum})_{start_frame-1:04}.png\")\n",
    "            else:\n",
    "                img_0 = cv2.imread('prevFrame.png')\n",
    "                center = (1*img_0.shape[1]//2, 1*img_0.shape[0]//2)\n",
    "                trans_mat = np.float32(\n",
    "                    [[1, 0, translation_x],\n",
    "                     [0, 1, translation_y]]\n",
    "                )\n",
    "                rot_mat = cv2.getRotationMatrix2D(center, angle, zoom)\n",
    "                trans_mat = np.vstack([trans_mat, [0, 0, 1]])\n",
    "                rot_mat = np.vstack([rot_mat, [0, 0, 1]])\n",
    "                transformation_matrix = np.matmul(rot_mat, trans_mat)\n",
    "                img_0 = cv2.warpPerspective(\n",
    "                    img_0,\n",
    "                    transformation_matrix,\n",
    "                    (img_0.shape[1], img_0.shape[0]),\n",
    "                    borderMode=cv2.BORDER_WRAP\n",
    "                )\n",
    "\n",
    "            cv2.imwrite('prevFrameScaled.png', img_0)\n",
    "            init_image = 'prevFrameScaled.png'\n",
    "        \n",
    "    if args.animation_mode == \"3D\":\n",
    "        if frame_num > 0:\n",
    "            # seed += 1\n",
    "            if resume_run and frame_num == start_frame:\n",
    "                img_filepath = batchFolder + \\\n",
    "                    f\"/{batch_name}({batchNum})_{start_frame-1:04}.png\"\n",
    "                if turbo_mode and frame_num > turbo_preroll:\n",
    "                    shutil.copyfile(img_filepath, 'oldFrameScaled.png')\n",
    "            else:\n",
    "                img_filepath = 'prevFrame.png'\n",
    "\n",
    "            next_step_pil = do_3d_step(\n",
    "                img_filepath, frame_num, midas_model, midas_transform)\n",
    "            next_step_pil.save('prevFrameScaled.png')\n",
    "\n",
    "            # Turbo mode - skip some diffusions, use 3d morph for clarity and to save time\n",
    "            if turbo_mode:\n",
    "                if frame_num == turbo_preroll:  # start tracking oldframe\n",
    "                    # stash for later blending\n",
    "                    next_step_pil.save('oldFrameScaled.png')\n",
    "                elif frame_num > turbo_preroll:\n",
    "                    # set up 2 warped image sequences, old & new, to blend toward new diff image\n",
    "                    old_frame = do_3d_step(\n",
    "                        'oldFrameScaled.png', frame_num, midas_model, midas_transform)\n",
    "                    old_frame.save('oldFrameScaled.png')\n",
    "                    if frame_num % int(turbo_steps) != 0:\n",
    "                        print(\n",
    "                            'turbo skip this frame: skipping clip diffusion steps')\n",
    "                        filename = f'{batch_name}({batchNum})_{frame_num:04}.png'\n",
    "                        blend_factor = (\n",
    "                            (frame_num % int(turbo_steps))+1)/int(turbo_steps)\n",
    "                        print(\n",
    "                            'turbo skip this frame: skipping clip diffusion steps and saving blended frame')\n",
    "                        # this is already updated..\n",
    "                        newWarpedImg = cv2.imread('prevFrameScaled.png')\n",
    "                        oldWarpedImg = cv2.imread('oldFrameScaled.png')\n",
    "                        blendedImage = cv2.addWeighted(\n",
    "                            newWarpedImg, blend_factor, oldWarpedImg, 1-blend_factor, 0.0)\n",
    "                        cv2.imwrite(\n",
    "                            f'{batchFolder}/{filename}', blendedImage)\n",
    "                        # save it also as prev_frame to feed next iteration\n",
    "                        next_step_pil.save(f'{img_filepath}')\n",
    "                        if vr_mode:\n",
    "                            generate_eye_views(\n",
    "                                TRANSLATION_SCALE, batchFolder, filename, frame_num, midas_model, midas_transform)\n",
    "                        continue\n",
    "                    else:\n",
    "                        # if not a skip frame, will run diffusion and need to blend.\n",
    "                        oldWarpedImg = cv2.imread('prevFrameScaled.png')\n",
    "                        # swap in for blending later\n",
    "                        cv2.imwrite(f'oldFrameScaled.png', oldWarpedImg)\n",
    "                        print('clip/diff this frame - generate clip diff image')\n",
    "\n",
    "            init_image = 'prevFrameScaled.png'\n",
    "\n",
    "        if args.animation_mode == \"Video Input\":\n",
    "            init_scale = args.video_init_frames_scale\n",
    "            skip_steps = args.calc_frames_skip_steps\n",
    "            if not video_init_seed_continuity:\n",
    "                seed += 1\n",
    "            if video_init_flow_warp:\n",
    "                if frame_num == 0:\n",
    "                    skip_steps = args.video_init_skip_steps\n",
    "                    init_image = f'{videoFramesFolder}/{frame_num+1:04}.jpg'\n",
    "                if frame_num > 0:\n",
    "                    prev = PIL.Image.open(\n",
    "                        batchFolder+f\"/{batch_name}({batchNum})_{frame_num-1:04}.png\")\n",
    "\n",
    "                    frame1_path = f'{videoFramesFolder}/{frame_num:04}.jpg'\n",
    "                    frame2 = PIL.Image.open(\n",
    "                        f'{videoFramesFolder}/{frame_num+1:04}.jpg')\n",
    "                    flo_path = f\"/{flo_folder}/{frame1_path.split('/')[-1]}.npy\"\n",
    "\n",
    "                    init_image = 'warped.png'\n",
    "                    print(video_init_flow_blend)\n",
    "                    weights_path = None\n",
    "                    if video_init_check_consistency:\n",
    "                        # TBD\n",
    "                        pass\n",
    "\n",
    "                    warp(prev, frame2, flo_path, blend=video_init_flow_blend,\n",
    "                         weights_path=weights_path).save(init_image)\n",
    "\n",
    "            else:\n",
    "                init_image = f'{videoFramesFolder}/{frame_num+1:04}.jpg'\n",
    "\n",
    "    seed_everything(args.seed+frame_num)\n",
    "\n",
    "    init = None\n",
    "    if init_image is not None:\n",
    "        init = load_img(init_image, (args.W, args.H)).to(device)\n",
    "        init = repeat(init, '1 ... -> b ...', b=args.batch_size)\n",
    "        init_latent = model.get_first_stage_encoding(\n",
    "            model.encode_first_stage(init))  # move to latent space\n",
    "\n",
    "        sampler.make_schedule(ddim_num_steps=args.ddim_steps,\n",
    "                              ddim_eta=args.ddim_eta, verbose=False)\n",
    "\n",
    "        noise_strength = args.init_noise_strength if frame_num == 0 else args.previous_frame_noise_strength\n",
    "        assert 0. <= noise_strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "        t_enc = int(noise_strength * args.ddim_steps)\n",
    "        print(f\"target t_enc is {t_enc} steps\")\n",
    "\n",
    "    if args.perlin_init:\n",
    "        if args.perlin_mode == 'color':\n",
    "            init = create_perlin_noise(\n",
    "                [1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "            init2 = create_perlin_noise(\n",
    "                [1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
    "        elif args.perlin_mode == 'gray':\n",
    "            init = create_perlin_noise(\n",
    "                [1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
    "            init2 = create_perlin_noise(\n",
    "                [1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "        else:\n",
    "            init = create_perlin_noise(\n",
    "                [1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "            init2 = create_perlin_noise(\n",
    "                [1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "        init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(\n",
    "            2).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "        del init2\n",
    "\n",
    "    print(f'Frame {frame_num}')\n",
    "\n",
    "    image_display = Output()\n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                tic = time.time()\n",
    "                all_samples = list()\n",
    "                print('')\n",
    "                display(image_display)\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                if perlin_init:\n",
    "                    init = regen_perlin()\n",
    "                    start_code = init\n",
    "\n",
    "                for prompts in tqdm(data, desc=\"data\"):\n",
    "                    uc = None\n",
    "                    if unconditional_guidance_scale != 1.0:\n",
    "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "                    \n",
    "                    if interpolate:\n",
    "                        c = slerp_c_vectors[frame_num%len(slerp_c_vectors)]\n",
    "                        c = torch.cat([c])\n",
    "                    else:\n",
    "                        c = torch.cat([get_conditioning_vector(prompt) for prompt in prompts])\n",
    "\n",
    "                    if init_image == None or init_image == '' or init_image == [] or init_image == ['']:\n",
    "                        shape = [args.C, args.H // args.f, args.W // args.f]\n",
    "                        samples_ddim, _ = sampler.sample(S=args.ddim_steps,\n",
    "                                                         conditioning=c,\n",
    "                                                         batch_size=args.batch_size,\n",
    "                                                         shape=shape,\n",
    "                                                         verbose=False,\n",
    "                                                         unconditional_guidance_scale=args.unconditional_guidance_scale,\n",
    "                                                         unconditional_conditioning=uc,\n",
    "                                                         eta=args.ddim_eta,\n",
    "                                                         x_T=start_code)\n",
    "\n",
    "                    else:\n",
    "                        # encode (scaled latent)\n",
    "                        z_enc = sampler.stochastic_encode(\n",
    "                            init_latent, torch.tensor([t_enc]*args.batch_size).to(device))\n",
    "                        # decode it\n",
    "                        samples_ddim = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=args.unconditional_guidance_scale,\n",
    "                                                      unconditional_conditioning=uc)\n",
    "\n",
    "                    x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                    x_samples_ddim = torch.clamp(\n",
    "                        (x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                    for x_sample in x_samples_ddim:\n",
    "                        x_sample = 255. * \\\n",
    "                            rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                        img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                        display(img)\n",
    "\n",
    "                        if args.animation_mode != \"None\":\n",
    "                            filename = f'{batch_name}({batchNum})_{frame_num}.png'\n",
    "                            img.save('prevFrame.png')\n",
    "                            img.save(f'{batchFolder}/{filename}')\n",
    "                            # if frame_num == 0:\n",
    "                            #     save_settings()\n",
    "                            if args.animation_mode == \"3D\":\n",
    "                                # If turbo, save a blended image\n",
    "                                if turbo_mode and frame_num > 0:\n",
    "                                    # Mix new image with prevFrameScaled\n",
    "                                    blend_factor = (1)/int(turbo_steps)\n",
    "                                    # This is already updated..\n",
    "                                    newFrame = cv2.imread('prevFrame.png')\n",
    "                                    prev_frame_warped = cv2.imread(\n",
    "                                        'prevFrameScaled.png')\n",
    "                                    blendedImage = cv2.addWeighted(\n",
    "                                        newFrame, blend_factor, prev_frame_warped, (1-blend_factor), 0.0)\n",
    "                                    cv2.imwrite(\n",
    "                                        f'{batchFolder}/{filename}', blendedImage)\n",
    "                                else:\n",
    "                                    img.save(f'{batchFolder}/{filename}')\n",
    "\n",
    "                                if vr_mode:\n",
    "                                    generate_eye_views(\n",
    "                                        TRANSLATION_SCALE, batchFolder, filename, frame_num, midas_model, midas_transform)\n",
    "                toc = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image, Mask -> Image (inpainting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coming Soon"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "latent-imagenet-diffusion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "96115ba3d133a87032cd70b9a322d48617443b59c68527f3d6b41f2029e5d2d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
