{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNHvQBhzyXCI",
    "outputId": "0a79e979-8484-4c62-96d9-7c79b1835162"
   },
   "outputs": [],
   "source": [
    "# may have to restart runtime/reset kernel after running\n",
    "!git clone https://github.com/CompVis/latent-diffusion.git\n",
    "!git clone https://github.com/CompVis/taming-transformers\n",
    "!pip install -e ./taming-transformers\n",
    "!pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install transformers kornia imageio imageio_ffmpeg pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/ldm/text2img-large/\n",
    "!wget -O models/ldm/text2img-large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import imageio\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange, repeat\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import contextmanager, nullcontext\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falling back to LAION 400M model...\n"
     ]
    }
   ],
   "source": [
    "# variables\n",
    "use_laion400m = True\n",
    "use_plms = False\n",
    "\n",
    "if use_laion400m:\n",
    "    print(\"Falling back to LAION 400M model...\")\n",
    "    config_location = \"configs/latent-diffusion/txt2img-1p4B-eval.yaml\"\n",
    "    model_location = \"models/ldm/text2img-large/model.ckpt\"\n",
    "    outdir = \"outputs/txt2img-samples-laion400m\"\n",
    "else:\n",
    "    print(\"Using Stable Diffusion model...\")\n",
    "    config_location = 'configs/stable-diffusion/v1-inference.yaml'\n",
    "    model_location = 'models/ldm/stable-diffusion-v1/model.ckpt'\n",
    "    outdir = 'outputs/txt2img-samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "fnGwQRhtyBhb"
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def load_model_from_config(config, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt)#, map_location=\"cpu\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_model():\n",
    "    config = OmegaConf.load(config_location)\n",
    "    model = load_model_from_config(config, model_location)\n",
    "    return model\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "def load_img(path, size):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    w, h = image.size\n",
    "    print(f\"loaded input image of size ({w}, {h}) and resized to ({size[0]}, {size[1]}) from {path}\")\n",
    "    w, h = size #map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
    "    image = image.resize((w, h), resample=Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.*image - 1.\n",
    "\n",
    "def slerp(val, low, high):\n",
    "    low_norm = low/torch.norm(low)\n",
    "    high_norm = high/torch.norm(high)\n",
    "    omega = torch.acos((low_norm*high_norm).sum())\n",
    "    so = torch.sin(omega)\n",
    "    res = (torch.sin((1.0-val)*omega)/so)*low + (torch.sin(val*omega)/so) * high\n",
    "    return res\n",
    "\n",
    "def get_slerp_vectors(start, end, frames=60):\n",
    "    out = torch.Tensor(frames, start.shape[0]).to(device)\n",
    "    factor = 1.0 / (frames - 1)\n",
    "    for i in range(frames):\n",
    "        out[i] = slerp(factor*i, start, end)\n",
    "    return out\n",
    "\n",
    "def get_starting_code_and_conditioning_vector(seed, prompt):\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(np.iinfo(np.int32).max)\n",
    "    seed_everything(seed)\n",
    "    start_code = torch.randn([batch_size, C, H // f, W // f], device=device)\n",
    "    c = model.get_learned_conditioning(prompt)\n",
    "    return (c, start_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPnyd-XUKbfE",
    "outputId": "0fcd10e4-0df2-4ab9-cbf5-f08f4902c954"
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "if use_plms:\n",
    "    print(\"Warning: img2img not compatible with PLMSSampler\")\n",
    "    sampler = PLMSSampler(model)\n",
    "else:\n",
    "    sampler = DDIMSampler(model)\n",
    "\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "sample_path = os.path.join(outdir, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "grid_count = len(os.listdir(outdir)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text -> Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Joe Biden\"\n",
    "] # list of string prompts\n",
    "seed = None\n",
    "\n",
    "init_image_location = \"\" # file location of init image (use a blank string if none desired)\n",
    "init_noise_strength = 0.75 # how much to noise init image (0-1.0 where 1.0 is full destruction of init image information)\n",
    "\n",
    "ddim_steps = 50 # ddim sampling steps\n",
    "ddim_eta = 0.0 # ddim eta (eta=0.0 corresponds to deterministic sampling) (must be 0.0 if using PLMS sampling)\n",
    "unconditional_guidance_scale = 7.5 # unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\n",
    "\n",
    "precision = 'autocast' # precision to evaluate at (full or autocast)\n",
    "\n",
    "n_iter = 1 # how many sample iterations\n",
    "batch_size = 1 # how many samples to generate per prompt\n",
    "n_rows = 0 # rows in the grid (will use batch_size if set to 0)\n",
    "\n",
    "fixed_code = False # if enabled, uses the same starting code across samples\n",
    "skip_grid = True # do not save a grid, only individual samples\n",
    "skip_save = False # do not save individual samples\n",
    "show_images = True # whether or not to show images after generation\n",
    "\n",
    "H = 512 # height\n",
    "W = 512 # width\n",
    "C = 4 # channels\n",
    "f = 8 # downsampling factor\n",
    "\n",
    "if n_rows == 0:\n",
    "    n_rows = batch_size\n",
    "\n",
    "if seed is None:\n",
    "    seed = np.random.randint(np.iinfo(np.int32).max)\n",
    "seed_everything(seed)\n",
    "\n",
    "start_code = None\n",
    "if fixed_code:\n",
    "    start_code = torch.randn([batch_size, C, H // f, W // f], device=device)\n",
    "\n",
    "precision_scope = autocast if precision==\"autocast\" else nullcontext\n",
    "data = chunk(prompts, batch_size)\n",
    "\n",
    "if init_image_location != \"\":\n",
    "    assert os.path.isfile(init_image_location)\n",
    "    init_image = load_img(init_image_location, ()).to(device)\n",
    "    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "\n",
    "    sampler.make_schedule(ddim_num_steps=ddim_steps, ddim_eta=ddim_eta, verbose=False)\n",
    "\n",
    "    assert 0. <= init_noise_strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "    t_enc = int(init_noise_strength * ddim_steps)\n",
    "    print(f\"target t_enc is {t_enc} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            tic = time.time()\n",
    "            all_samples = list()\n",
    "            for n in trange(n_iter, desc=\"Sampling\"):\n",
    "                for prompts in tqdm(data, desc=\"data\"):\n",
    "                    uc = None\n",
    "                    if unconditional_guidance_scale != 1.0:\n",
    "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "                    c = model.get_learned_conditioning(prompts)\n",
    "\n",
    "                    if len(init_image_location) == 0:\n",
    "                        shape = [C, H // f, W // f]\n",
    "                        samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                            conditioning=c,\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            shape=shape,\n",
    "                                                            verbose=False,\n",
    "                                                            unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                                            unconditional_conditioning=uc,\n",
    "                                                            eta=ddim_eta,\n",
    "                                                            x_T=start_code)\n",
    "                    else:\n",
    "                        # encode (scaled latent)\n",
    "                        z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "                        # decode it\n",
    "                        samples_ddim = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                                unconditional_conditioning=uc)\n",
    "\n",
    "                    x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                    x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                    if not skip_save:\n",
    "                        for x_sample in x_samples_ddim:\n",
    "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                            img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                            img.save(\n",
    "                                os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "                            base_count += 1\n",
    "                            \n",
    "                            if show_images:\n",
    "                                display(img)\n",
    "                                \n",
    "                    if skip_save and show_images:\n",
    "                        for x_sample in x_samples_ddim:\n",
    "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                            img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                            display(img)\n",
    "\n",
    "                    if not skip_grid:\n",
    "                        all_samples.append(x_samples_ddim)\n",
    "\n",
    "            if not skip_grid:\n",
    "                # additionally, save as grid\n",
    "                grid = torch.stack(all_samples, 0)\n",
    "                grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "                # to image\n",
    "                grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "                Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outdir, f'grid-{grid_count:04}.png'))\n",
    "                grid_count += 1\n",
    "\n",
    "            toc = time.time()\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outdir}\\n\"\n",
    "        f\"\\nTime elapsed: {round(toc - tic, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text, Text -> Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92QkRfm0e6K0"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    (741, \"Joe Biden exhaling a large smoke cloud, featured on artstation\"),\n",
    "    (None, \"The world on fire, trending on artstation\"),\n",
    "    (420, \"Barack Obama exhaling a large smoke cloud, featured on artstation\"),\n",
    "] # (seed, prompt)\n",
    "\n",
    "ddim_steps = 50 # ddim sampling steps\n",
    "ddim_eta = 0.0 # ddim eta (eta=0.0 corresponds to deterministic sampling) (must be 0.0 if using PLMS sampling)\n",
    "unconditional_guidance_scale = 7.5 # unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\n",
    "\n",
    "precision = 'autocast' # precision to evaluate at (full or autocast)\n",
    "\n",
    "batch_size = 1 # how many samples to generate per prompt (currently must be set to one for animation)\n",
    "\n",
    "loop = True # if enabled, make animation loop by interpolating between end and start vectors\n",
    "fixed_code = True # if enabled, uses the same starting code across samples\n",
    "skip_save = False # do not save individual samples\n",
    "show_images = True # whether or not to show images after generation\n",
    "\n",
    "H = 512 # height\n",
    "W = 512 # width\n",
    "C = 4 # channels\n",
    "f = 8 # downsampling factor\n",
    "\n",
    "precision_scope = autocast if precision==\"autocast\" else nullcontext\n",
    "\n",
    "frames = 60 # number of frames between text prompts\n",
    "fps = 60 # frames per second of output mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_c = None\n",
    "previous_start_code = None\n",
    "slerp_c_vectors = []\n",
    "slerp_start_codes = []\n",
    "frames = frames + 2 # pad for beginning and end frame\n",
    "for i, data in enumerate(map(lambda x: get_starting_code_and_conditioning_vector(*x), prompts)):\n",
    "    c, start_code = data\n",
    "    if i == 0:\n",
    "        slerp_c_vectors.append(c)\n",
    "        slerp_start_codes.append(start_code)\n",
    "    else:\n",
    "        original_c_shape = c.shape\n",
    "        original_start_code_shape = start_code.shape\n",
    "        c_vectors = get_slerp_vectors(previous_c.flatten(), c.flatten(), frames=frames)\n",
    "        c_vectors = c_vectors.reshape(-1, *original_c_shape)\n",
    "        slerp_c_vectors.extend(list(c_vectors[1:])) # drop first frame to prevent repeating frames\n",
    "        start_codes = get_slerp_vectors(previous_start_code.flatten(), start_code.flatten(), frames=frames)\n",
    "        start_codes = start_codes.reshape(-1, *original_start_code_shape)\n",
    "        slerp_start_codes.extend(list(start_codes[1:])) # drop first frame to prevent repeating frames\n",
    "        if loop and i == len(prompts) - 1:\n",
    "            c_vectors = get_slerp_vectors(c.flatten(), slerp_c_vectors[0].flatten(), frames=frames)\n",
    "            c_vectors = c_vectors.reshape(-1, *original_c_shape)\n",
    "            slerp_c_vectors.extend(list(c_vectors[1:-1])) # drop first frame to prevent repeating frames\n",
    "            start_codes = get_slerp_vectors(start_code.flatten(), slerp_start_codes[0].flatten(), frames=frames)\n",
    "            start_codes = start_codes.reshape(-1, *original_start_code_shape)\n",
    "            slerp_start_codes.extend(list(start_codes[1:-1])) # drop first and last frame to prevent repeating frames\n",
    "    previous_c = c\n",
    "    previous_start_code = start_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_out = imageio.get_writer('test.mp4', mode='I', fps=fps, codec='libx264')\n",
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            tic = time.time()\n",
    "            #all_samples = list()\n",
    "            for c, start_code in tqdm(zip(slerp_c_vectors, slerp_start_codes), desc=\"data\", total=len(slerp_c_vectors)):\n",
    "                uc = None\n",
    "                if unconditional_guidance_scale != 1.0:\n",
    "                    uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                if isinstance(c, tuple) or isinstance(c, list):\n",
    "                    c = torch.stack(list(c), dim=0)\n",
    "                if isinstance(start_code, tuple) or isinstance(start_code, list):\n",
    "                    start_code = start_code[0]\n",
    "                shape = [C, H // f, W // f]\n",
    "\n",
    "                samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                    conditioning=c,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shape=shape,\n",
    "                                                    verbose=False,\n",
    "                                                    unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                                    unconditional_conditioning=uc,\n",
    "                                                    eta=ddim_eta,\n",
    "                                                    x_T=start_code)\n",
    "\n",
    "                x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                if not skip_save:\n",
    "                    for x_sample in x_samples_ddim:\n",
    "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                        video_out.append_data(x_sample)\n",
    "                        img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                        base_count += 1\n",
    "\n",
    "                        if show_images:\n",
    "                            display(img)\n",
    "\n",
    "                if skip_save and show_images:\n",
    "                    for x_sample in x_samples_ddim:\n",
    "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                        img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                        display(img)\n",
    "\n",
    "            toc = time.time()\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outdir}\\n\"\n",
    "        f\"\\nTime elapsed: {round(toc - tic, 2)} seconds\")\n",
    "video_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image, Mask -> Image (inpainting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "latent-imagenet-diffusion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "96115ba3d133a87032cd70b9a322d48617443b59c68527f3d6b41f2029e5d2d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
