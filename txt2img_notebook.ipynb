{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "cNHvQBhzyXCI",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0a79e979-8484-4c62-96d9-7c79b1835162",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# necessary packages and repos - may have to restart runtime/reset kernel after running\n",
    "!apt update\n",
    "!apt install python3-opencv -y\n",
    "\n",
    "!git clone https://github.com/alembics/disco-diffusion.git\n",
    "!mv disco-diffusion/disco_xform_utils.py disco_xform_utils.py\n",
    "!git clone https://github.com/isl-org/MiDaS.git\n",
    "!git clone https://github.com/MSFTserver/pytorch3d-lite.git\n",
    "!mv MiDaS/utils.py MiDaS/midas_utils.py\n",
    "!git clone https://github.com/shariqfarooq123/AdaBins.git\n",
    "\n",
    "!git clone https://github.com/CompVis/latent-diffusion.git\n",
    "!git clone https://github.com/CompVis/taming-transformers\n",
    "!git clone https://github.com/crowsonkb/k-diffusion\n",
    "!git clone https://github.com/mlfoundations/open_clip.git\n",
    "!git clone https://github.com/assafshocher/ResizeRight.git\n",
    "!pip install -e ./taming-transformers\n",
    "!pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops\n",
    "!pip install git+https://github.com/openai/CLIP.git huggingface_hub lpips timm opencv-python matplotlib pandas\n",
    "!pip install transformers kornia imageio imageio_ffmpeg pillow scikit-image jsonmerge clean-fid resize-right torchdiffeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SD weights download/setup\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "model_location = hf_hub_download(repo_id=\"CompVis/stable-diffusion-v-1-4-original\", filename=\"sd-v1-4.ckpt\", use_auth_token='')\n",
    "model_location = model_location.split('snapshots')[0]+'blobs/'+os.listdir(model_location.split('snapshots')[0]+'blobs/')[0]\n",
    "!mv $model_location models/ldm/stable-diffusion-v1/model.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # fallback LAION 400M model download/setup (not needed if using SD weights)\n",
    "# !mkdir -p models/ldm/text2img-large/\n",
    "# !wget -O models/ldm/text2img-large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MiDaS depth model\n",
    "!mkdir models/depth/\n",
    "!mkdir models/depth/midas/\n",
    "!wget -O models/depth/midas/model.ckpt https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBins depth model\n",
    "!mkdir pretrained/\n",
    "!wget -O pretrained/AdaBins_nyu.pt https://cloudflare-ipfs.com/ipfs/Qmd2mMnDLWePKmgfS8m6ntAg4nhV5VkUyAydYBp8cWWeB7/AdaBins_nyu.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import imageio\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import Output\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from omegaconf import OmegaConf\n",
    "from types import SimpleNamespace\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm.auto import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange, repeat\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import contextmanager, nullcontext\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "\n",
    "import disco_xform_utils as dxf\n",
    "\n",
    "############################\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(f'{os.path.abspath(os.getcwd())}/k-diffusion')\n",
    "sys.path.append(f'{os.path.abspath(os.getcwd())}/ResizeRight')\n",
    "sys.path.append(f'{os.path.abspath(os.getcwd())}/open_clip/src')\n",
    "sys.path.append(f'{os.path.abspath(os.getcwd())}')\n",
    "sys.path.append(f'{os.path.abspath(os.getcwd())}/MiDaS')\n",
    "sys.path.append(f'{os.path.abspath(os.getcwd())}/pytorch3d-lite')\n",
    "sys.path.append(f'{os.path.abspath(os.getcwd())}/AdaBins/')\n",
    "\n",
    "from resize_right import resize\n",
    "import open_clip\n",
    "import py3d_tools as p3dT\n",
    "from midas.dpt_depth import DPTDepthModel\n",
    "from midas.midas_net import MidasNet\n",
    "from midas.midas_net_custom import MidasNet_small\n",
    "from midas.transforms import Resize, NormalizeImage, PrepareForNet\n",
    "import k_diffusion as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# variables\n",
    "use_laion400m = False\n",
    "use_plms = True\n",
    "use_k_lms = False\n",
    "use_fps16 = True\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if use_laion400m:\n",
    "    print(\"Falling back to LAION 400M model...\")\n",
    "    config_location = \"configs/latent-diffusion/txt2img-1p4B-eval.yaml\"\n",
    "    model_location = \"models/ldm/text2img-large/model.ckpt\"\n",
    "    outdir = \"outputs/txt2img-samples-laion400m\"\n",
    "else:\n",
    "    print(\"Using Stable Diffusion model...\")\n",
    "    config_location = 'configs/stable-diffusion/v1-inference.yaml'\n",
    "    model_location = 'models/ldm/stable-diffusion-v1/model.ckpt'\n",
    "    outdir = 'outputs/txt2img-samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "fnGwQRhtyBhb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def load_model_from_config(config, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt)#, map_location=\"cpu\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if use_fp16:\n",
    "        model = model.half()\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    return model\n",
    "\n",
    "def get_model():\n",
    "    config = OmegaConf.load(config_location)\n",
    "    model = load_model_from_config(config, model_location)\n",
    "    return model\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "def load_img(path, size, mask=False, outcrop_animation=False, factor=0.5, show_generated_mask_and_image=False):\n",
    "    if not outcrop_animation:\n",
    "        factor = 1.0\n",
    "        if mask:\n",
    "            image = Image.open(path).convert(\"L\")\n",
    "        else:\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "        w_, h_ = image.size\n",
    "    elif outcrop_animation and not mask:\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        w_, h_ = image.size\n",
    "    w, h = map(lambda x: x - x % 32, (size[0], size[1]))  # resize to integer multiple of 32\n",
    "    if outcrop_animation:\n",
    "        w_scaled, h_scaled = map(lambda x: int(x*factor), (w, h))  # scale image\n",
    "    if outcrop_animation and mask:\n",
    "        assert 0.0 <= factor <= 1.0\n",
    "        image = Image.new('L', (w_scaled, h_scaled), 'white')\n",
    "        padding = Image.new(image.mode, (w, h), 'black')\n",
    "        padding.paste(image, (int((w-w_scaled)/2), int((h-h_scaled)/2)))\n",
    "        image = padding\n",
    "    else:\n",
    "        print(f\"loaded input image of size ({w_}, {h_}) and resized to ({w}, {h}) from {path}\")\n",
    "        if outcrop_animation:\n",
    "            assert 0.0 <= factor <= 1.0\n",
    "            image = image.resize((w_scaled, h_scaled), resample=Image.LANCZOS)\n",
    "            padding = Image.new(image.mode, (w, h), 'black')\n",
    "            padding.paste(image, (int((w-w_scaled)/2), int((h-h_scaled)/2)))\n",
    "            image = padding\n",
    "        else:\n",
    "            image = image.resize((w, h), resample=Image.LANCZOS)\n",
    "    if show_generated_mask_and_image:\n",
    "        display(image)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    if mask:\n",
    "        image = image[None,None]\n",
    "        image[image < 0.5] = 0\n",
    "        image[image >= 0.5] = 1\n",
    "        return torch.from_numpy(image)\n",
    "    else:\n",
    "        image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.*image - 1.\n",
    "\n",
    "def slerp(val, low, high):\n",
    "    low_norm = low/torch.norm(low)\n",
    "    high_norm = high/torch.norm(high)\n",
    "    omega = torch.acos((low_norm*high_norm).sum())\n",
    "    so = torch.sin(omega)\n",
    "    res = (torch.sin((1.0-val)*omega)/so)*low + (torch.sin(val*omega)/so) * high\n",
    "    return res\n",
    "\n",
    "def get_slerp_vectors(start, end, frames=20):\n",
    "    out = torch.Tensor(frames, start.shape[0]).to(device)\n",
    "    factor = 1.0 / (frames - 1)\n",
    "    for i in range(frames):\n",
    "        out[i] = slerp(factor*i, start, end)\n",
    "    return out\n",
    "\n",
    "def get_conditioning_vector(prompt):\n",
    "    split_prompt = re.split(r'::([-\\d.]+)', prompt)\n",
    "    if len(split_prompt) == 0:\n",
    "        raise(AttributeError('not a valid prompt'))\n",
    "    elif len(split_prompt) == 1:\n",
    "        return model.get_learned_conditioning(prompt)\n",
    "\n",
    "    split_prompt = iter(split_prompt)\n",
    "    c_tensors = []\n",
    "    weights = []\n",
    "    for text in split_prompt:\n",
    "        if text == '':\n",
    "            continue\n",
    "        text = text.strip()\n",
    "        try:\n",
    "            weight = float(next(split_prompt))\n",
    "            weights.append(weight)\n",
    "            c_tensors.append(model.get_learned_conditioning(text))\n",
    "        except:\n",
    "            print(f'Prompt: \"{text}\" dropped due to invalid weight') \n",
    "            continue\n",
    "    abs_weight = [abs(weight) for weight in weights] if average_weights else weights\n",
    "\n",
    "    c = c_tensors[0] * weights[0]\n",
    "    for c_tensor, weight in zip(c_tensors[1:], weights[1:]):\n",
    "        c += c_tensor * weight\n",
    "    c = c/sum(abs_weight)\n",
    "    return c\n",
    "\n",
    "def get_starting_code_and_conditioning_vector(seed, prompt):\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(np.iinfo(np.int32).max)\n",
    "    seed_everything(seed)\n",
    "    start_code = torch.randn([1, C, H // f, W // f], device=device)\n",
    "    c = get_conditioning_vector(prompt)\n",
    "    return (c, start_code)\n",
    "\n",
    "def unflatten(l, n):\n",
    "    res = []\n",
    "    t = l[:]\n",
    "    while len(t) > 0:\n",
    "        res.append(t[:n])\n",
    "        t = t[n:]\n",
    "    return res\n",
    "\n",
    "def zoom_at(org_img, zoom, x, y):\n",
    "    w, h = org_img.size\n",
    "\n",
    "    xz = x / zoom\n",
    "    yz = y / zoom\n",
    "    box = (int(x - xz), int(y - yz), int(x - xz + w / zoom), int(y - yz + h / zoom))\n",
    "\n",
    "    img = org_img.crop(box)\n",
    "    img = img.resize((w, h), Image.LANCZOS)\n",
    "    return img\n",
    "\n",
    "def outcropping_frames_to_mp4(out_crop_dir, project_name, out_mp4, frame_count, frames, fps, factor, zoom_cropping):\n",
    "    log_scale = math.log(1/factor)\n",
    "    anim_size = None\n",
    "    with imageio.get_writer(out_mp4, mode='I', fps=fps) as writer:\n",
    "        for i in range(frame_count, 0, -1):\n",
    "            if i == 1 and original_image_location != '':\n",
    "                img = Image.open(original_image_location)\n",
    "                img = img.resize(anim_size, Image.LANCZOS)\n",
    "            else:\n",
    "                img = Image.open(f\"{out_crop_dir}/{project_name}_{str(i)}.png\")\n",
    "                anim_size = img.size\n",
    "            w, h = img.size\n",
    "            for frame in tqdm(range(frames)):\n",
    "                if i == 1 and frame == frames // 2:\n",
    "                    break\n",
    "                zoom = math.exp(log_scale * frame / (frames))\n",
    "                zoomed = zoom_at(img, zoom, w//2, h//2)\n",
    "                w, h = zoomed.size\n",
    "                if zoom_cropping != (0, 0):\n",
    "                    zoomed = zoomed.crop((0 + zoom_cropping[0], 0 + zoom_cropping[1], w - zoom_cropping[0], h - zoom_cropping[1]))\n",
    "                writer.append_data(np.array(zoomed))\n",
    "    print(f\"Outcropping animation is available at {out_mp4}\")\n",
    "\n",
    "####################################\n",
    "# disco style animations utility functions\n",
    "def init_midas_depth_model():\n",
    "    midas_model = None\n",
    "    net_w = None\n",
    "    net_h = None\n",
    "    resize_mode = None\n",
    "    normalization = None\n",
    "\n",
    "    print(f\"Initializing MiDaS depth model...\")\n",
    "    # load network\n",
    "    midas_model_path = 'models/depth/midas/model.ckpt'\n",
    "    midas_model = DPTDepthModel(\n",
    "        path=midas_model_path,\n",
    "        backbone=\"vitl16_384\",\n",
    "        non_negative=True,\n",
    "    )\n",
    "    net_w, net_h = 384, 384\n",
    "    resize_mode = \"minimal\"\n",
    "    normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    "    midas_transform = T.Compose(\n",
    "        [\n",
    "            Resize(\n",
    "                net_w,\n",
    "                net_h,\n",
    "                resize_target=None,\n",
    "                keep_aspect_ratio=True,\n",
    "                ensure_multiple_of=32,\n",
    "                resize_method=resize_mode,\n",
    "                image_interpolation_method=cv2.INTER_CUBIC,\n",
    "            ),\n",
    "            normalization,\n",
    "            PrepareForNet(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    midas_model.eval()\n",
    "\n",
    "    midas_model = midas_model.to(memory_format=torch.channels_last)  \n",
    "    midas_model = midas_model.half()\n",
    "\n",
    "    midas_model.to(device)\n",
    "\n",
    "    print(f\"MiDaS depth model initialized.\")\n",
    "    return midas_model, midas_transform, net_w, net_h, resize_mode, normalization\n",
    "\n",
    "def do_3d_step(img_filepath, frame_num, midas_model, midas_transform):\n",
    "  if args.key_frames:\n",
    "    translation_x = args.translation_x_series[frame_num]\n",
    "    translation_y = args.translation_y_series[frame_num]\n",
    "    translation_z = args.translation_z_series[frame_num]\n",
    "    rotation_3d_x = args.rotation_3d_x_series[frame_num]\n",
    "    rotation_3d_y = args.rotation_3d_y_series[frame_num]\n",
    "    rotation_3d_z = args.rotation_3d_z_series[frame_num]\n",
    "    print(\n",
    "        f'translation_x: {translation_x}',\n",
    "        f'translation_y: {translation_y}',\n",
    "        f'translation_z: {translation_z}',\n",
    "        f'rotation_3d_x: {rotation_3d_x}',\n",
    "        f'rotation_3d_y: {rotation_3d_y}',\n",
    "        f'rotation_3d_z: {rotation_3d_z}',\n",
    "    )\n",
    "\n",
    "  translate_xyz = [-translation_x*TRANSLATION_SCALE, translation_y*TRANSLATION_SCALE, -translation_z*TRANSLATION_SCALE]\n",
    "  rotate_xyz_degrees = [rotation_3d_x, rotation_3d_y, rotation_3d_z]\n",
    "  print('translation:',translate_xyz)\n",
    "  print('rotation:',rotate_xyz_degrees)\n",
    "  rotate_xyz = [math.radians(rotate_xyz_degrees[0]), math.radians(rotate_xyz_degrees[1]), math.radians(rotate_xyz_degrees[2])]\n",
    "  rot_mat = p3dT.euler_angles_to_matrix(torch.tensor(rotate_xyz, device=device), \"XYZ\").unsqueeze(0)\n",
    "  print(\"rot_mat: \" + str(rot_mat))\n",
    "  next_step_pil = dxf.transform_image_3d(img_filepath, midas_model, midas_transform, torch.device(\"cuda\"),\n",
    "                                          rot_mat, translate_xyz, args.near_plane, args.far_plane,\n",
    "                                          args.fov, padding_mode=args.padding_mode,\n",
    "                                          sampling_mode=args.sampling_mode, midas_weight=args.midas_weight)\n",
    "  return next_step_pil\n",
    "\n",
    "def interp(t):\n",
    "    return 3 * t**2 - 2 * t ** 3\n",
    "\n",
    "def perlin(width, height, scale=10):\n",
    "    gx, gy = torch.randn(2, width + 1, height + 1, 1, 1, device=device)\n",
    "    xs = torch.linspace(0, 1, scale + 1)[:-1, None].to(device)\n",
    "    ys = torch.linspace(0, 1, scale + 1)[None, :-1].to(device)\n",
    "    wx = 1 - interp(xs)\n",
    "    wy = 1 - interp(ys)\n",
    "    dots = 0\n",
    "    dots += wx * wy * (gx[:-1, :-1] * xs + gy[:-1, :-1] * ys)\n",
    "    dots += (1 - wx) * wy * (-gx[1:, :-1] * (1 - xs) + gy[1:, :-1] * ys)\n",
    "    dots += wx * (1 - wy) * (gx[:-1, 1:] * xs - gy[:-1, 1:] * (1 - ys))\n",
    "    dots += (1 - wx) * (1 - wy) * (-gx[1:, 1:] * (1 - xs) - gy[1:, 1:] * (1 - ys))\n",
    "    return dots.permute(0, 2, 1, 3).contiguous().view(width * scale, height * scale)\n",
    "\n",
    "def perlin_ms(octaves, width, height, grayscale):\n",
    "    out_array = [0.5] if grayscale else [0.5, 0.5, 0.5, 0.5]\n",
    "    # out_array = [0.0] if grayscale else [0.0, 0.0, 0.0]\n",
    "    for i in range(1 if grayscale else 4):\n",
    "        scale = 2 ** len(octaves)\n",
    "        oct_width = width\n",
    "        oct_height = height\n",
    "        for oct in octaves:\n",
    "            p = perlin(oct_width, oct_height, scale)\n",
    "            out_array[i] += p * oct\n",
    "            scale //= 2\n",
    "            oct_width *= 2\n",
    "            oct_height *= 2\n",
    "    return torch.cat(out_array)\n",
    "\n",
    "def create_perlin_noise(octaves=[1, 1, 1, 1], width=2, height=2, grayscale=True):\n",
    "    out = perlin_ms(octaves, width, height, grayscale)\n",
    "    if grayscale:\n",
    "        out = TF.resize(size=(args.W//args.f, args.H//args.f), img=out.unsqueeze(0))\n",
    "        out = TF.to_pil_image(out.clamp(0, 1)).convert('RGBA')\n",
    "    else:\n",
    "        out = out.reshape(-1, 4, out.shape[0]//4, out.shape[1])\n",
    "        out = TF.resize(size=(args.W//args.f, args.H//args.f), img=out)\n",
    "        out = TF.to_pil_image(out.clamp(0, 1).squeeze())\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def regen_perlin():\n",
    "    if args.perlin_mode == 'color':\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
    "    elif args.perlin_mode == 'gray':\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "    else:\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "    display(init)\n",
    "    init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "    del init2\n",
    "    return init.expand(args.batch_size, -1, -1, -1)\n",
    "\n",
    "# def save_settings():\n",
    "#     setting_list = {\n",
    "#       'text_prompts': text_prompts,\n",
    "#       'image_prompts': image_prompts,\n",
    "#       'clip_guidance_scale': clip_guidance_scale,\n",
    "#       'tv_scale': tv_scale,\n",
    "#       'range_scale': range_scale,\n",
    "#       'sat_scale': sat_scale,\n",
    "#       # 'cutn': cutn,\n",
    "#       'cutn_batches': cutn_batches,\n",
    "#       'max_frames': max_frames,\n",
    "#       'interp_spline': interp_spline,\n",
    "#       # 'rotation_per_frame': rotation_per_frame,\n",
    "#       'init_image': init_image,\n",
    "#       'init_scale': init_scale,\n",
    "#       'skip_steps': skip_steps,\n",
    "#       # 'zoom_per_frame': zoom_per_frame,\n",
    "#       'frames_scale': frames_scale,\n",
    "#       'frames_skip_steps': frames_skip_steps,\n",
    "#       'perlin_init': perlin_init,\n",
    "#       'perlin_mode': perlin_mode,\n",
    "#       'skip_augs': skip_augs,\n",
    "#       'randomize_class': randomize_class,\n",
    "#       'clip_denoised': clip_denoised,\n",
    "#       'clamp_grad': clamp_grad,\n",
    "#       'clamp_max': clamp_max,\n",
    "#       'seed': seed,\n",
    "#       'fuzzy_prompt': fuzzy_prompt,\n",
    "#       'rand_mag': rand_mag,\n",
    "#       'eta': eta,\n",
    "#       'width': width_height[0],\n",
    "#       'height': width_height[1],\n",
    "#       'diffusion_model': diffusion_model,\n",
    "#       'use_secondary_model': use_secondary_model,\n",
    "#       'steps': steps,\n",
    "#       'diffusion_steps': diffusion_steps,\n",
    "#       'diffusion_sampling_mode': diffusion_sampling_mode,\n",
    "#       'ViTB32': ViTB32,\n",
    "#       'ViTB16': ViTB16,\n",
    "#       'ViTL14': ViTL14,\n",
    "#       'ViTL14_336px': ViTL14_336px,\n",
    "#       'RN101': RN101,\n",
    "#       'RN50': RN50,\n",
    "#       'RN50x4': RN50x4,\n",
    "#       'RN50x16': RN50x16,\n",
    "#       'RN50x64': RN50x64,\n",
    "#       'ViTB32_laion2b_e16': ViTB32_laion2b_e16,\n",
    "#       'ViTB32_laion400m_e31': ViTB32_laion400m_e31,\n",
    "#       'ViTB32_laion400m_32': ViTB32_laion400m_32,\n",
    "#       'ViTB32quickgelu_laion400m_e31': ViTB32quickgelu_laion400m_e31,\n",
    "#       'ViTB32quickgelu_laion400m_e32': ViTB32quickgelu_laion400m_e32,\n",
    "#       'ViTB16_laion400m_e31': ViTB16_laion400m_e31,\n",
    "#       'ViTB16_laion400m_e32': ViTB16_laion400m_e32,\n",
    "#       'RN50_yffcc15m': RN50_yffcc15m,\n",
    "#       'RN50_cc12m': RN50_cc12m,\n",
    "#       'RN50_quickgelu_yfcc15m': RN50_quickgelu_yfcc15m,\n",
    "#       'RN50_quickgelu_cc12m': RN50_quickgelu_cc12m,\n",
    "#       'RN101_yfcc15m': RN101_yfcc15m,\n",
    "#       'RN101_quickgelu_yfcc15m': RN101_quickgelu_yfcc15m,\n",
    "#       'cut_overview': str(cut_overview),\n",
    "#       'cut_innercut': str(cut_innercut),\n",
    "#       'cut_ic_pow': str(cut_ic_pow),\n",
    "#       'cut_icgray_p': str(cut_icgray_p),\n",
    "#       'key_frames': key_frames,\n",
    "#       'max_frames': max_frames,\n",
    "#       'angle': angle,\n",
    "#       'zoom': zoom,\n",
    "#       'translation_x': translation_x,\n",
    "#       'translation_y': translation_y,\n",
    "#       'translation_z': translation_z,\n",
    "#       'rotation_3d_x': rotation_3d_x,\n",
    "#       'rotation_3d_y': rotation_3d_y,\n",
    "#       'rotation_3d_z': rotation_3d_z,\n",
    "#       'midas_depth_model': midas_depth_model,\n",
    "#       'midas_weight': midas_weight,\n",
    "#       'near_plane': near_plane,\n",
    "#       'far_plane': far_plane,\n",
    "#       'fov': fov,\n",
    "#       'padding_mode': padding_mode,\n",
    "#       'sampling_mode': sampling_mode,\n",
    "#       'video_init_path':video_init_path,\n",
    "#       'extract_nth_frame':extract_nth_frame,\n",
    "#       'video_init_seed_continuity': video_init_seed_continuity,\n",
    "#       'turbo_mode':turbo_mode,\n",
    "#       'turbo_steps':turbo_steps,\n",
    "#       'turbo_preroll':turbo_preroll,\n",
    "#       'use_horizontal_symmetry':use_horizontal_symmetry,\n",
    "#       'use_vertical_symmetry':use_vertical_symmetry,\n",
    "#       'transformation_percent':transformation_percent,\n",
    "#       #video init settings\n",
    "#       'video_init_steps': video_init_steps,\n",
    "#       'video_init_clip_guidance_scale': video_init_clip_guidance_scale,\n",
    "#       'video_init_tv_scale': video_init_tv_scale,\n",
    "#       'video_init_range_scale': video_init_range_scale,\n",
    "#       'video_init_sat_scale': video_init_sat_scale,\n",
    "#       'video_init_cutn_batches': video_init_cutn_batches,\n",
    "#       'video_init_skip_steps': video_init_skip_steps,\n",
    "#       'video_init_frames_scale': video_init_frames_scale,\n",
    "#       'video_init_frames_skip_steps': video_init_frames_skip_steps,\n",
    "#       #warp settings\n",
    "#       'video_init_flow_warp':video_init_flow_warp,\n",
    "#       'video_init_flow_blend':video_init_flow_blend,\n",
    "#       'video_init_check_consistency':video_init_check_consistency,\n",
    "#       'video_init_blend_mode':video_init_blend_mode\n",
    "#     }\n",
    "#     # print('Settings:', setting_list)\n",
    "#     with open(f\"{batchFolder}/{batch_name}({batchNum})_settings.txt\", \"w+\") as f:   #save settings\n",
    "#         json.dump(setting_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def parse_key_frames(string, prompt_parser=None):\n",
    "    \"\"\"Given a string representing frame numbers paired with parameter values at that frame,\n",
    "    return a dictionary with the frame numbers as keys and the parameter values as the values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    string: string\n",
    "        Frame numbers paired with parameter values at that frame number, in the format\n",
    "        'framenumber1: (parametervalues1), framenumber2: (parametervalues2), ...'\n",
    "    prompt_parser: function or None, optional\n",
    "        If provided, prompt_parser will be applied to each string of parameter values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Frame numbers as keys, parameter values at that frame number as values\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    RuntimeError\n",
    "        If the input string does not match the expected format.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\")\n",
    "    {10: 'Apple: 1| Orange: 0', 20: 'Apple: 0| Orange: 1| Peach: 1'}\n",
    "\n",
    "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\", prompt_parser=lambda x: x.lower()))\n",
    "    {10: 'apple: 1| orange: 0', 20: 'apple: 0| orange: 1| peach: 1'}\n",
    "    \"\"\"\n",
    "    import re\n",
    "    pattern = r'((?P<frame>[0-9]+):[\\s]*[\\(](?P<param>[\\S\\s]*?)[\\)])'\n",
    "    frames = dict()\n",
    "    for match_object in re.finditer(pattern, string):\n",
    "        frame = int(match_object.groupdict()['frame'])\n",
    "        param = match_object.groupdict()['param']\n",
    "        if prompt_parser:\n",
    "            frames[frame] = prompt_parser(param)\n",
    "        else:\n",
    "            frames[frame] = param\n",
    "\n",
    "    if frames == {} and len(string) != 0:\n",
    "        raise RuntimeError('Key Frame string not correctly formatted')\n",
    "    return frames\n",
    "\n",
    "\n",
    "def get_inbetweens(key_frames, integer=False):\n",
    "    \"\"\"Given a dict with frame numbers as keys and a parameter value as values,\n",
    "    return a pandas Series containing the value of the parameter at every frame from 0 to max_frames.\n",
    "    Any values not provided in the input dict are calculated by linear interpolation between\n",
    "    the values of the previous and next provided frames. If there is no previous provided frame, then\n",
    "    the value is equal to the value of the next provided frame, or if there is no next provided frame,\n",
    "    then the value is equal to the value of the previous provided frame. If no frames are provided,\n",
    "    all frame values are NaN.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key_frames: dict\n",
    "        A dict with integer frame numbers as keys and numerical values of a particular parameter as values.\n",
    "    integer: Bool, optional\n",
    "        If True, the values of the output series are converted to integers.\n",
    "        Otherwise, the values are floats.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        A Series with length max_frames representing the parameter values for each frame.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> max_frames = 5\n",
    "    >>> get_inbetweens({1: 5, 3: 6})\n",
    "    0    5.0\n",
    "    1    5.0\n",
    "    2    5.5\n",
    "    3    6.0\n",
    "    4    6.0\n",
    "    dtype: float64\n",
    "\n",
    "    >>> get_inbetweens({1: 5, 3: 6}, integer=True)\n",
    "    0    5\n",
    "    1    5\n",
    "    2    5\n",
    "    3    6\n",
    "    4    6\n",
    "    dtype: int64\n",
    "    \"\"\"\n",
    "    key_frame_series = pd.Series([np.nan for a in range(max_frames)])\n",
    "\n",
    "    for i, value in key_frames.items():\n",
    "        key_frame_series[i] = value\n",
    "    key_frame_series = key_frame_series.astype(float)\n",
    "\n",
    "    interp_method = interp_spline\n",
    "\n",
    "    if interp_method == 'Cubic' and len(key_frames.items()) <= 3:\n",
    "        interp_method = 'Quadratic'\n",
    "\n",
    "    if interp_method == 'Quadratic' and len(key_frames.items()) <= 2:\n",
    "        interp_method = 'Linear'\n",
    "\n",
    "    key_frame_series[0] = key_frame_series[key_frame_series.first_valid_index()]\n",
    "    key_frame_series[max_frames -\n",
    "                     1] = key_frame_series[key_frame_series.last_valid_index()]\n",
    "    # key_frame_series = key_frame_series.interpolate(method=intrp_method,order=1, limit_direction='both')\n",
    "    key_frame_series = key_frame_series.interpolate(\n",
    "        method=interp_method.lower(), limit_direction='both')\n",
    "    if integer:\n",
    "        return key_frame_series.astype(int)\n",
    "    return key_frame_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run if using score corrector (make sure to set any clip models you wish to use to True)\n",
    "import clip\n",
    "import lpips\n",
    "# use_secondary_model = False\n",
    "\n",
    "use_checkpoint = True #@param {type: 'boolean'}\n",
    "ViTB32 = False #@param{type:\"boolean\"}\n",
    "ViTB16 = False #@param{type:\"boolean\"}\n",
    "ViTL14 = False #@param{type:\"boolean\"}\n",
    "ViTL14_336px = False #@param{type:\"boolean\"}\n",
    "RN101 = False #@param{type:\"boolean\"}\n",
    "RN50 = False #@param{type:\"boolean\"}\n",
    "RN50x4 = False #@param{type:\"boolean\"}\n",
    "RN50x16 = False #@param{type:\"boolean\"}\n",
    "RN50x64 = False #@param{type:\"boolean\"}\n",
    "\n",
    "ViTB32_laion2b_e16 = True #@param{type:\"boolean\"}\n",
    "ViTB32_laion400m_e31 = False #@param{type:\"boolean\"}\n",
    "ViTB32_laion400m_32 = False #@param{type:\"boolean\"}\n",
    "ViTB32quickgelu_laion400m_e31 = False #@param{type:\"boolean\"}\n",
    "ViTB32quickgelu_laion400m_e32 = False #@param{type:\"boolean\"}\n",
    "ViTB16_laion400m_e31 = False #@param{type:\"boolean\"}\n",
    "ViTB16_laion400m_e32 = False #@param{type:\"boolean\"}\n",
    "RN50_yffcc15m = False #@param{type:\"boolean\"}\n",
    "RN50_cc12m = False #@param{type:\"boolean\"}\n",
    "RN50_quickgelu_yfcc15m = False #@param{type:\"boolean\"}\n",
    "RN50_quickgelu_cc12m = False #@param{type:\"boolean\"}\n",
    "RN101_yfcc15m = False #@param{type:\"boolean\"}\n",
    "RN101_quickgelu_yfcc15m = False #@param{type:\"boolean\"}\n",
    "\n",
    "# if use_secondary_model:\n",
    "#     secondary_model = SecondaryDiffusionImageNet2()\n",
    "#     secondary_model.load_state_dict(torch.load(f'{model_path}/secondary_model_imagenet_2.pth', map_location='cpu'))\n",
    "#     secondary_model.eval().requires_grad_(False).to(device)\n",
    "\n",
    "clip_models = []\n",
    "if ViTB32: clip_models.append(clip.load('ViT-B/32', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTB16: clip_models.append(clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTL14: clip_models.append(clip.load('ViT-L/14', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTL14_336px: clip_models.append(clip.load('ViT-L/14@336px', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN50: clip_models.append(clip.load('RN50', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN50x4: clip_models.append(clip.load('RN50x4', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN50x16: clip_models.append(clip.load('RN50x16', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN50x64: clip_models.append(clip.load('RN50x64', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN101: clip_models.append(clip.load('RN101', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTB32_laion2b_e16: clip_models.append(open_clip.create_model('ViT-B-32', pretrained='laion2b_e16').eval().requires_grad_(False).to(device))\n",
    "if ViTB32_laion400m_e31: clip_models.append(open_clip.create_model('ViT-B-32', pretrained='laion400m_e31').eval().requires_grad_(False).to(device))\n",
    "if ViTB32_laion400m_32: clip_models.append(open_clip.create_model('ViT-B-32', pretrained='laion400m_e32').eval().requires_grad_(False).to(device))\n",
    "if ViTB32quickgelu_laion400m_e31: clip_models.append(open_clip.create_model('ViT-B-32-quickgelu', pretrained='laion400m_e31').eval().requires_grad_(False).to(device))\n",
    "if ViTB32quickgelu_laion400m_e32: clip_models.append(open_clip.create_model('ViT-B-32-quickgelu', pretrained='laion400m_e32').eval().requires_grad_(False).to(device))\n",
    "if ViTB16_laion400m_e31: clip_models.append(open_clip.create_model('ViT-B-16', pretrained='laion400m_e31').eval().requires_grad_(False).to(device))\n",
    "if ViTB16_laion400m_e32: clip_models.append(open_clip.create_model('ViT-B-16', pretrained='laion400m_e32').eval().requires_grad_(False).to(device))\n",
    "if RN50_yffcc15m: clip_models.append(open_clip.create_model('RN50', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\n",
    "if RN50_cc12m: clip_models.append(open_clip.create_model('RN50', pretrained='cc12m').eval().requires_grad_(False).to(device))\n",
    "if RN50_quickgelu_yfcc15m: clip_models.append(open_clip.create_model('RN50-quickgelu', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\n",
    "if RN50_quickgelu_cc12m: clip_models.append(open_clip.create_model('RN50-quickgelu', pretrained='cc12m').eval().requires_grad_(False).to(device))\n",
    "if RN101_yfcc15m: clip_models.append(open_clip.create_model('RN101', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\n",
    "if RN101_quickgelu_yfcc15m: clip_models.append(open_clip.create_model('RN101-quickgelu', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\n",
    "\n",
    "normalize = T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "lpips_model = lpips.LPIPS(net='vgg').to(device)\n",
    "\n",
    "loss_values = []\n",
    "\n",
    "def parse_prompt(prompt):\n",
    "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
    "        vals = prompt.rsplit('::', 2)\n",
    "        vals = [vals[0] + '::' + vals[1], *vals[2:]]\n",
    "    else:\n",
    "        vals = prompt.rsplit('::', 1)\n",
    "    vals = vals + ['', '1'][len(vals):]\n",
    "    return vals[0], float(vals[1])\n",
    "\n",
    "def get_clip_embeds(prompts, seed):\n",
    "    seed_everything(seed)\n",
    "    target_embeds, weights = [], []\n",
    "    model_stats = []\n",
    "    for clip_model in clip_models:\n",
    "        cutn = 16\n",
    "        model_stat = {\"clip_model\": None, \"target_embeds\": [],\n",
    "                    \"make_cutouts\": None, \"weights\": []}\n",
    "        model_stat[\"clip_model\"] = clip_model\n",
    "\n",
    "        for prompt in prompts:\n",
    "            txt, weight = parse_prompt(prompt)\n",
    "            txt = clip_model.encode_text(clip.tokenize(prompt).to(device)).float()\n",
    "\n",
    "            if fuzzy_prompt:\n",
    "                for i in range(25):\n",
    "                    model_stat[\"target_embeds\"].append(\n",
    "                        (txt + torch.randn(txt.shape).cuda() * args.rand_mag).clamp(0, 1))\n",
    "                    model_stat[\"weights\"].append(weight)\n",
    "            else:\n",
    "                model_stat[\"target_embeds\"].append(txt)\n",
    "                model_stat[\"weights\"].append(weight)\n",
    "\n",
    "        if image_prompt:\n",
    "            model_stat[\"make_cutouts\"] = MakeCutouts(\n",
    "                clip_model.visual.input_resolution, cutn, skip_augs=skip_augs)\n",
    "            for prompt in image_prompt:\n",
    "                path, weight = parse_prompt(prompt)\n",
    "                img = Image.open(fetch(path)).convert('RGB')\n",
    "                img = TF.resize(img, min(side_x, side_y, *img.size),\n",
    "                                T.InterpolationMode.LANCZOS)\n",
    "                batch = model_stat[\"make_cutouts\"](TF.to_tensor(\n",
    "                    img).to(device).unsqueeze(0).mul(2).sub(1))\n",
    "                embed = clip_model.encode_image(normalize(batch)).float()\n",
    "                if fuzzy_prompt:\n",
    "                    for i in range(25):\n",
    "                        model_stat[\"target_embeds\"].append(\n",
    "                            (embed + torch.randn(embed.shape).cuda() * rand_mag).clamp(0, 1))\n",
    "                        weights.extend([weight / cutn] * cutn)\n",
    "                else:\n",
    "                    model_stat[\"target_embeds\"].append(embed)\n",
    "                    model_stat[\"weights\"].extend([weight / cutn] * cutn)\n",
    "\n",
    "        model_stat[\"target_embeds\"] = torch.cat(model_stat[\"target_embeds\"])\n",
    "        model_stat[\"weights\"] = torch.tensor(model_stat[\"weights\"], device=device)\n",
    "        if model_stat[\"weights\"].sum().abs() < 1e-3:\n",
    "            raise RuntimeError('The weights must not sum to 0.')\n",
    "        model_stat[\"weights\"] /= model_stat[\"weights\"].sum().abs()\n",
    "        model_stats.append(model_stat)\n",
    "    return model_stats\n",
    "\n",
    "#############################################################\n",
    "\n",
    "cutout_debug = False\n",
    "padargs = {}\n",
    "class MakeCutoutsDango(nn.Module):\n",
    "    def __init__(self, cut_size,\n",
    "                 Overview=4,\n",
    "                 InnerCrop=0, IC_Size_Pow=0.5, IC_Grey_P=0.2\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.Overview = Overview\n",
    "        self.InnerCrop = InnerCrop\n",
    "        self.IC_Size_Pow = IC_Size_Pow\n",
    "        self.IC_Grey_P = IC_Grey_P\n",
    "        if animation_mode == 'None':\n",
    "            self.augs = T.Compose([\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomAffine(degrees=10, translate=(0.05, 0.05),\n",
    "                               interpolation=T.InterpolationMode.BILINEAR),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomGrayscale(p=0.1),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.ColorJitter(brightness=0.1, contrast=0.1,\n",
    "                              saturation=0.1, hue=0.1),\n",
    "            ])\n",
    "        elif animation_mode == 'Video Input':\n",
    "            self.augs = T.Compose([\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomPerspective(distortion_scale=0.4, p=0.7),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomGrayscale(p=0.15),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                # T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "            ])\n",
    "        elif animation_mode == '2D' or animation_mode == '3D':\n",
    "            self.augs = T.Compose([\n",
    "                T.RandomHorizontalFlip(p=0.4),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomAffine(degrees=10, translate=(0.05, 0.05),\n",
    "                               interpolation=T.InterpolationMode.BILINEAR),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomGrayscale(p=0.1),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.ColorJitter(brightness=0.1, contrast=0.1,\n",
    "                              saturation=0.1, hue=0.3),\n",
    "            ])\n",
    "            \n",
    "    def forward(self, input):\n",
    "        cutouts = []\n",
    "        gray = T.Grayscale(3)\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        l_size = max(sideX, sideY)\n",
    "        output_shape = [1,3,self.cut_size,self.cut_size] \n",
    "        output_shape_2 = [1,3,self.cut_size+2,self.cut_size+2]\n",
    "        pad_input = F.pad(input,((sideY-max_size)//2,(sideY-max_size)//2,(sideX-max_size)//2,(sideX-max_size)//2), **padargs)\n",
    "        cutout = resize(pad_input, out_shape=output_shape)\n",
    "\n",
    "        if self.Overview>0:\n",
    "            if self.Overview<=4:\n",
    "                if self.Overview>=1:\n",
    "                    cutouts.append(cutout)\n",
    "                if self.Overview>=2:\n",
    "                    cutouts.append(gray(cutout))\n",
    "                if self.Overview>=3:\n",
    "                    cutouts.append(TF.hflip(cutout))\n",
    "                if self.Overview==4:\n",
    "                    cutouts.append(gray(TF.hflip(cutout)))\n",
    "            else:\n",
    "                cutout = resize(pad_input, out_shape=output_shape)\n",
    "                for _ in range(self.Overview):\n",
    "                    cutouts.append(cutout)\n",
    "\n",
    "            if cutout_debug:\n",
    "                if is_colab:\n",
    "                    TF.to_pil_image(cutouts[0].clamp(0, 1).squeeze(0)).save(\"/content/cutout_overview0.jpg\",quality=99)\n",
    "                else:\n",
    "                    TF.to_pil_image(cutouts[0].clamp(0, 1).squeeze(0)).save(\"cutout_overview0.jpg\",quality=99)\n",
    "\n",
    "                              \n",
    "        if self.InnerCrop >0:\n",
    "            for i in range(self.InnerCrop):\n",
    "                size = int(torch.rand([])**self.IC_Size_Pow * (max_size - min_size) + min_size)\n",
    "                offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "                offsety = torch.randint(0, sideY - size + 1, ())\n",
    "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "                if i <= int(self.IC_Grey_P * self.InnerCrop):\n",
    "                    cutout = gray(cutout)\n",
    "                cutout = resize(cutout, out_shape=output_shape)\n",
    "                cutouts.append(cutout)\n",
    "            if cutout_debug:\n",
    "                TF.to_pil_image(cutouts[-1].clamp(0, 1).squeeze(0)).save(\"cutout_InnerCrop.jpg\",quality=99)\n",
    "        cutouts = torch.cat(cutouts)\n",
    "        if skip_augs is not True: cutouts=self.augs(cutouts)\n",
    "        return cutouts\n",
    "\n",
    "def spherical_dist_loss(x, y):\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    y = F.normalize(y, dim=-1)\n",
    "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)     \n",
    "\n",
    "def tv_loss(input):\n",
    "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
    "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
    "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
    "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
    "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
    "\n",
    "\n",
    "def range_loss(input):\n",
    "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])\n",
    "\n",
    "def _extract_into_tensor(arr, timesteps, broadcast_shape):\n",
    "    \"\"\"\n",
    "    Extract values from a 1-D numpy array for a batch of indices.\n",
    "    :param arr: the 1-D numpy array.\n",
    "    :param timesteps: a tensor of indices into the array to extract.\n",
    "    :param broadcast_shape: a larger shape of K dimensions with the batch\n",
    "                            dimension equal to the length of timesteps.\n",
    "    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        res = torch.from_numpy(arr).to(device=device)[timesteps].float()\n",
    "    except:\n",
    "         res = arr[timesteps].float()\n",
    "    while len(res.shape) < len(broadcast_shape):\n",
    "        res = res[..., None]\n",
    "    return res.expand(broadcast_shape)\n",
    "\n",
    "def cond_fn(diffusion, x, t, c, y=None):\n",
    "    cur_t = t\n",
    "    with torch.enable_grad():\n",
    "        x_is_NaN = False\n",
    "        x = x.detach().requires_grad_()\n",
    "        n = x.shape[0]\n",
    "        # if use_secondary_model is True:\n",
    "        #     alpha = torch.tensor(\n",
    "        #         diffusion.sqrt_alphas_cumprod[cur_t], device=device, dtype=torch.float32)\n",
    "        #     sigma = torch.tensor(\n",
    "        #         diffusion.sqrt_one_minus_alphas_cumprod[cur_t], device=device, dtype=torch.float32)\n",
    "        #     cosine_t = alpha_sigma_to_t(alpha, sigma)\n",
    "        #     out = secondary_model(x, cosine_t[None].repeat([n])).pred\n",
    "        #     fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
    "        #     x_in = out * fac + x * (1 - fac)\n",
    "        #     x_in_grad = torch.zeros_like(x_in)\n",
    "        # else:\n",
    "        my_t = torch.ones([n], device=device, dtype=torch.long) * cur_t\n",
    "        out = diffusion.p_mean_variance(x, c, my_t, clip_denoised=False, return_x0=False)\n",
    "        fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
    "        x_in = out[-1] * fac + x * (1 - fac)\n",
    "        x_in.requires_grad_()\n",
    "        x_in_grad = torch.zeros_like(x_in)\n",
    "        if clip_guidance_scale != 0:\n",
    "            for model_stat in model_stats:\n",
    "                for i in range(cutn_batches):\n",
    "                    t_int = int(t.item())+1 # errors on last step without +1, need to find source\n",
    "                    # when using SLIP Base model the dimensions need to be hard coded to avoid AttributeError: 'VisionTransformer' object has no attribute 'input_resolution'\n",
    "                    try:\n",
    "                        input_resolution = model_stat[\"clip_model\"].visual.input_resolution\n",
    "                    except:\n",
    "                        input_resolution = 224\n",
    "                    cuts = MakeCutoutsDango(input_resolution,\n",
    "                                            Overview=cut_overview[1000-t_int],\n",
    "                                            InnerCrop=cut_innercut[1000-t_int],\n",
    "                                            IC_Size_Pow=cut_ic_pow[1000-t_int],\n",
    "                                            IC_Grey_P=cut_icgray_p[1000-t_int]\n",
    "                                            )\n",
    "                    clip_in = normalize(cuts(model.differentiable_decode_first_stage(x_in).add(1).div(2)))\n",
    "                    image_embeds = model_stat[\"clip_model\"].encode_image(\n",
    "                        clip_in).float()\n",
    "                    dists = spherical_dist_loss(image_embeds.unsqueeze(\n",
    "                        1), model_stat[\"target_embeds\"].unsqueeze(0))\n",
    "                    dists = dists.view(\n",
    "                        [cut_overview[1000-t_int]+cut_innercut[1000-t_int], n, -1])\n",
    "                    losses = dists.mul(model_stat[\"weights\"]).sum(2).mean(0)\n",
    "                    # log loss, probably shouldn't do per cutn_batch\n",
    "                    loss_values.append(losses.sum().item())\n",
    "                    x_in_grad += torch.autograd.grad(losses.sum()\n",
    "                                                    * clip_guidance_scale, x_in)[0] / cutn_batches\n",
    "        if use_tv_scale != 0:\n",
    "            tv_losses = tv_loss(model.differentiable_decode_first_stage(x_in))\n",
    "        else:\n",
    "            tv_losses = toch.tensor(0)\n",
    "        # if use_secondary_model is True:\n",
    "        #     range_losses = range_loss(out)\n",
    "        # else:\n",
    "        if range_scale != 0:\n",
    "            range_losses = range_loss(model.differentiable_decode_first_stage(x_in))\n",
    "        else:\n",
    "            range_losses = torch.tensor(0)\n",
    "        if sat_scale != 0:\n",
    "            sat_losses = torch.abs(model.differentiable_decode_first_stage(x_in) - model.differentiable_decode_first_stage(x_in).clamp(min=-1, max=1)).mean()\n",
    "        else:\n",
    "            sat_losses = torch.tensor(0)\n",
    "        loss = tv_losses.sum() * tv_scale + range_losses.sum() * \\\n",
    "            range_scale + sat_losses.sum() * sat_scale\n",
    "        if init is not None and init_scale != 0:\n",
    "            init_losses = lpips_model(model.differentiable_decode_first_stage(x_in), model.differentiable_decode_first_stage(model.q_sample(init_latent, t)))\n",
    "            loss = loss + init_losses.sum() * init_scale\n",
    "        x_in_grad += torch.autograd.grad(loss, x_in)[0]\n",
    "        if torch.isnan(x_in_grad).any() == False:\n",
    "            grad = -torch.autograd.grad(x_in, x, x_in_grad)[0]\n",
    "        else:\n",
    "            # print(\"NaN'd\")\n",
    "            x_is_NaN = True\n",
    "            grad = torch.zeros_like(x)\n",
    "    if clamp_grad and x_is_NaN == False:\n",
    "        # min=-0.02, min=-clamp_max,\n",
    "        magnitude = grad.square().mean().sqrt()\n",
    "        return grad * magnitude.clamp(max=clamp_max) / magnitude\n",
    "    return grad\n",
    "\n",
    "class ScoreCorrector():\n",
    "    def __init__():\n",
    "        return\n",
    "    def modify_score(model, e_t, x, t, c, corrector_kwargs={}):\n",
    "        \"\"\"\n",
    "        Compute what the p_mean_variance output would have been, should the\n",
    "        model's score function be conditioned by cond_fn.\n",
    "        See condition_mean() for details on cond_fn.\n",
    "        Unlike condition_mean(), this instead uses the conditioning strategy\n",
    "        from Song et al (2020).\n",
    "        \"\"\"\n",
    "        alpha_bar = _extract_into_tensor(model.alphas_cumprod, t, x.shape)\n",
    "\n",
    "        eps = model._predict_eps_from_xstart(x, t, e_t)\n",
    "        eps = eps - (1 - alpha_bar).sqrt() * cond_fn(\n",
    "            model, x, t, c, **corrector_kwargs\n",
    "        )\n",
    "\n",
    "        # out = p_mean_var.copy()\n",
    "        out = model.predict_start_from_noise(x, t, eps)\n",
    "        # out[\"pred_xstart\"] = model._predict_xstart_from_eps(x, t, eps)\n",
    "        # out[\"mean\"], _, _ = model.q_posterior_mean_variance(\n",
    "        #     x_start=out[\"pred_xstart\"], x_t=x, t=t\n",
    "        # )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "BPnyd-XUKbfE",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0fcd10e4-0df2-4ab9-cbf5-f08f4902c954",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up model and sampler\n",
    "model = get_model()\n",
    "\n",
    "if use_plms:\n",
    "    sampler = PLMSSampler(model)\n",
    "    ddimsampler = DDIMSampler(model)\n",
    "else:\n",
    "    sampler = DDIMSampler(model)\n",
    "\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "sample_path = os.path.join(outdir, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "grid_count = len(os.listdir(outdir)) - 1\n",
    "\n",
    "if use_k_lms:\n",
    "    class CFGDenoiser(nn.Module):\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.inner_model = model\n",
    "\n",
    "        def forward(self, x, sigma, uncond, cond, cond_scale):\n",
    "            x_in = torch.cat([x] * 2)\n",
    "            sigma_in = torch.cat([sigma] * 2)\n",
    "            cond_in = torch.cat([uncond, cond])\n",
    "            uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
    "            return uncond + (cond - uncond) * cond_scale\n",
    "        \n",
    "    model_wrap = K.external.CompVisDenoiser(model)\n",
    "    sigma_min, sigma_max = model_wrap.sigmas[0].item(), model_wrap.sigmas[-1].item()\n",
    "    ddimsampler = DDIMSampler(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text -> Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "prompts = [\n",
    "    \"joe biden::1 happy::0.2\"\n",
    "] # list of string prompts - negative weights don't work well, needs investigating\n",
    "\n",
    "seed = 741 # seed for reproducible generations - use None for random seed\n",
    "average_weights = True # If using prompt weights, whether or not to average them\n",
    "\n",
    "init_image_location = \"\" # file location of init image (use a blank string if none desired)\n",
    "init_noise_strength = 0.75 # how much to noise init image (0-1.0 where 1.0 is full destruction of init image information)\n",
    "\n",
    "ddim_steps = 50 # ddim sampling steps\n",
    "ddim_eta = 0.0 # ddim eta (eta=0.0 corresponds to deterministic sampling) (must be 0.0 if using PLMS/k_lms sampling)\n",
    "unconditional_guidance_scale = 7.5 # unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\n",
    "\n",
    "precision = 'autocast' # precision to evaluate at (full or autocast)\n",
    "\n",
    "n_iter = 1 # how many sample iterations\n",
    "batch_size = 1 # how many samples to generate per prompt\n",
    "n_rows = 0 # rows in the grid (will use batch_size if set to 0)\n",
    "\n",
    "fixed_code = True # if enabled, uses the same starting code across samples\n",
    "skip_grid = True # do not save a grid, only individual samples\n",
    "skip_save = False # do not save individual samples\n",
    "show_images = True # whether or not to show images after generation\n",
    "\n",
    "H = 512 # height\n",
    "W = 512 # width\n",
    "C = 4 # channels\n",
    "f = 8 # downsampling factor\n",
    "\n",
    "use_score_corrector = False # whether or not to use score correction\n",
    "# Score corrector parameters\n",
    "fuzzy_prompt = False\n",
    "image_prompt = False\n",
    "\n",
    "cutn_batches = 2\n",
    "\n",
    "clip_guidance_scale = 0\n",
    "tv_scale = 0\n",
    "range_scale = 0\n",
    "sat_scale = 0\n",
    "\n",
    "init = None\n",
    "init_scale = 0\n",
    "\n",
    "animation_mode = 'None'\n",
    "skip_augs = True\n",
    "\n",
    "clamp_max = 0.02\n",
    "clamp_grad = True\n",
    "\n",
    "cut_overview = [12]*400+[4]*600 \n",
    "cut_innercut = [4]*400+[12]*600 \n",
    "cut_ic_pow = [1]*1000\n",
    "cut_icgray_p = [0.2]*400+[0]*600\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "if n_rows == 0:\n",
    "    n_rows = batch_size\n",
    "\n",
    "if seed is None:\n",
    "    seed = np.random.randint(np.iinfo(np.int32).max)\n",
    "seed_everything(seed)\n",
    "\n",
    "start_code = None\n",
    "if fixed_code:\n",
    "    start_code = torch.randn([batch_size, C, H // f, W // f], device=device)\n",
    "\n",
    "precision_scope = autocast if precision==\"autocast\" else nullcontext\n",
    "data = list(chunk(prompts, batch_size))\n",
    "\n",
    "if init_image_location != \"\":\n",
    "    assert os.path.isfile(init_image_location)\n",
    "    init_image = load_img(init_image_location, (W, H)).to(device)\n",
    "    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "    init_image_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "\n",
    "    sampler.make_schedule(ddim_num_steps=ddim_steps, ddim_eta=ddim_eta, verbose=False)\n",
    "\n",
    "    assert 0. <= init_noise_strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "    t_enc = int(init_noise_strength * ddim_steps)\n",
    "    print(f\"target t_enc is {t_enc} steps\")\n",
    "if isinstance(init, str):\n",
    "    assert os.path.isfile(init)\n",
    "    init_image_ = load_img(init, (W, H)).to(device)\n",
    "    init_image_ = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image_))  # move to latent space\n",
    "else:\n",
    "    init = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run generation(s)\n",
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            tic = time.time()\n",
    "            all_samples = list()\n",
    "            seed_everything(seed)\n",
    "            for n in trange(n_iter, desc=\"Sampling\"):\n",
    "                for prompts in tqdm(data, desc=\"data\"):\n",
    "                    if use_score_corrector:\n",
    "                        model_stats = get_clip_embeds(prompts, seed)\n",
    "                    uc = None\n",
    "                    if unconditional_guidance_scale != 1.0:\n",
    "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "                    c = torch.cat([get_conditioning_vector(prompt) for prompt in prompts])\n",
    "\n",
    "                    if len(init_image_location) == 0 or init_image_location == '':\n",
    "                        shape = [C, H // f, W // f]\n",
    "                        if use_k_lms:\n",
    "                            sigmas = model_wrap.get_sigmas(ddim_steps)\n",
    "                            torch.manual_seed(seed) # changes manual seeding procedure\n",
    "                            x = torch.randn([batch_size, *shape], device=device) * sigmas[0] # for GPU draw\n",
    "                            model_wrap_cfg = CFGDenoiser(model_wrap)\n",
    "                            extra_args = {'cond': c, 'uncond': uc, 'cond_scale': unconditional_guidance_scale}\n",
    "                            samples_ddim = K.sampling.sample_lms(model_wrap_cfg, x, sigmas, extra_args=extra_args)\n",
    "                        else:\n",
    "                            samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                                conditioning=c,\n",
    "                                                                batch_size=batch_size,\n",
    "                                                                shape=shape,\n",
    "                                                                verbose=False,\n",
    "                                                                unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                                                unconditional_conditioning=uc,\n",
    "                                                                eta=ddim_eta,\n",
    "                                                                x_T=start_code,\n",
    "                                                                score_corrector=ScoreCorrector if use_score_corrector else None,\n",
    "                                                                corrector_kwargs={}\n",
    "                                                            )\n",
    "                    else:\n",
    "                        # encode (scaled latent)\n",
    "                        z_enc = sampler.stochastic_encode(init_image_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "                        # decode it\n",
    "                        samples_ddim = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                                unconditional_conditioning=uc)\n",
    "\n",
    "                    x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                    x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                    if not skip_save:\n",
    "                        for x_sample in x_samples_ddim:\n",
    "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                            img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                            img.save(\n",
    "                                os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "                            base_count += 1\n",
    "                            \n",
    "                            if show_images:\n",
    "                                display(img)\n",
    "                                \n",
    "                    if skip_save and show_images:\n",
    "                        for x_sample in x_samples_ddim:\n",
    "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                            img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                            display(img)\n",
    "\n",
    "                    if not skip_grid:\n",
    "                        all_samples.append(x_samples_ddim)\n",
    "\n",
    "            if not skip_grid:\n",
    "                # additionally, save as grid\n",
    "                grid = torch.stack(all_samples, 0)\n",
    "                grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "                # to image\n",
    "                grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "                Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outdir, f'grid-{grid_count:04}.png'))\n",
    "                grid_count += 1\n",
    "\n",
    "            toc = time.time()\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outdir}\\n\"\n",
    "        f\"\\nTime elapsed: {round(toc - tic, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "prompts = [\n",
    "    \"A waterpainting of the universe by studio ghibli\",\n",
    "    \"A waterpainting of the comsos by studio ghibli\",\n",
    "    \"A waterpainting of the birth of space by studio ghibli\"\n",
    "] # list of string prompts - negative weights don't work well, needs investigating\n",
    "seed = 987643416 # seed for reproducible generations - use None for random seed\n",
    "average_weights = True # If using prompt weights, whether or not to average them\n",
    "\n",
    "frame = 2 # only used when running single_generations (for easier animation)\n",
    "\n",
    "project_name = 'outcrop'\n",
    "out_crop_dir = \"out_crop\" # directory to save images in\n",
    "\n",
    "outcropping_animation = True # whether or not to run an outcropping animation\n",
    "outcropping_factor = 0.5 # how much to outcrop each step (0.0 - 1.0)\n",
    "outcropping_frames = 10\n",
    "\n",
    "original_image_location = \"\" # file location of the original image\n",
    "mask_location = \"/workspace/stable-diffusion/mask.png\" # file location of the image mask (not used if using outcropping_animation)\n",
    "\n",
    "ddim_steps = 80 # ddim sampling steps\n",
    "ddim_eta = 0.0 # ddim eta (eta=0.0 corresponds to deterministic sampling) (must be 0.0 if using PLMS/k_lms sampling)\n",
    "unconditional_guidance_scale = 7.5 # unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\n",
    "\n",
    "precision = 'autocast' # precision to evaluate at (full or autocast)\n",
    "\n",
    "n_iter = 1 # how many sample iterations\n",
    "batch_size = 1 # how many samples to generate per prompt\n",
    "n_rows = 0 # rows in the grid (will use batch_size if set to 0)\n",
    "\n",
    "fixed_code = True # if enabled, uses the same starting code across samples\n",
    "increment_seed_every_frame = True # whether or not to increment the seed by the frame (only with fixed code)\n",
    "randomize_seed_every_frame = True # whether or not to randomize the seed by the frame (only with False increment_seed_every_frame)\n",
    "skip_grid = True # do not save a grid, only individual samples\n",
    "skip_save = False # do not save individual samples\n",
    "show_images = True # whether or not to show images after generation\n",
    "\n",
    "H = 512 # height\n",
    "W = 512 # width\n",
    "C = 4 # channels\n",
    "f = 8 # downsampling factor\n",
    "\n",
    "use_score_corrector = False # whether or not to use score correction\n",
    "# Score corrector parameters\n",
    "fuzzy_prompt = False\n",
    "image_prompt = False\n",
    "\n",
    "cutn_batches = 2\n",
    "\n",
    "clip_guidance_scale = 0\n",
    "tv_scale = 0\n",
    "range_scale = 0\n",
    "sat_scale = 0\n",
    "\n",
    "init = None\n",
    "init_scale = 0\n",
    "\n",
    "animation_mode = 'None'\n",
    "skip_augs = True\n",
    "\n",
    "clamp_max = 0.02\n",
    "clamp_grad = True\n",
    "\n",
    "cut_overview = [12]*400+[4]*600 \n",
    "cut_innercut = [4]*400+[12]*600 \n",
    "cut_ic_pow = [1]*1000\n",
    "cut_icgray_p = [0.2]*400+[0]*600\n",
    "\n",
    "####################################################################################\n",
    "show_generated_mask_and_image = False # debug\n",
    "\n",
    "if n_rows == 0:\n",
    "    n_rows = batch_size\n",
    "\n",
    "if seed is None:\n",
    "    seed = np.random.randint(np.iinfo(np.int32).max)\n",
    "seed_everything(seed)\n",
    "\n",
    "start_code = None\n",
    "if fixed_code:\n",
    "    start_code = torch.randn([batch_size, C, H // f, W // f], device=device)\n",
    "\n",
    "precision_scope = autocast if precision==\"autocast\" else nullcontext\n",
    "\n",
    "if outcropping_animation:\n",
    "    data = prompts * (outcropping_frames//len(prompts))\n",
    "    data.extend(prompts[:outcropping_frames%len(prompts)])\n",
    "    data = [[d] for d in data]\n",
    "else:\n",
    "    data = chunk(prompts, batch_size)\n",
    "\n",
    "if original_image_location != '':\n",
    "    assert os.path.isfile(original_image_location)\n",
    "    init = True\n",
    "    original_image = load_img(original_image_location, (W, H), outcrop_animation=outcropping_animation, factor=outcropping_factor).to(device)\n",
    "    original_image = repeat(original_image, '1 ... -> b ...', b=batch_size)\n",
    "    x0 = model.get_first_stage_encoding(model.encode_first_stage(original_image))  # move to latent space\n",
    "    init_latent = x0\n",
    "if isinstance(init, str):\n",
    "    assert os.path.isfile(init)\n",
    "    init_image_ = load_img(init, (W, H)).to(device)\n",
    "    init_image_ = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image_))  # move to latent space\n",
    "else:\n",
    "    init = None\n",
    "\n",
    "if not outcropping_animation:\n",
    "    assert os.path.isfile(mask_location)\n",
    "mask = load_img(mask_location, (W//f, H//f), mask=True, outcrop_animation=outcropping_animation, factor=outcropping_factor, show_generated_mask_and_image=show_generated_mask_and_image).to(device)\n",
    "mask = repeat(mask, '1 ... -> b ...', b=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run generation(s)\n",
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            tic = time.time()\n",
    "            all_samples = list()\n",
    "            for n in trange(n_iter, desc=\"Sampling\"):\n",
    "                if outcropping_animation:\n",
    "                    if original_image_location == '':\n",
    "                        frame = 1\n",
    "                        x0 = None\n",
    "                        mask_ = mask\n",
    "                        mask = None\n",
    "                        init = None\n",
    "                    else:\n",
    "                        frame = 2\n",
    "                for prompts in tqdm(data, desc=\"data\"):\n",
    "                    if use_score_corrector:\n",
    "                        model_stats = get_clip_embeds(prompts, seed)\n",
    "                    if outcropping_animation and (frame > 2 or (original_image_location == '' and frame == 2)):\n",
    "                        init = True\n",
    "                        image = load_img('prevframe.png', (W, H), outcrop_animation=True, factor=outcropping_factor).to(device)\n",
    "                        image = repeat(image, '1 ... -> b ...', b=batch_size)\n",
    "                        x0 = model.get_first_stage_encoding(model.encode_first_stage(image))\n",
    "\n",
    "                        if use_score_corrector:\n",
    "                            image = load_img('prevframe.png', (W, H), outcrop_animation=False, factor=outcropping_factor).to(device) # load image without cropping\n",
    "                            image = repeat(image, '1 ... -> b ...', b=batch_size)\n",
    "                            init_latent = model.get_first_stage_encoding(model.encode_first_stage(image))\n",
    "                        if increment_seed_every_frame and fixed_code:\n",
    "                            seed_everything(seed+frame)\n",
    "                            start_code = torch.randn([batch_size, C, H // f, W // f], device=device)\n",
    "                        elif randomize_seed_every_frame:\n",
    "                            start_code = None\n",
    "                            if fixed_code:\n",
    "                                start_code = torch.randn([batch_size, C, H // f, W // f], device=device)\n",
    "                        if mask == None:\n",
    "                            mask = mask_\n",
    "                                \n",
    "                    uc = None\n",
    "                    if unconditional_guidance_scale != 1.0:\n",
    "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "                    c = torch.cat([get_conditioning_vector(prompt) for prompt in prompts])\n",
    "\n",
    "                    shape = [C, H // f, W // f]\n",
    "                    samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                        conditioning=c,\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shape=shape,\n",
    "                                                        verbose=False,\n",
    "                                                        unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                                        unconditional_conditioning=uc,\n",
    "                                                        eta=ddim_eta,\n",
    "                                                        x_T=start_code,\n",
    "                                                        x0=x0,\n",
    "                                                        mask=mask,\n",
    "                                                        score_corrector=ScoreCorrector if use_score_corrector else None,\n",
    "                                                        corrector_kwargs={})\n",
    "\n",
    "                    x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                    x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                    if outcropping_animation:\n",
    "                        for x_sample in x_samples_ddim:\n",
    "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                            img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                            img.save(os.path.join(\"prevframe.png\"))\n",
    "\n",
    "                    if not skip_save:\n",
    "                        for i, x_sample in enumerate(x_samples_ddim):\n",
    "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                            img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                            if i == 0:\n",
    "                                img.save(\n",
    "                                        os.path.join(out_crop_dir, f\"{project_name}_{frame}.png\"))\n",
    "                            else:\n",
    "                                img.save(\n",
    "                                        os.path.join(out_crop_dir, f\"{project_name}_{i}_{frame}.png\"))\n",
    "                            base_count += 1\n",
    "                            frame += 1\n",
    "                            \n",
    "                            if show_images:\n",
    "                                display(img)\n",
    "                                \n",
    "                    if skip_save and show_images:\n",
    "                        for x_sample in x_samples_ddim:\n",
    "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                            img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                            display(img)\n",
    "\n",
    "                    if not skip_grid:\n",
    "                        all_samples.append(x_samples_ddim)\n",
    "\n",
    "            if not skip_grid:\n",
    "                # additionally, save as grid\n",
    "                grid = torch.stack(all_samples, 0)\n",
    "                grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "                # to image\n",
    "                grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "                Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outdir, f'grid-{grid_count:04}.png'))\n",
    "                grid_count += 1\n",
    "\n",
    "            toc = time.time()\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outdir}\\n\"\n",
    "        f\"\\nTime elapsed: {round(toc - tic, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save outcropping frames to mp4 animation\n",
    "out_mp4 = 'outcrop.mp4'\n",
    "frames_between_crops = 30\n",
    "fps = 30\n",
    "frame_count = None # leave none to use outcropping frames (number of outcropped frames)\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "if frame_count == None:\n",
    "    frame_count = outcropping_frames\n",
    "\n",
    "outcropping_frames_to_mp4(out_crop_dir, project_name, out_mp4, frame_count, frames_between_crops, fps, outcropping_factor, zoom_cropping=(0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text, Text -> Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "92QkRfm0e6K0",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "prompts = [\n",
    "    (26208852, \"Joe Biden exhaling a large smoke cloud from his bong, candid photography\"),\n",
    "    (685626752, \"Barack Obama exhaling a large smoke cloud from his bong, candid photography\"),\n",
    "] # (seed, prompt)\n",
    "\n",
    "save_mp4 = 'test.mp4'\n",
    "\n",
    "average_weights = True # If using prompt weights, whether or not to average them\n",
    "\n",
    "ddim_steps = 50 # ddim sampling steps\n",
    "ddim_eta = 0.0 # ddim eta (eta=0.0 corresponds to deterministic sampling) (must be 0.0 if using PLMS sampling)\n",
    "unconditional_guidance_scale = 7.5 # unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\n",
    "\n",
    "precision = 'autocast' # precision to evaluate at (full or autocast)\n",
    "\n",
    "batch_size = 10 # how many samples to generate per prompt (currently must be set to one for animation)\n",
    "\n",
    "loop = True # if enabled, make animation loop by interpolating between end and start vectors\n",
    "fixed_code = False # if enabled, uses the same starting code across samples\n",
    "fixed_seed = None # fixed seed to use if using fixed_code\n",
    "skip_save = True # do not save individual frames\n",
    "skip_save_video = False # do not save mp4\n",
    "show_images = True # whether or not to show images after generation\n",
    "\n",
    "H = 512 # height\n",
    "W = 512 # width\n",
    "C = 4 # channels\n",
    "f = 8 # downsampling factor\n",
    "\n",
    "degrees_per_second = 20 # degrees to travel per second\n",
    "fps = 40 # frames per second of output mp4\n",
    "\n",
    "use_score_corrector = False # whether or not to use score correction\n",
    "# Score corrector parameters\n",
    "fuzzy_prompt = False\n",
    "image_prompt = False\n",
    "\n",
    "cutn_batches = 2\n",
    "\n",
    "clip_guidance_scale = 0 # must be zero for this mode\n",
    "tv_scale = 0\n",
    "range_scale = 0\n",
    "sat_scale = 0\n",
    "\n",
    "init = None\n",
    "init_scale = 0\n",
    "\n",
    "animation_mode = 'None'\n",
    "skip_augs = True\n",
    "\n",
    "clamp_max = 0.02\n",
    "clamp_grad = True\n",
    "\n",
    "cut_overview = [12]*400+[4]*600 \n",
    "cut_innercut = [4]*400+[12]*600 \n",
    "cut_ic_pow = [1]*1000\n",
    "cut_icgray_p = [0.2]*400+[0]*600\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "precision_scope = autocast if precision==\"autocast\" else nullcontext\n",
    "\n",
    "frames_per_degree = fps / degrees_per_second\n",
    "\n",
    "if fixed_code:\n",
    "    for i in range(len(prompts)):\n",
    "        if fixed_seed is None:\n",
    "            fixed_seed = np.random.randint(np.iinfo(np.int32).max)\n",
    "        if isinstance(prompts[i], str):\n",
    "            prompts[i] = (fixed_seed, prompts[i])\n",
    "        else:\n",
    "            prompts[i][0] = fixed_seed\n",
    "\n",
    "# interpolation setup\n",
    "previous_c = None\n",
    "previous_start_code = None\n",
    "slerp_c_vectors = []\n",
    "slerp_start_codes = []\n",
    "for i, data in enumerate(map(lambda x: get_starting_code_and_conditioning_vector(*x), prompts)):\n",
    "    c, start_code = data\n",
    "    if i == 0:\n",
    "        slerp_c_vectors.append(c)\n",
    "        slerp_start_codes.append(start_code)\n",
    "    else:\n",
    "        start_norm = previous_c.flatten()/torch.norm(previous_c.flatten())\n",
    "        end_norm = c.flatten()/torch.norm(c.flatten())\n",
    "        omega = torch.acos((start_norm*end_norm).sum())\n",
    "        frames_c = round(omega.item() * frames_per_degree * 57.2957795)\n",
    "        start_norm = previous_start_code.flatten()/torch.norm(previous_start_code.flatten())\n",
    "        end_norm = start_code.flatten()/torch.norm(start_code.flatten())\n",
    "        omega = torch.acos((start_norm*end_norm).sum())\n",
    "        frames_start_code = round(omega.item() * frames_per_degree * 57.2957795)\n",
    "        \n",
    "        frames = frames_c if frames_c >= frames_start_code else frames_start_code\n",
    "        \n",
    "        original_c_shape = c.shape\n",
    "        original_start_code_shape = start_code.shape\n",
    "        c_vectors = get_slerp_vectors(previous_c.flatten(), c.flatten(), frames=frames)\n",
    "        c_vectors = c_vectors.reshape(-1, *original_c_shape)\n",
    "        slerp_c_vectors.extend(list(c_vectors[1:])) # drop first frame to prevent repeating frames\n",
    "        start_codes = get_slerp_vectors(previous_start_code.flatten(), start_code.flatten(), frames=frames)\n",
    "        start_codes = start_codes.reshape(-1, *original_start_code_shape)\n",
    "        slerp_start_codes.extend(list(start_codes[1:])) # drop first frame to prevent repeating frames\n",
    "        if loop and i == len(prompts) - 1:\n",
    "            c_vectors = get_slerp_vectors(c.flatten(), slerp_c_vectors[0].flatten(), frames=frames)\n",
    "            c_vectors = c_vectors.reshape(-1, *original_c_shape)\n",
    "            slerp_c_vectors.extend(list(c_vectors[1:-1])) # drop first and last frame to prevent repeating frames\n",
    "            start_codes = get_slerp_vectors(start_code.flatten(), slerp_start_codes[0].flatten(), frames=frames)\n",
    "            start_codes = start_codes.reshape(-1, *original_start_code_shape)\n",
    "            slerp_start_codes.extend(list(start_codes[1:-1])) # drop first and last frame to prevent repeating frames\n",
    "    previous_c = c\n",
    "    previous_start_code = start_code\n",
    "    \n",
    "slerp_c_vectors = unflatten(slerp_c_vectors, batch_size)\n",
    "slerp_start_codes = unflatten(slerp_start_codes, batch_size)\n",
    "\n",
    "if isinstance(init, str):\n",
    "    assert os.path.isfile(init)\n",
    "    init_image_ = load_img(init, (W, H)).to(device)\n",
    "    init_image_ = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image_))  # move to latent space\n",
    "else:\n",
    "    init = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run generation(s)\n",
    "if not skip_save_video:\n",
    "    video_out = imageio.get_writer(save_mp4, mode='I', fps=fps, codec='libx264')\n",
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            tic = time.time()\n",
    "            #all_samples = list()\n",
    "            for c, start_code in tqdm(zip(slerp_c_vectors, slerp_start_codes), desc=\"data\", total=len(slerp_c_vectors)):\n",
    "                uc = None\n",
    "                model_stats = []\n",
    "                if unconditional_guidance_scale != 1.0:\n",
    "                    uc = model.get_learned_conditioning(len(c) * [\"\"])\n",
    "                if isinstance(c, tuple) or isinstance(c, list):\n",
    "                    c = torch.stack(list(c), dim=0)\n",
    "                \n",
    "                c = torch.cat(tuple(c))\n",
    "                start_code = torch.cat(tuple(start_code))\n",
    "                \n",
    "                shape = [C, H // f, W // f]\n",
    "                \n",
    "                if use_k_lms:\n",
    "                            sigmas = model_wrap.get_sigmas(ddim_steps)\n",
    "                            model_wrap_cfg = CFGDenoiser(model_wrap)\n",
    "                            extra_args = {'cond': c, 'uncond': uc, 'cond_scale': unconditional_guidance_scale}\n",
    "                            samples_ddim = K.sampling.sample_lms(model_wrap_cfg, start_code*sigmas[0], sigmas, extra_args=extra_args)\n",
    "                else:\n",
    "                    samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                        conditioning=c,\n",
    "                                                        batch_size=len(c),\n",
    "                                                        shape=shape,\n",
    "                                                        verbose=False,\n",
    "                                                        unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                                        unconditional_conditioning=uc,\n",
    "                                                        eta=ddim_eta,\n",
    "                                                        x_T=start_code,\n",
    "                                                        score_corrector=ScoreCorrector if use_score_corrector else None,\n",
    "                                                        corrector_kwargs={})\n",
    "\n",
    "                x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                if not skip_save or not skip_save_video or show_images:\n",
    "                    for x_sample in x_samples_ddim:\n",
    "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                        if not skip_save_video:\n",
    "                            video_out.append_data(x_sample)\n",
    "                        if not skip_save or show_images:\n",
    "                            img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                        if not skip_save:\n",
    "                            img.save(os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "                        if show_images:\n",
    "                            display(img)\n",
    "\n",
    "            toc = time.time()\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outdir}\\n\"\n",
    "        f\"\\nTime elapsed: {round(toc - tic, 2)} seconds\")\n",
    "video_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disco Diffusion Style Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"A dragons lair, epic matte painting, concept art, trending on artstation\",\n",
    "    \"A wizards magical potion room, epic matte painting, concept art, trending on artstation\"\n",
    "]\n",
    "\n",
    "average_weights = True\n",
    "seed = None\n",
    "loop = True\n",
    "interpolate = True\n",
    "\n",
    "device='cuda'\n",
    "\n",
    "ddim_steps = 250 # ddim sampling steps\n",
    "ddim_eta = 0.05 # ddim eta (eta=0.0 corresponds to deterministic sampling) (must be 0.0 if using PLMS sampling)\n",
    "unconditional_guidance_scale = 12 # unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\n",
    "\n",
    "init_image = None\n",
    "init_noise_strength = 0.4\n",
    "previous_frame_noise_strength = 0.35\n",
    "animation_mode = '3D'\n",
    "perlin_init = False # whether ot not to use perlin noi\n",
    "perlin_mode = 'color' # color or gray\n",
    "\n",
    "fixed_code = True\n",
    "\n",
    "n_iter = 1\n",
    "batch_size = 1\n",
    "\n",
    "start_frame = 0\n",
    "\n",
    "resume_run = False\n",
    "\n",
    "batch_name = 'test'\n",
    "batchNum = 1\n",
    "batchFolder = 'outputs'\n",
    "\n",
    "H = 512 # height\n",
    "W = 512 # width\n",
    "C = 4 # channels\n",
    "f = 8 # downsampling factor\n",
    "\n",
    "degrees_per_second = 20 # degrees to travel per second\n",
    "fps = 40 # frames per second of output mp4\n",
    "\n",
    "use_score_corrector = False # whether or not to use score correction\n",
    "# Score corrector parameters\n",
    "fuzzy_prompt = False\n",
    "image_prompt = False\n",
    "\n",
    "cutn_batches = 2\n",
    "\n",
    "clip_guidance_scale = 0\n",
    "tv_scale = 0\n",
    "range_scale = 0\n",
    "sat_scale = 0\n",
    "\n",
    "init = None\n",
    "init_scale = 0\n",
    "\n",
    "animation_mode = 'None'\n",
    "skip_augs = True\n",
    "\n",
    "clamp_max = 0.02\n",
    "clamp_grad = True\n",
    "\n",
    "cut_overview = [12]*400+[4]*600 \n",
    "cut_innercut = [4]*400+[12]*600 \n",
    "cut_ic_pow = [1]*1000\n",
    "cut_icgray_p = [0.2]*400+[0]*600\n",
    "\n",
    "frames_per_degree = fps / degrees_per_second\n",
    "\n",
    "precision = 'autocast' # precision to evaluate at (full or autocast)\n",
    "\n",
    "video_init_path = \"init.mp4\"  # @param {type: 'string'}\n",
    "extract_nth_frame = 2  # @param {type: 'number'}\n",
    "persistent_frame_output_in_batch_folder = True  # @param {type: 'boolean'}\n",
    "video_init_seed_continuity = False  # @param {type: 'boolean'}\n",
    "# @markdown #####**Video Optical Flow Settings:**\n",
    "video_init_flow_warp = True  # @param {type: 'boolean'}\n",
    "# Call optical flow from video frames and warp prev frame with flow\n",
    "# @param {type: 'number'} #0 - take next frame, 1 - take prev warped frame\n",
    "video_init_flow_blend = 0.999\n",
    "video_init_check_consistency = False  # Insert param here when ready\n",
    "# @param ['None', 'linear', 'optical flow']\n",
    "video_init_blend_mode = \"optical flow\"\n",
    "# Call optical flow from video frames and warp prev frame with flow\n",
    "if animation_mode == \"Video Input\":\n",
    "    # suggested by Chris the Wizard#8082 at discord\n",
    "    if persistent_frame_output_in_batch_folder or (not is_colab):\n",
    "        videoFramesFolder = f'{batchFolder}/videoFrames'\n",
    "    else:\n",
    "        videoFramesFolder = f'/content/videoFrames'\n",
    "    createPath(videoFramesFolder)\n",
    "    print(f\"Exporting Video Frames (1 every {extract_nth_frame})...\")\n",
    "    try:\n",
    "        for f in pathlib.Path(f'{videoFramesFolder}').glob('*.jpg'):\n",
    "            f.unlink()\n",
    "    except:\n",
    "        print('')\n",
    "    vf = f'select=not(mod(n\\,{extract_nth_frame}))'\n",
    "    if os.path.exists(video_init_path):\n",
    "        subprocess.run(['ffmpeg', '-i', f'{video_init_path}', '-vf', f'{vf}', '-vsync', 'vfr', '-q:v', '2', '-loglevel',\n",
    "                       'error', '-stats', f'{videoFramesFolder}/%04d.jpg'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    else:\n",
    "        print(\n",
    "            f'\\nWARNING!\\n\\nVideo not found: {video_init_path}.\\nPlease check your video path.\\n')\n",
    "    #!ffmpeg -i {video_init_path} -vf {vf} -vsync vfr -q:v 2 -loglevel error -stats {videoFramesFolder}/%04d.jpg\n",
    "\n",
    "\n",
    "# @markdown ---\n",
    "\n",
    "# @markdown ####**2D Animation Settings:**\n",
    "# @markdown `zoom` is a multiplier of dimensions, 1 is no zoom.\n",
    "# @markdown All rotations are provided in degrees.\n",
    "\n",
    "key_frames = True  # @param {type:\"boolean\"}\n",
    "max_frames = 10000  # @param {type:\"number\"}\n",
    "\n",
    "if animation_mode == \"Video Input\":\n",
    "    max_frames = len(glob(f'{videoFramesFolder}/*.jpg'))\n",
    "\n",
    "# Do not change, currently will not look good. param ['Linear','Quadratic','Cubic']{type:\"string\"}\n",
    "interp_spline = 'Linear'\n",
    "angle = \"0:(0)\"  # @param {type:\"string\"}\n",
    "zoom = \"0: (1), 10: (1.0)\"  # @param {type:\"string\"}\n",
    "translation_x = \"0: (0)\"  # @param {type:\"string\"}\n",
    "translation_y = \"0: (0)\"  # @param {type:\"string\"}\n",
    "translation_z = \"0: (5.0)\"  # @param {type:\"string\"}\n",
    "rotation_3d_x = \"0: (0.2)\"  # @param {type:\"string\"}\n",
    "rotation_3d_y = \"0: (0)\"  # @param {type:\"string\"}\n",
    "rotation_3d_z = \"0: (0.0)\"  # @param {type:\"string\"}\n",
    "midas_depth_model = \"dpt_large\"  # @param {type:\"string\"}\n",
    "midas_weight = 0.3  # @param {type:\"number\"}\n",
    "near_plane = 200  # @param {type:\"number\"}\n",
    "far_plane = 10000  # @param {type:\"number\"}\n",
    "fov = 40  # @param {type:\"number\"}\n",
    "padding_mode = 'border'  # @param {type:\"string\"}\n",
    "sampling_mode = 'bicubic'  # @param {type:\"string\"}\n",
    "\n",
    "# ======= TURBO MODE\n",
    "# @markdown ---\n",
    "# @markdown ####**Turbo Mode (3D anim only):**\n",
    "# @markdown (Starts after frame 10,) skips diffusion steps and just uses depth map to warp images for skipped frames.\n",
    "# @markdown Speeds up rendering by 2x-4x, and may improve image coherence between frames.\n",
    "# @markdown For different settings tuned for Turbo Mode, refer to the original Disco-Turbo Github: https://github.com/zippy731/disco-diffusion-turbo\n",
    "\n",
    "turbo_mode = False  # @param {type:\"boolean\"}\n",
    "turbo_steps = \"3\"  # @param [\"2\",\"3\",\"4\",\"5\",\"6\"] {type:\"string\"}\n",
    "turbo_preroll = 10  # frames\n",
    "\n",
    "# insist turbo be used only w 3d anim.\n",
    "if turbo_mode and animation_mode != '3D':\n",
    "    print('=====')\n",
    "    print('Turbo mode only available with 3D animations. Disabling Turbo.')\n",
    "    print('=====')\n",
    "    turbo_mode = False\n",
    "\n",
    "# @markdown ---\n",
    "\n",
    "# @markdown ####**Coherency Settings:**\n",
    "# @markdown `frame_scale` tries to guide the new frame to looking like the old one. A good default is 1500.\n",
    "frames_scale = 1500  # @param{type: 'integer'}\n",
    "# @markdown `frame_skip_steps` will blur the previous frame - higher values will flicker less but struggle to add enough new detail to zoom into.\n",
    "# @param ['40%', '50%', '60%', '70%', '80%'] {type: 'string'}\n",
    "frames_skip_steps = '60%'\n",
    "\n",
    "# @markdown ####**Video Init Coherency Settings:**\n",
    "# @markdown `frame_scale` tries to guide the new frame to looking like the old one. A good default is 1500.\n",
    "video_init_frames_scale = 15000  # @param{type: 'integer'}\n",
    "# @markdown `frame_skip_steps` will blur the previous frame - higher values will flicker less but struggle to add enough new detail to zoom into.\n",
    "# @param ['40%', '50%', '60%', '70%', '80%'] {type: 'string'}\n",
    "video_init_frames_skip_steps = '70%'\n",
    "\n",
    "# ======= VR MODE\n",
    "# @markdown ---\n",
    "# @markdown ####**VR Mode (3D anim only):**\n",
    "# @markdown Enables stereo rendering of left/right eye views (supporting Turbo) which use a different (fish-eye) camera projection matrix.\n",
    "# @markdown Note the images you're prompting will work better if they have some inherent wide-angle aspect\n",
    "# @markdown The generated images will need to be combined into left/right videos. These can then be stitched into the VR180 format.\n",
    "# @markdown Google made the VR180 Creator tool but subsequently stopped supporting it. It's available for download in a few places including https://www.patrickgrunwald.de/vr180-creator-download\n",
    "# @markdown The tool is not only good for stitching (videos and photos) but also for adding the correct metadata into existing videos, which is needed for services like YouTube to identify the format correctly.\n",
    "# @markdown Watching YouTube VR videos isn't necessarily the easiest depending on your headset. For instance Oculus have a dedicated media studio and store which makes the files easier to access on a Quest https://creator.oculus.com/manage/mediastudio/\n",
    "# @markdown\n",
    "# @markdown The command to get ffmpeg to concat your frames for each eye is in the form: `ffmpeg -framerate 15 -i frame_%4d_l.png l.mp4` (repeat for r)\n",
    "\n",
    "vr_mode = False  # @param {type:\"boolean\"}\n",
    "# @markdown `vr_eye_angle` is the y-axis rotation of the eyes towards the center\n",
    "vr_eye_angle = 0.5  # @param{type:\"number\"}\n",
    "# @markdown interpupillary distance (between the eyes)\n",
    "vr_ipd = 5.0  # @param{type:\"number\"}\n",
    "\n",
    "# insist VR be used only w 3d anim.\n",
    "if vr_mode and animation_mode != '3D':\n",
    "    print('=====')\n",
    "    print('VR mode only available with 3D animations. Disabling VR.')\n",
    "    print('=====')\n",
    "    vr_mode = False\n",
    "\n",
    "\n",
    "def parse_key_frames(string, prompt_parser=None):\n",
    "    \"\"\"Given a string representing frame numbers paired with parameter values at that frame,\n",
    "    return a dictionary with the frame numbers as keys and the parameter values as the values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    string: string\n",
    "        Frame numbers paired with parameter values at that frame number, in the format\n",
    "        'framenumber1: (parametervalues1), framenumber2: (parametervalues2), ...'\n",
    "    prompt_parser: function or None, optional\n",
    "        If provided, prompt_parser will be applied to each string of parameter values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Frame numbers as keys, parameter values at that frame number as values\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    RuntimeError\n",
    "        If the input string does not match the expected format.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\")\n",
    "    {10: 'Apple: 1| Orange: 0', 20: 'Apple: 0| Orange: 1| Peach: 1'}\n",
    "\n",
    "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\", prompt_parser=lambda x: x.lower()))\n",
    "    {10: 'apple: 1| orange: 0', 20: 'apple: 0| orange: 1| peach: 1'}\n",
    "    \"\"\"\n",
    "    import re\n",
    "    pattern = r'((?P<frame>[0-9]+):[\\s]*[\\(](?P<param>[\\S\\s]*?)[\\)])'\n",
    "    frames = dict()\n",
    "    for match_object in re.finditer(pattern, string):\n",
    "        frame = int(match_object.groupdict()['frame'])\n",
    "        param = match_object.groupdict()['param']\n",
    "        if prompt_parser:\n",
    "            frames[frame] = prompt_parser(param)\n",
    "        else:\n",
    "            frames[frame] = param\n",
    "\n",
    "    if frames == {} and len(string) != 0:\n",
    "        raise RuntimeError('Key Frame string not correctly formatted')\n",
    "    return frames\n",
    "\n",
    "\n",
    "def get_inbetweens(key_frames, integer=False):\n",
    "    \"\"\"Given a dict with frame numbers as keys and a parameter value as values,\n",
    "    return a pandas Series containing the value of the parameter at every frame from 0 to max_frames.\n",
    "    Any values not provided in the input dict are calculated by linear interpolation between\n",
    "    the values of the previous and next provided frames. If there is no previous provided frame, then\n",
    "    the value is equal to the value of the next provided frame, or if there is no next provided frame,\n",
    "    then the value is equal to the value of the previous provided frame. If no frames are provided,\n",
    "    all frame values are NaN.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key_frames: dict\n",
    "        A dict with integer frame numbers as keys and numerical values of a particular parameter as values.\n",
    "    integer: Bool, optional\n",
    "        If True, the values of the output series are converted to integers.\n",
    "        Otherwise, the values are floats.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        A Series with length max_frames representing the parameter values for each frame.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> max_frames = 5\n",
    "    >>> get_inbetweens({1: 5, 3: 6})\n",
    "    0    5.0\n",
    "    1    5.0\n",
    "    2    5.5\n",
    "    3    6.0\n",
    "    4    6.0\n",
    "    dtype: float64\n",
    "\n",
    "    >>> get_inbetweens({1: 5, 3: 6}, integer=True)\n",
    "    0    5\n",
    "    1    5\n",
    "    2    5\n",
    "    3    6\n",
    "    4    6\n",
    "    dtype: int64\n",
    "    \"\"\"\n",
    "    key_frame_series = pd.Series([np.nan for a in range(max_frames)])\n",
    "\n",
    "    for i, value in key_frames.items():\n",
    "        key_frame_series[i] = value\n",
    "    key_frame_series = key_frame_series.astype(float)\n",
    "\n",
    "    interp_method = interp_spline\n",
    "\n",
    "    if interp_method == 'Cubic' and len(key_frames.items()) <= 3:\n",
    "        interp_method = 'Quadratic'\n",
    "\n",
    "    if interp_method == 'Quadratic' and len(key_frames.items()) <= 2:\n",
    "        interp_method = 'Linear'\n",
    "\n",
    "    key_frame_series[0] = key_frame_series[key_frame_series.first_valid_index()]\n",
    "    key_frame_series[max_frames -\n",
    "                     1] = key_frame_series[key_frame_series.last_valid_index()]\n",
    "    # key_frame_series = key_frame_series.interpolate(method=intrp_method,order=1, limit_direction='both')\n",
    "    key_frame_series = key_frame_series.interpolate(\n",
    "        method=interp_method.lower(), limit_direction='both')\n",
    "    if integer:\n",
    "        return key_frame_series.astype(int)\n",
    "    return key_frame_series\n",
    "\n",
    "if key_frames:\n",
    "    try:\n",
    "        angle_series = get_inbetweens(parse_key_frames(angle))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `angle` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `angle` as \"\n",
    "            f'\"0: ({angle})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        angle = f\"0: ({angle})\"\n",
    "        angle_series = get_inbetweens(parse_key_frames(angle))\n",
    "\n",
    "    try:\n",
    "        zoom_series = get_inbetweens(parse_key_frames(zoom))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `zoom` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `zoom` as \"\n",
    "            f'\"0: ({zoom})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        zoom = f\"0: ({zoom})\"\n",
    "        zoom_series = get_inbetweens(parse_key_frames(zoom))\n",
    "\n",
    "    try:\n",
    "        translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `translation_x` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `translation_x` as \"\n",
    "            f'\"0: ({translation_x})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        translation_x = f\"0: ({translation_x})\"\n",
    "        translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n",
    "\n",
    "    try:\n",
    "        translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `translation_y` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `translation_y` as \"\n",
    "            f'\"0: ({translation_y})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        translation_y = f\"0: ({translation_y})\"\n",
    "        translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n",
    "\n",
    "    try:\n",
    "        translation_z_series = get_inbetweens(parse_key_frames(translation_z))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `translation_z` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `translation_z` as \"\n",
    "            f'\"0: ({translation_z})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        translation_z = f\"0: ({translation_z})\"\n",
    "        translation_z_series = get_inbetweens(parse_key_frames(translation_z))\n",
    "\n",
    "    try:\n",
    "        rotation_3d_x_series = get_inbetweens(parse_key_frames(rotation_3d_x))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `rotation_3d_x` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `rotation_3d_x` as \"\n",
    "            f'\"0: ({rotation_3d_x})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        rotation_3d_x = f\"0: ({rotation_3d_x})\"\n",
    "        rotation_3d_x_series = get_inbetweens(parse_key_frames(rotation_3d_x))\n",
    "\n",
    "    try:\n",
    "        rotation_3d_y_series = get_inbetweens(parse_key_frames(rotation_3d_y))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `rotation_3d_y` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `rotation_3d_y` as \"\n",
    "            f'\"0: ({rotation_3d_y})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        rotation_3d_y = f\"0: ({rotation_3d_y})\"\n",
    "        rotation_3d_y_series = get_inbetweens(parse_key_frames(rotation_3d_y))\n",
    "\n",
    "    try:\n",
    "        rotation_3d_z_series = get_inbetweens(parse_key_frames(rotation_3d_z))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `rotation_3d_z` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `rotation_3d_z` as \"\n",
    "            f'\"0: ({rotation_3d_z})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        rotation_3d_z = f\"0: ({rotation_3d_z})\"\n",
    "        rotation_3d_z_series = get_inbetweens(parse_key_frames(rotation_3d_z))\n",
    "\n",
    "else:\n",
    "    angle = float(angle)\n",
    "    zoom = float(zoom)\n",
    "    translation_x = float(translation_x)\n",
    "    translation_y = float(translation_y)\n",
    "    translation_z = float(translation_z)\n",
    "    rotation_3d_x = float(rotation_3d_x)\n",
    "    rotation_3d_y = float(rotation_3d_y)\n",
    "    rotation_3d_z = float(rotation_3d_z)\n",
    "\n",
    "\n",
    "if seed is None:\n",
    "    seed = np.random.randint(np.iinfo(np.int32).max)\n",
    "\n",
    "args = {\n",
    "    'seed': seed,\n",
    "    'animation_mode': animation_mode,\n",
    "    'init_image': init_image,\n",
    "    'perlin_init': perlin_init,\n",
    "    'perlin_mode': perlin_mode,\n",
    "    'H': H,\n",
    "    'W': W,\n",
    "    'C': C,\n",
    "    'f': f,\n",
    "    'start_frame': start_frame,\n",
    "    'max_frames': max_frames,\n",
    "    'n_iter': n_iter,\n",
    "    'batch_size': batch_size,\n",
    "    'init_noise_strength': init_noise_strength,\n",
    "    'previous_frame_noise_strength': previous_frame_noise_strength,\n",
    "    'ddim_steps': ddim_steps,\n",
    "    'ddim_eta': ddim_eta,\n",
    "    'unconditional_guidance_scale': unconditional_guidance_scale,\n",
    "    'fixed_code': fixed_code,\n",
    "    'key_frames': key_frames,\n",
    "    'angle_series': angle_series,\n",
    "    'zoom_series': zoom_series,\n",
    "    'translation_x_series': translation_x_series,\n",
    "    'translation_y_series': translation_y_series,\n",
    "    'translation_z_series': translation_z_series,\n",
    "    'rotation_3d_x_series': rotation_3d_x_series,\n",
    "    'rotation_3d_y_series': rotation_3d_y_series,\n",
    "    'rotation_3d_z_series': rotation_3d_z_series,\n",
    "    'near_plane': near_plane,\n",
    "    'far_plane': far_plane,\n",
    "    'fov': fov,\n",
    "    'padding_mode': padding_mode,\n",
    "    'sampling_mode': sampling_mode,\n",
    "    'midas_weight': midas_weight\n",
    "}\n",
    "\n",
    "args = SimpleNamespace(**args)\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "start_code = None\n",
    "if fixed_code:\n",
    "    start_code = torch.randn([batch_size, C, H // f, W // f], device=device)\n",
    "\n",
    "precision_scope = autocast if precision==\"autocast\" else nullcontext\n",
    "\n",
    "if interpolate and len(prompts) > 1:\n",
    "    previous_c = None\n",
    "    slerp_c_vectors = []\n",
    "    for i, c in enumerate(map(lambda x: get_conditioning_vector(x), prompts)):\n",
    "        if i == 0:\n",
    "            slerp_c_vectors.append(c)\n",
    "        else:\n",
    "            start_norm = previous_c.flatten()/torch.norm(previous_c.flatten())\n",
    "            end_norm = c.flatten()/torch.norm(c.flatten())\n",
    "            omega = torch.acos((start_norm*end_norm).sum())\n",
    "            frames = round(omega.item() * frames_per_degree * 57.2957795)\n",
    "\n",
    "            original_c_shape = c.shape\n",
    "            c_vectors = get_slerp_vectors(previous_c.flatten(), c.flatten(), frames=frames)\n",
    "            c_vectors = c_vectors.reshape(-1, *original_c_shape)\n",
    "            slerp_c_vectors.extend(list(c_vectors[1:])) # drop first frame to prevent repeating frames\n",
    "            if loop and i == len(prompts) - 1:\n",
    "                c_vectors = get_slerp_vectors(c.flatten(), slerp_c_vectors[0].flatten(), frames=frames)\n",
    "                c_vectors = c_vectors.reshape(-1, *original_c_shape)\n",
    "                slerp_c_vectors.extend(list(c_vectors[1:-1])) # drop first and last frame to prevent repeating frames\n",
    "        previous_c = c\n",
    "    data = ['']\n",
    "else:\n",
    "    data = list(chunk(prompts, batch_size))\n",
    "    interpolate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Run animation loop - WIP\n",
    "TRANSLATION_SCALE = 1.0/200.0\n",
    "stop_on_next_loop = False\n",
    "\n",
    "# initialize midas depth model\n",
    "midas_model, midas_transform, midas_net_w, midas_net_h, midas_resize_mode, midas_normalization = init_midas_depth_model()\n",
    "\n",
    "# make sure sampler is DDIM\n",
    "sampler = DDIMSampler(model)\n",
    "for frame_num in tqdm(range(args.start_frame, args.max_frames), desc='Frames'):\n",
    "    if stop_on_next_loop:\n",
    "        break\n",
    "\n",
    "    if frame_num == 0:\n",
    "        init_image = args.init_image\n",
    "\n",
    "    if args.animation_mode == \"2D\":\n",
    "        if args.key_frames:\n",
    "            angle = args.angle_series[frame_num]\n",
    "            zoom = args.zoom_series[frame_num]\n",
    "            translation_x = args.translation_x_series[frame_num]\n",
    "            translation_y = args.translation_y_series[frame_num]\n",
    "            print(\n",
    "                f'angle: {angle}',\n",
    "                f'zoom: {zoom}',\n",
    "                f'translation_x: {translation_x}',\n",
    "                f'translation_y: {translation_y}',\n",
    "            )\n",
    "\n",
    "        if frame_num > 0:\n",
    "            # seed += 1\n",
    "            if resume_run and frame_num == start_frame:\n",
    "                img_0 = cv2.imread(\n",
    "                    batchFolder+f\"/{batch_name}({batchNum})_{start_frame-1:04}.png\")\n",
    "            else:\n",
    "                img_0 = cv2.imread('prevFrame.png')\n",
    "                center = (1*img_0.shape[1]//2, 1*img_0.shape[0]//2)\n",
    "                trans_mat = np.float32(\n",
    "                    [[1, 0, translation_x],\n",
    "                     [0, 1, translation_y]]\n",
    "                )\n",
    "                rot_mat = cv2.getRotationMatrix2D(center, angle, zoom)\n",
    "                trans_mat = np.vstack([trans_mat, [0, 0, 1]])\n",
    "                rot_mat = np.vstack([rot_mat, [0, 0, 1]])\n",
    "                transformation_matrix = np.matmul(rot_mat, trans_mat)\n",
    "                img_0 = cv2.warpPerspective(\n",
    "                    img_0,\n",
    "                    transformation_matrix,\n",
    "                    (img_0.shape[1], img_0.shape[0]),\n",
    "                    borderMode=cv2.BORDER_WRAP\n",
    "                )\n",
    "\n",
    "            cv2.imwrite('prevFrameScaled.png', img_0)\n",
    "            init_image = 'prevFrameScaled.png'\n",
    "        \n",
    "    if args.animation_mode == \"3D\":\n",
    "        if frame_num > 0:\n",
    "            # seed += 1\n",
    "            if resume_run and frame_num == start_frame:\n",
    "                img_filepath = batchFolder + \\\n",
    "                    f\"/{batch_name}({batchNum})_{start_frame-1:04}.png\"\n",
    "                if turbo_mode and frame_num > turbo_preroll:\n",
    "                    shutil.copyfile(img_filepath, 'oldFrameScaled.png')\n",
    "            else:\n",
    "                img_filepath = 'prevFrame.png'\n",
    "\n",
    "            next_step_pil = do_3d_step(\n",
    "                img_filepath, frame_num, midas_model, midas_transform)\n",
    "            next_step_pil.save('prevFrameScaled.png')\n",
    "\n",
    "            # Turbo mode - skip some diffusions, use 3d morph for clarity and to save time\n",
    "            if turbo_mode:\n",
    "                if frame_num == turbo_preroll:  # start tracking oldframe\n",
    "                    # stash for later blending\n",
    "                    next_step_pil.save('oldFrameScaled.png')\n",
    "                elif frame_num > turbo_preroll:\n",
    "                    # set up 2 warped image sequences, old & new, to blend toward new diff image\n",
    "                    old_frame = do_3d_step(\n",
    "                        'oldFrameScaled.png', frame_num, midas_model, midas_transform)\n",
    "                    old_frame.save('oldFrameScaled.png')\n",
    "                    if frame_num % int(turbo_steps) != 0:\n",
    "                        print(\n",
    "                            'turbo skip this frame: skipping clip diffusion steps')\n",
    "                        filename = f'{batch_name}({batchNum})_{frame_num:04}.png'\n",
    "                        blend_factor = (\n",
    "                            (frame_num % int(turbo_steps))+1)/int(turbo_steps)\n",
    "                        print(\n",
    "                            'turbo skip this frame: skipping clip diffusion steps and saving blended frame')\n",
    "                        # this is already updated..\n",
    "                        newWarpedImg = cv2.imread('prevFrameScaled.png')\n",
    "                        oldWarpedImg = cv2.imread('oldFrameScaled.png')\n",
    "                        blendedImage = cv2.addWeighted(\n",
    "                            newWarpedImg, blend_factor, oldWarpedImg, 1-blend_factor, 0.0)\n",
    "                        cv2.imwrite(\n",
    "                            f'{batchFolder}/{filename}', blendedImage)\n",
    "                        # save it also as prev_frame to feed next iteration\n",
    "                        next_step_pil.save(f'{img_filepath}')\n",
    "                        if vr_mode:\n",
    "                            generate_eye_views(\n",
    "                                TRANSLATION_SCALE, batchFolder, filename, frame_num, midas_model, midas_transform)\n",
    "                        continue\n",
    "                    else:\n",
    "                        # if not a skip frame, will run diffusion and need to blend.\n",
    "                        oldWarpedImg = cv2.imread('prevFrameScaled.png')\n",
    "                        # swap in for blending later\n",
    "                        cv2.imwrite(f'oldFrameScaled.png', oldWarpedImg)\n",
    "                        print('clip/diff this frame - generate clip diff image')\n",
    "\n",
    "            init_image = 'prevFrameScaled.png'\n",
    "\n",
    "        if args.animation_mode == \"Video Input\":\n",
    "            init_scale = args.video_init_frames_scale\n",
    "            skip_steps = args.calc_frames_skip_steps\n",
    "            if not video_init_seed_continuity:\n",
    "                seed += 1\n",
    "            if video_init_flow_warp:\n",
    "                if frame_num == 0:\n",
    "                    skip_steps = args.video_init_skip_steps\n",
    "                    init_image = f'{videoFramesFolder}/{frame_num+1:04}.jpg'\n",
    "                if frame_num > 0:\n",
    "                    prev = PIL.Image.open(\n",
    "                        batchFolder+f\"/{batch_name}({batchNum})_{frame_num-1:04}.png\")\n",
    "\n",
    "                    frame1_path = f'{videoFramesFolder}/{frame_num:04}.jpg'\n",
    "                    frame2 = PIL.Image.open(\n",
    "                        f'{videoFramesFolder}/{frame_num+1:04}.jpg')\n",
    "                    flo_path = f\"/{flo_folder}/{frame1_path.split('/')[-1]}.npy\"\n",
    "\n",
    "                    init_image = 'warped.png'\n",
    "                    print(video_init_flow_blend)\n",
    "                    weights_path = None\n",
    "                    if video_init_check_consistency:\n",
    "                        # TBD\n",
    "                        pass\n",
    "\n",
    "                    warp(prev, frame2, flo_path, blend=video_init_flow_blend,\n",
    "                         weights_path=weights_path).save(init_image)\n",
    "\n",
    "            else:\n",
    "                init_image = f'{videoFramesFolder}/{frame_num+1:04}.jpg'\n",
    "\n",
    "    seed_everything(args.seed+frame_num)\n",
    "    if interpolate:\n",
    "        model_stats = []\n",
    "    else:\n",
    "        model_stats = get_clip_embeds(prompts, seed)\n",
    "    init = None\n",
    "    if init_image is not None:\n",
    "        init = load_img(init_image, (args.W, args.H)).to(device)\n",
    "        init = repeat(init, '1 ... -> b ...', b=args.batch_size)\n",
    "        init_latent = model.get_first_stage_encoding(\n",
    "            model.encode_first_stage(init))  # move to latent space\n",
    "\n",
    "        sampler.make_schedule(ddim_num_steps=args.ddim_steps,\n",
    "                              ddim_eta=args.ddim_eta, verbose=False)\n",
    "\n",
    "        noise_strength = args.init_noise_strength if frame_num == 0 else args.previous_frame_noise_strength\n",
    "        assert 0. <= noise_strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "        t_enc = int(noise_strength * args.ddim_steps)\n",
    "        print(f\"target t_enc is {t_enc} steps\")\n",
    "\n",
    "    if args.perlin_init:\n",
    "        if args.perlin_mode == 'color':\n",
    "            init = create_perlin_noise(\n",
    "                [1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "            init2 = create_perlin_noise(\n",
    "                [1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
    "        elif args.perlin_mode == 'gray':\n",
    "            init = create_perlin_noise(\n",
    "                [1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
    "            init2 = create_perlin_noise(\n",
    "                [1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "        else:\n",
    "            init = create_perlin_noise(\n",
    "                [1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "            init2 = create_perlin_noise(\n",
    "                [1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "        init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(\n",
    "            2).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "        del init2\n",
    "\n",
    "    print(f'Frame {frame_num}')\n",
    "\n",
    "    image_display = Output()\n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                tic = time.time()\n",
    "                all_samples = list()\n",
    "                print('')\n",
    "                display(image_display)\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                if perlin_init:\n",
    "                    init = regen_perlin()\n",
    "                    start_code = init\n",
    "\n",
    "                for prompts in tqdm(data, desc=\"data\"):\n",
    "                    uc = None\n",
    "                    if unconditional_guidance_scale != 1.0:\n",
    "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "                    \n",
    "                    if interpolate:\n",
    "                        c = slerp_c_vectors[frame_num%len(slerp_c_vectors)]\n",
    "                        c = torch.cat([c])\n",
    "                    else:\n",
    "                        c = torch.cat([get_conditioning_vector(prompt) for prompt in prompts])\n",
    "\n",
    "                    if init_image == None or init_image == '' or init_image == [] or init_image == ['']:\n",
    "                        shape = [args.C, args.H // args.f, args.W // args.f]\n",
    "                        samples_ddim, _ = sampler.sample(S=args.ddim_steps,\n",
    "                                                         conditioning=c,\n",
    "                                                         batch_size=args.batch_size,\n",
    "                                                         shape=shape,\n",
    "                                                         verbose=False,\n",
    "                                                         unconditional_guidance_scale=args.unconditional_guidance_scale,\n",
    "                                                         unconditional_conditioning=uc,\n",
    "                                                         eta=args.ddim_eta,\n",
    "                                                         x_T=start_code,\n",
    "                                                        score_corrector=ScoreCorrector if use_score_corrector else None,\n",
    "                                                        corrector_kwargs={})\n",
    "\n",
    "                    else:\n",
    "                        # encode (scaled latent)\n",
    "                        z_enc = sampler.stochastic_encode(\n",
    "                            init_latent, torch.tensor([t_enc]*args.batch_size).to(device))\n",
    "                        # decode it\n",
    "                        samples_ddim = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=args.unconditional_guidance_scale,\n",
    "                                                      unconditional_conditioning=uc)\n",
    "\n",
    "                    x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                    x_samples_ddim = torch.clamp(\n",
    "                        (x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                    for x_sample in x_samples_ddim:\n",
    "                        x_sample = 255. * \\\n",
    "                            rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                        img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                        display(img)\n",
    "\n",
    "                        if args.animation_mode != \"None\":\n",
    "                            filename = f'{batch_name}({batchNum})_{frame_num}.png'\n",
    "                            img.save('prevFrame.png')\n",
    "                            img.save(f'{batchFolder}/{filename}')\n",
    "                            # if frame_num == 0:\n",
    "                            #     save_settings()\n",
    "                            if args.animation_mode == \"3D\":\n",
    "                                # If turbo, save a blended image\n",
    "                                if turbo_mode and frame_num > 0:\n",
    "                                    # Mix new image with prevFrameScaled\n",
    "                                    blend_factor = (1)/int(turbo_steps)\n",
    "                                    # This is already updated..\n",
    "                                    newFrame = cv2.imread('prevFrame.png')\n",
    "                                    prev_frame_warped = cv2.imread(\n",
    "                                        'prevFrameScaled.png')\n",
    "                                    blendedImage = cv2.addWeighted(\n",
    "                                        newFrame, blend_factor, prev_frame_warped, (1-blend_factor), 0.0)\n",
    "                                    cv2.imwrite(\n",
    "                                        f'{batchFolder}/{filename}', blendedImage)\n",
    "                                else:\n",
    "                                    img.save(f'{batchFolder}/{filename}')\n",
    "\n",
    "                                if vr_mode:\n",
    "                                    generate_eye_views(\n",
    "                                        TRANSLATION_SCALE, batchFolder, filename, frame_num, midas_model, midas_transform)\n",
    "                toc = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "latent-imagenet-diffusion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "96115ba3d133a87032cd70b9a322d48617443b59c68527f3d6b41f2029e5d2d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
